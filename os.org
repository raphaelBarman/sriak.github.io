#+TITLE:  CS-323: Introduction to Operating Systems
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/home/raph/school/theme.css"/>
#+OPTIONS: toc:2, H:4
Grading
- Midterm 50%
- Final 50%
* Lecture 1
** What does an OS do?
- Abstracts the hardware (CPU, Memory, Disks, Devices), makes it easier to use (Process, Address space, File, Virtual Device)
| Hardware | Abstraction    | OS Components      |
|----------+----------------+--------------------|
| CPU      | Process        | Process Management |
| Memory   | Address space  | Memory Management  |
| Disks    | File           | File Systems       |
| Devices  | Virtual Device | Device Management  |
- Manages the resources of the hardware
- OS does both abstraction and resource management
  - Web browser = only abstraction, not part of OS
  - Device driver = both, part of OS
** Where does the OS live?
- CPU operates in dual-mode, kernel mode vs user mode
- Hardware mode bit
- Kernel mode
  - Privileged instructions. Eg. set mode bit
  - Direct access to all of memory
  - Direct access to devices
- User mode
  - Applications
  - System programs (e.g daemons)
- Allow to separate user application to OS application
- From user to kernel mode
  - Device generates interrupt
  - Program executes a system call
** System calls
- *only* way to access from program to OS
- machine instruction
- Kernel API: additional layer simplifying system calls
- Language api, (eg. libc): wraps the kernel API
  - Makes system call look like function call
  - It is *not* a function call
  - It is a user-kernel transition
  - Much more expensive
** Traps
- Generated by CPU as a result of error
- Works like an "involuntary" system calls
- Identified by a trap number
** Interrupts
- Generated by a device that needs attention
- Identified by an interrupt number
** OS control flow: event-driven program
- Nothing to do \Rightarrow do nothing, idle loop
- interrupt (from device), trap, system call (from process) \Rightarrow start running
*** What does the hardware on a {} i
- Puts the machine in kernel mode 
- Sets the PC = {}Vector[i] 
- {}Vector is a predefined location
Where {} can be system call, trap, interrupt
*** Simplified execution flow
- User executes system call /i/
- Hardware 
  - Puts machine in kernel mode 
  - Sets PC to SystemCallVector[i] 
- Kernel  
  - Executes system call i handler routine (includes check on inbound parameters). 
  - Executes a return from kernel instruction 
- Hardware 
  - Puts machine in user mode 
- User executes instruction aver system call 
*** OS design goals
- Correct abstractions
- Performance
- Portability
- Reliability
** OS structure
- OS is a huge piece of software
- Something goes wrong in kernel mode
- Incentive to move stuff out of kernel mode
- No need for entire OS in kernel mode: some pieces can be in user mode, such as
  daemons (e.g system logs)
- Microkernel, absolute minimum in kernel mode, interprocess communication
  primitives, all the rest in user mode
* Lecture 2
** Process
- Process = program that currently executes
- Process identification (pid) = unique identifier
- Operations:
  - Create
  - Terminate
     - Normal exit
     - Error
     - Terminated by another process
*** Linux process primitives
**** pid = fork()
- Creates an /identical/ copy of parent
- In parent, returns pid of child
- In child returns 0
**** exec( filename )
Loads executable from file with filename
**** wait()
Wait for one of its children to terminate
**** exit()
Terminate the process
**** Example
#+BEGIN_SRC sh
if( pid = fork()) {
    wait()
}
else {
    exec( filename )
}
#+END_SRC
*** Linux shell
#+BEGIN_SRC shell
forever{
    read from input
    if(logout) exit()
    if( pid = fork() ) {
        wait()
    }
    else {
        exec( filename )
   }
}
#+END_SRC
*** Operation
New command line (!= logout)
- shell forks a new process and waits
- Child executes program on cli
** Linux process tree
*** Boot
- First process after boot is the init process
- Happens by black magic
*** User logs in
- Init forks and waits
- Child execs shell
*** User runs make
- Shell forks and waits
- Child execs make
*** Why fork+exec vs. create?
Process = environment (ownership,open files, values of env variables) + code

\Rightarrow child automatically inherits environment

This way, we can write code before exec, shell can manipulate environment of
child, e.g. can manipulate stdin and stdout.
** What does a process do?
- Either computes (use CPU)
- Or does I/O
*** Single process system
- Very inefficient (very poor CPU utilization)
- Very annoying (can't do anything else
*** Multi-process system
- Many process in the system
- One uses CPU
- When it does I/O, waits for I/O and leaves CPU idle
- Another process get the CPU
** Process switch
- Switch from one process running on the cpu to another process
- Such that you can later switch back to the process currently holding the CPU
*** Implementation
Process consists of: 
- Code (including libraries) 
- Stack 
- Heap 
- Registers (including PC) 
- MMU info (ignore for now)
*** Process Control Block (PCB)
Register and MMU resides in shared locations, needs a way to save and restore
stuff when switching processes
- Kernel must remember processes 
- Each process has a process control block (PCB) 
- Process control block contains 
- Process identifier (unique id) 
- Process state 
- Space to support process switch (save area) 
- Process Control Block Array 
- Indexed by hash( pid ) 
*** Caveat
- A process switch is an expensive operation! 
- Requires saving and restoring lots of stuff 
- Not just registers 
- Also MMU information 
- Has to be implemented very efficiently 
- Has to be used with care 
** Process scheduler
[[file:Lecture 2/screenshot_2017-03-01_14-42-09.png]]
*** Preemptive vs Non-preemptive Scheduler
Non-preemptive
- Process only voluntarily relinquishes CPU  
Preemptive
- Process may be forced off CPU 
**** Advantages - Disadvantages
- Non-preemptive
  - Process can monopolize CPU 
  - Only useful in special circumstances 
- Preemptive
  - Process can be thrown out at any time
  - Usually not a problem, but sometimes it is 
- Intermediate solutions are possible 
*** Process Scheduling Implementation
- Remember running process
- Maintain sets of queues
  - (CPU) ready queue
  - I/O device queue (one per device)
- PCBs sit in queues
*** How does the Scheduler run?  
- Scheduler is part of the kernel
- Scheduler runs when
  - process starts or terminates (system call) 
  - running process performs an I/O (system call) 
  - I/O completes (I/O interrupt) 
  - timer expires (timer interrupt) 
- At end of handlers for
  - System calls
  - Interrupts
  - Traps
- Scheduler runs: decides on process to run
- Switches to a new process
- Sets another timer
*** Scheduling Algorithm
Decides	which ready process gets to run	
**** Interactive vs. Batch
Interactive = you are waiting for the result (short)
- Needs short response time
- response time = wait from ready to running
- Scheduler run often
Batch = you will look at result later (long)
- High throughput
- Throughput = number of jobs completed
- Scheduler is overhead
Often, scheduler does not know a priori if a process is interactive or batch   
*** Example scheduling algorithms
**** First come, first served (FCFS) 
- Process ready: insert at tail of queue 
- Head of queue: “oldest” ready process 
- By definition, non-preemptive 
 

- Low overhead – few scheduling events 
- Good throughput 
- Uneven response time – stuck behind long job 
- Extreme case – process monopolizes CPU 
**** Shortest Job First (SJF)	
- Process Ready, insert in queue according to length
- Head of queue = "shortest" process
- Can be preemptive or non-preemptive
- From now on, only consider preemptive


- Good response time for short jobs
- Can lead to starvation of long jobs
- Difficult to predict job length
**** Round Robin (RR)
- Define time quantum \Delta
- Process ready: put at tail of queue
- Head of queue: run for \Delta time
- After \Delta
  - Put running process at the tail of the queue
  - Re-schedule


- Short jobs finish quickly (a few rounds)
- Long jobs not postponed forever
- No need to know job length
- Discover length by how many \Delta 's it needs

How do we pick \Delta?
- If too small:
  - Many scheduling events
  - Good response time
  - Low throughput
- If too large:
  - Few scheduling events
  - Poor response time
  - Good throughput
**** Priority (PR)
- Assign each process a priority Pr(P)
- Process ready: insert in queue according to Pr(P)
- Head of queue: highest-priority process


- Differentiation according to job importance
- Prone to starvation of low-priority jobs
**** Priority + Aging (PR+A)  
- Assign each process a priority Pr(P)
- Process ready: insert in q according to Pr(P)
- Reduce priority over time

- Lessens problem of starving low-priority jobs
**** PR + RR
- As with priority, but RR between process with equal priority
- Typical implementation:
  - Multiple queues, one for each priority
  - Process ready: insert at tail of queue with its priority
  - Schedule: had of non-empty queue with highest priority for \Delta
**** RR + FCFS
- Two queues: one for RR, one for FCFS
- Initially, process goes in RR queue
- After n \Delta 's, goes in FCFS queue
* Lecture 3
** Multiprocess program 
- Single process, too wasteful.
- Multiprocess where you create a worker for each request = bad, because
  creating a worker process is expensive
- Better version = create a pool of worker and then dispatch work, then only
  need to send a message to the worker
** Interprocess Communication (IPC)
*** Where is it needed?
- Client-Server Communication
- Between	cooperating process
  - Access to System Processes (e.g. user with daemon) 
*** Message	passing primitives
- Send / Receive
- By value communication
- Never by reference
- Receiver cannot affect message in sender.
*** Message Passing Implementation
- Message sent to kernel, added to tail of pid proctable
- Kernel sends it to the given pid
*** Alternatives
**** Symmetric addressing
- Send(msg,topid)
- Receive(msg,frompid)
- Message is (typically) a struct
- topid, frompid are process identifier
- Symmetric addressing seldom used
**** Assymetric addressing
- Send( msg, pid ): Send msg to process pid  
- pid = Receive( msg )  
  - Receive msg from any process
  - Return the pid of sending process  
- More common and useful form of addressing  
*** Blocking or Nonblocking
**** Send
- Nonblocking: send returns immediately after message is sent  
- Blocking: sender blocks message is delivered  
- Nonblocking is the more common form  
**** Receive
- Nonblocking  
  - Receive returns immediately  
  - Regardless of message present or not   
- Blocking: Receive blocks until message is present  
- Blocking is the more common form  
** [[https://fr.wikipedia.org/wiki/Remote_procedure_call][Remote procedure call (RPC)]] 
*** RPC Interface
- List of remotely callable procedures
- With their arguments and return values
*** Problem
- Want a procedure call interface  
- Have only message passing between processes  
- How to bridge the gap?  
*** Stub library
- Client stub and server stub  
- Client stub linked with client process  
- Server stub linked with server process  
**** Client Stub
- Sends arguments in call message
- Receiver return values in return message
**** Server stub
- Receives arguments in call message
- Invokes procedure
- Send returns in return message
**** Example:
Client stub
#+BEGIN_SRC C
GetTime()  
{ 
    msg->procno = 1 
    Send( msg ) 
    Receive( msg ) 
    return( msg->retval0 ) 
} 
SetTime( long Bme ) 
{ 
    msg->procno = 2 
    msg->arg0 = Bme 
    Send( msg ) 
    Receive( msg ) 
    return( msg->retval0 ) 
}
#+END_SRC

Server stub
#+BEGIN_SRC C
while( true ) do 
  { 
    Receive( msg ) 
    switch msg->procno { 
        case 1: { Bme = GetTime() 
                        msg->retval0 = Bme 
                        Send( msg ) } 
        case 2: { ret = SetTime( msg->arg0 ) 
                msg->retval0 = ret 
                Send( msg ) } 
  } 
#+END_SRC
* Lecture 4
** Multi-threading vs. multi-processing
- Processes provide separation  
  - In particular, memory separation (no shared data)  
  - Suitable for coarse-grain interaction  
- Threads do not  
  - In particular, share memory (shared data)  
  - Suitable for tighter integration  
Most important difference:
- Process crashes \rightarrow other processes are not affected
- Thread crashes \rightarrow the entire process, including other threads, crashes
*** Concrete Example: Web Server
- Serving static content
  - Probably no bugs
  - Can easily be done in multi-threading process
- Serving dynamic (third-party) content
  - No guarantees about bugs
  - Keep in a different process
** Basic Approach to Multi-threading
- Divide “work” among multiple threads  
- Which data is shared?  
  - Globals and heap  
  - Not locals  
  - Not readonly  
- Where is shared data accessed?  
- Define one mutex
- Put lock/unlock around each shared access
- Put shared data access in critical section  
  - Only one process at a time can access it  
Mostly works, because:
- Trouble with multi-threaded execution:
  - data races
  - data change by another thread
- Critical section: no other thread can change data
** Locking
- Single lock strategies: Does not work very well, prevent good parallelism
- Fine Grain Locking, define various separated locks
- Privatization, define local variable, use lock only after heavy computation
** Pthreads: Condition variables
- Pthread_cond_wait(cond, mutex)
  - Wait for a signal on cond
  - *Release mutex*
- Pthread_cond_signalcond, mutex*)
  - Signal one thread waiting on cond
  - Signaled thread *re-acquire mutex* \to at some later time, not necessarily immediately
  - if no thread waiting, no-op
 - Pthread_cond_broadcast(cond, mutex)
  - Signal all threads waiting on cond
  - If not thread waiting, no-op
*** Pthread Implementation
**** Simplified
- Pthreads primitives result in syscalls
- Kernel runs when invoked
**** Data Structures
Kernel maintains for each lock
- Whether lock is held
- If so, which thread holds lock
- Queue of threads waiting to acquire lock
**** Execution
- On mutex_lock
  - If lock is not held, set lock to held by this thread
  - If is held, queue this thread in queue for this lock
- On mutex_unlock
  - In no threads in queue, set lock to not held
  - If thread in queue
    - Dequeue one thread from queue
    - Set lock to held by this thread
    - Add thread to ready queue
** Kernel as a server
- Requests from users: syscalls, traps
- Requests from devices: interrupts
- For simplicity, one kernel thread for each user thread, 1-to-1, case in Linus, not in other Oss
*** How does it work?
**** User to Kernel
- User thread makes syscall
- Switch to kernel mode
- PC = system call handler routine
- SP = kernel stack of kernel thread
**** Kernel to User
- SP = stack of user thread
- PC = user thread PC (after syscall)
- Return from kernel mode
- Run in user thread
**** Not: Separate Stack
- User thread and corresponding kernel thread have separate stacks
- Why? Security: While one thread of a process in kernel, other thread could modify stack
*** Device interrupt
What makes kernel different.
- Device interrupt
- PC = interrupt handler
- SP = interrupt thread stack
- Run interrupt handler
*** Kernel Synchronization
- Different kernel threads access shared data 
- Must be synchronized 
- As in any multithreaded program 
- But interrupts make things different:
  - Must be server quickly
  - Interrupt handling must not block
**** Solution
- Add another set threads: Soft interrupt threads 
- Interrupt  
  - Does absolute minimum to service device 
  - Never blocks! 
  - Put request in queue for soft interrupt thread 
  - Get soft interrupt thread ready 
- Soft interrupt thread : Does bulk of work 
**** Advantages
- Interrupts can be served quickly 
- Narrow interface: Interrupt and rest of the kernel 
- Soft interrupt threads ~ other kernel threads: With some exceptions, not going into it here 
* Lecture 5
Assumption: Ignore L caches + for this week only: program in memory, only look
at processor and main memory, not disk.
** Main Memory Allocation
*** Allocating Main Memory for the kernel
- Almost always in low memory  
- Why? Interrupt vectors are in low memory  
** Protection
One process must not be able to read or write the memory of
- another process
- the kernel
Protected access: check before writing in memory, since must be done a lot,
implemented in hardware.
** Transparency
Programmer should not have to worry   
- where his program is in memory  
- where or what other programs are in memory  
Program can be anywhere in main memory
** Virtual vs. Physical Address Space
What the program thinks is its memory vs where the program is in physical
memory.

Virtual addresses are mapped into physical address \to done by the Memory
Management Unit (MMU, is hardware).
** Different Virtual/Physical Schemes
For each scheme:
- Virtual address space  
- Physical address space  
- Virtual address  
- MMU  
*** Base and Bounds
**** Virtual Address Space
- Linear address space
- From 0 to MAX
**** Physical Address Space
- Linear address space
- From BASE to BOUNDS=BASE+MAX

[[file:files/Lecture 5/screenshot_2017-03-22_13-46-24.png]]
**** MMU for Base and Bounds
- Relocation register: holds the base value  
- Limit register: holds the bounds value  

[[file:files/Lecture 5/screenshot_2017-03-22_13-47-53.png]]
*** Segmentation
- Virtual address space:  
  - Two-dimensional  
  - Set of segments 0..n  
  - Each segment i is linear from 0 to MAX_i
- Physical address space  
  - Set of segments, each linear  
[[file:files/Lecture 5/screenshot_2017-03-22_13-50-17.png]]

What is a Segment:
- Anything you want it to be, eg. code, heap stack
**** Virtual Address
+ Two-dimensional address:  
  + Segment number s  
  + Offset d within segment (starting at 0)  
+ It is like multiple base-and-bounds  
**** MMU
- STBR: points to segment table in memory  
- STLR: length of segment table  
- Segment table  
  - Indexed by segment number  
  - Contains (base, limit) pair  
  - Base: physical address of segment in memory  
  - Limit: length of segment  
[[file:files/Lecture 5/screenshot_2017-03-22_13-53-51.png]]
A segment for each process
*** (Simplified) Paging
- Page: fixed-size portion of virtual memory  
- Frame: fixed-size portion of physical memory  
- *Page size = frame size*
- Typical size: 4k – 8k (always power of 2)  
Virtual Address Space: linear from 0 up to a multiple of page size
Physical Address Space: Non-contiguous set of frames, one per page

[[file:files/Lecture 5/screenshot_2017-03-22_13-59-29.png]]
**** Virtual Address
- Virtual address: 0 - MAX (page size multiple)
- Page size = 2^n
- Virtual address for mapping purposes:
  - Page number p: first sequence of bits
  - Offset within page d: n remaining bits
**** MMU
- PTBR: points to page table in memory  
- PTLR: length of page table  
- Page table  
  - Indexed by page number  
  - Contains frame number of page in memory  
[[file:files/Lecture 5/screenshot_2017-03-22_14-02-10.png]]

p not visible to the program, not as in segmentation, where the programs sees s.
*** Segmentation with Paging
As segmentation, but every segment is paged.
- Virtual address space:
  - two-dimensional
  - Set of segments 0..n
  - Each segment linear from 0 to MAX_i
  - MAX_i now a multiple of page size
- Physical address space: non-contiguous set of frames
**** Virtual Address
- Virtual Address:
  - Segment number s
  - Offset with segment d
- Virtual address for mapping
  - Segment number s
  - Page number within segment p
  - Offset within page d'
**** MMU
- STBR: points to segment table in memory  
- STLR: length of segment table   
- Segment table:  
  - Indexed by segment number  
  - Contains page table base, segment length   
- Page table for each segment:  
  - Index by page number  
  - Contains frameno of frame that contains page
[[file:files/Lecture 5/screenshot_2017-03-22_14-26-05.png]]

Here we need
- 1 segment table per process
- We need 1 page table per valid segment of a process
- Tables are located
** Typical Virtual Address Space
Linear from 0 up to a multiple of page size, true, but address space is often
sparsely used.

Problem: Access to unused portion will appear valid, would prefer to have an
error
*** Solution
- Abandon PTLR  
- Page table has length 2^p  
- Instead, have valid bit in each PTE  
  - Set to valid for used portions of address space  
  - Invalid for unused portions  
- This is the common approach
** Main Memory Allocation: processes
We want as many processes in memory, in order to be able to quickly switch on
I/O of running process.

Question: how to find memory for a newly arrived process?
*** Base and Bounds
- Main memory:
  - Regions in use
  - "Holes"
- New process needs to go in "hole"
- Which hole to pick?
**** Dynamic Memory Allocation Methods
 - First-fit  
   - Take first hole bigger than requested  
   - Easy to find  
 - Best-fit  
   - Take smallest hole bigger than requested  
   - Leaves smallest hole behind  
 - Worst-fit?!  
   - Takes largest hole  
   - Leaves biggest hole behind  
**** (External) Fragmentation
 - Small holes become unusable
 - Part of memory cannot be used
 - Serious problem
*** Segmentation
- Problem is similar  
  - Dynamic memory allocation  
  - Pieces are typically smaller  
  - But there are more (than 1) pieces  
- Easier problem  
  - External fragmentation smaller  
No place wasted inside a segment.
*** Paging
- Logical address space: fixed size pages  
  - Physical address space: fixed size frames  
- New process:  
  - Find frames for all of process’s pages  
- Easier problem  
  - Fixed size
No need to keep a list of holes with size, only need to keep location of holes,
already at the right size.

No external fragmentation, but internal fragmentation, since fixed size, par of
last page may be unused, not a big problem with reasonable page size.
** Swapping
What if we are out of memory?
- We need to get rid of one or more process
- Store them temporarily on disk
- Called swapping
- /!\ Whole process, not part of it
Process switch to a swapped process?
- Latency can be very high
- Need to read image from disk...
- Better solution = demand paging, not all of a process needs to be in memory
** Finer-Grain Protection
Different protection for different parts of address space
- Valid bit in page table
- May also have valid bit in segment table
- May also have other bits in page/segment table
  - Read-only / Read-write
  - Executable / not-executable

For instance, code should be valid, read-only and executable:
- Base and bounds: not really possible
- Segmentation: Set those bits in segment table
- Paging: Set those bit in every code page

- Easier to do with segmentation
- Segments correspond to logical entities
- Typically have the same protection attributes
** Sharing Memory between Processes
*** Why?
- Run twice the same program in different processes, may want to share code
- Read twice the same file in different processes, may want to share memory
  corresponding to file
*** How?
Processes occasionally share memory
- With base and bounds, not possible
- With segmentation
  - Create segment for shared data
  - Entry in segment table of both processes
  - Points to shared segment in memory
- With paging
  - Need to share pages
  - Entries in page table of both processes
  - Point to shared pages
*** Advantages/Disadvantages
Easier to do with segmentation
|                       | Segmentation | Paging | Segmentation with Paging |
|-----------------------+--------------+--------+--------------------------|
| Sharing               | X            |        | X                        |
| Fine-grain protection | X            |        | X                        |
| Memory allocation     |              | X      | X                        |
*** In Reality
- Based-and-bounds only for niche
- Segmentation abandoned
  - Complexity for little gain
  - Effect approximated with paging with valid bits
- Paging is now universal
** Translation Lookaside Buffer (TLB)
Solves performance issue, since page table in memory, 1 virtual address \to 2
physical memory accesses, reduces performance by 2.

TLB:
- Small fast cache of (pageno,frameno) maps
- If mapping for pageno found in TLB
  - Use frameno from TLB
  - Abort mapping using page table
- If not
  - Perform mapping using page table
  - Insert (pageno,frameno) in TLB

#+DOWNLOADED: /tmp/screenshot.png @ 2017-04-18 14:31:00
[[file:files/Lecture 5/screenshot_2017-04-18_14-31-00.png]]

*** How to make TLB fast?
- Use associative memory (special HW)
- Regular memory: lookup by address
- Associative memory: lookup by contents
*** TLB Size
- Associative memory very expensive
- Therefore, TLB small (64-1024 entries)
- If TLB full, need to replace existing entry
* Week 6
** Large Virtual Address Spaces
- 64-bit virtual address space 
- 4kB pages (12-bit page offset) 
- Leaves 52 bits for pageno 
- Would require 2^52 page table entries 
- Let’s say every page table entry 4 bytes 
- Page table size = 2^54 bytes 
- More than main memory! 
*** Single vs two level page tables
- Single: Virtual address is broken in two parts, one for page, other for offset
- Two level, first part is broken in two parts
*** MMU for Two-level Page Tables
- PTBR: points to top-level page table 
- Top-level page table entry: 
- Indexed by p1 
- Pointer to second-level page table 
- Valid bit 
- Second-level page table entry: 
- Indexed by p2 
- Frameno containing page (p1,p2) 
- Valid bit 

#+DOWNLOADED: /tmp/screenshot.png @ 2017-04-18 14:39:31
[[file:files/Week 6/screenshot_2017-04-18_14-39-31.png]]
*** Memory Use 
- most address spaces are sparsely populated
-One-level: need page table for entire address space
- Two-level:
  - need top-level page table for entire address space
  - Need only second-level page tables for populated parts of the address space
*** Example
- Virtual address – 32 bits 
- Only low 20Mb and upper 2Mb valid 
- Page size – 4k or d = 12 bits, so p = 20 bits 
- 1-level page table: 2^20 = 1M PTEs 
- 2-level page table (P1 = 8, P2 = 12) 
  - 2^8 for 1 st  level 
  - 2 x 2^12 + 1 x 2^12 for 2 nd  level 
  - ~12K PTEs 
*** Dense Address spaces
Two level: not useful, even counter-productive, but most address spaces are
sparse
*** Two-level enough?
- Second-level page table fits in a page 
- Easy to allocate 
- Let’s assume 
  - 4k pages 
  - 4 bytes per PTE 
- It follows: d = 12, p2 = 10 
- Two level:
  - p1 = 42
  - top level = 2^42 entries
- 3-level: p1 = 32, p2 = 10, p3 = 10
- 4-level: p1 = 22, p2 = 10, p3 = 10, p4 = 10
*** Cost of more levels
- Each level adds another memory access 
- N-level page table: 1 memory access \to n+1 memory accesses 
- But, TLB still works 
  - If hit, 1 memory access \to 1 memory accesses 
  - If miss, 1 memory access  \to  n+1 memory accesses 
- TLB hit rate must be very high (99+ %) 
** Process Switching and Memory Management
*** Memory  Mapping Info
When process switching, need to save and restore PC and registers, but also
needs to restore memory mapping information.

- Base and bounds: base and limit integer
- Segmentation: STBR and STLR
- Paging: PTBR and PTLR
(Need not save/restore segment and page table, already in memory)
*** Process switching and TLB
**** Issue
- Suppose 
  - Process P1 is running 
  - Entry (pageno, frameno) in TLB 
  - Switch from P1 to P2 
  - P2 issues virtual address in page pageno 
- P2 accesses P1’s memory!
**** Solution 1
On process switch, invalidate all TLB entries 
- Simply requires invalid bit in each TLB entry 
- Makes process switch expensive 
- New process initially incurs 100% TLB misses 
**** Solution 2
- Have process identifier in TLB entries 
  - Match = match on pid and match on pageno 
  - Makes TLB more complicated and expensive 
- Process switch  
  - Nothing to do 
  - Cheaper 
- All modern machines have this feature 
* Week 7
** Demand Paging
*** Introduction
- Virtual address space > physical address space, large because we don't want to
  worry about running out
Other benefits:
- Shorter process startup latency 
  - Can start process without all of it in memory 
  - Even 1 page suffices 
- Better use of main memory
  - Program often do not use certain parts, E.g., error
    handling routines 
- Program often goes through different parts, E.g., initialization, computation,
  termination
If the program is not in memory, where?
- Part in memory
- (Typically) all of it on disk
- Different from swapping, where all program on disk or all on memory
Also
- CPU can only directly access memory
- CPU can only access data on disk through OS
*** Process
- If program accesses part only on disk \to *page fault*
- Program is suspended
- OS runs, get page from disk
- Program is restarted \to *page fault handling*
*** Discover page fault
- Use the valid bit in page table 
- Without demand paging: 
  - Valid bit = 0: page is invalid 
  - Valid bit = 1: page is valid 
- With demand paging 
  - Valid bit = 0: page is invalid OR page is on disk 
  - Valid bit = 1: page is valid AND page is in memory 
- OS needs additional table: invalid / on-disk? 

#+DOWNLOADED: /tmp/screenshot.png @ 2017-04-19 08:29:15
[[file:files/Week 7/screenshot_2017-04-19_08-29-15.png]]
*** Suspending the faulting process
- Invalid bit access generates a trap
- As before, save process information in PCB
*** Getting the Page from Disk
- Assume there is at least one free frame 
- Allocate a free frame to process 
- Find page on disk, Note: need an extra table for that 
- Get disk to transfer page from disk to frame 
*** While the Disk is busy
- Invoke scheduler to run another process 
- When disk interrupt arrives 
  - Suspend running process 
  - Get back to page fault handling
*** Completing Page Fault Handling	
- Pagetable[pageno].frameno = new frameno 
- Pagetable[pageno].valid = 1 
- Set process state to ready 
- Invoke scheduler
*** When Process Runs Again
- Restarts the previously faulting instruction 
- Now finds  
  - Valid bit to be set to 1 
  - Page in corresponding frame in memory	
** Page Replacement Policy  
If no free frame available
- Pick a frame to be replaced 
- Invalidate its page table entry (and TLB entry) 
- You may have to write that frame to disk 
- Page table has a modified bit 
  - If set, write out page to disk 
  - If not, proceed with page fault handling
*** Page Faults and Performance
- Normal memory access: ~ nanoseconds 
- Faulting memory access: Disk i/o ~ 10 milliseconds 
- Too many page faults \to program very slow 
- Hence, importance of good page replacement 
In general, prefer replacing clean over dirty, 1 disk i/o instead of 2
*** Policy Evaluation
- Pick a “reference string” of page accesses 
- Pick a certain number of frames 
- Count the number of page faults 
  - When simulating that reference string 
  - On a machine with that number of frames 
  - Using the specific page replacement policy 
*** FIFO
- Oldest page is replaced: Age = Time since brought into memory  
- Easy to implement 
- Keep a queue of pages 
- Bring in a page: stick at the end of the queue 
- Need replacement: pick head of queue 
[[file:files/Week 7/screenshot_2017-04-19_08-43-06.png]]
12 page faults (not counting initial paging in)
*** OPT: Optimal Algorithm
- Cannot be implemented in general (need to predict the future)
- Basis of comparison for others algorithms
Replace the page that will be referenced the furthest in the future.
[[file:files/Week 7/screenshot_2017-04-19_08-45-06.png]] 
6 page faults (not counting initial paging in)
*** LRU: Least Recently Used
[[file:files/Week 7/screenshot_2017-04-19_08-47-29.png]]
9 page faults (not counting initial paging in)
**** Implementation
- Can also not be implemented in general 
- Need to timestamp every memory reference 
- Too expensive 
- But can be (well) approximated 
**** Approximation
Reference bit = bit in page table, HW sets bit when page is referenced.

- Pick on of the page with reference bit set
- Reset all references bits to zero

Better approximation
- Periodically 
  - Read out and store all reference bits 
  - Reset all reference bits to zero 
- Keep all reference bits for some time 
- The more bits kept, the better approximation 
- Replacement: Page with smallest value of reference bit history 
*** FIFO with Second Chance
- As FIFO, but each page gets a second chance 
- Bring in page: put at tail of queue (as before) 
- Replacement: 
  - Look at head of queue 
  - If reference bit is 0, replace 
  - While reference bit is 1 /* give second chance */ 
    - Put at tail of queue 
    - Set reference bit to 0 
    - Look at head of queue
**** Advantage 1
- Combination of 
  - FIFO 
  - LRU approximation with one reference bit 
- In LRU approximation with one reference bit: take any page with reference bit
  0 
- Here: Take “oldest” page with reference bit 0 
**** Advantage 2
- Combination of 
  - FIFO 
  - LRU approximation with one reference bit 
- In LRU approximation with one reference bit: Set all ref bits to 0 
- Here: Set ref bits of “old” pages to 0 
*** Clock
- Imagine pages arranged around a clock 
- Replacement: 
  - Look at page where hand of clock is 
  - If reference bit = 0, replace 
  - If reference bit = 1 
    - Set reference bit to 0 
    - Move hand of clock to next page 
  - Insert new page where old one was replaced 

#+DOWNLOADED: /tmp/screenshot.png @ 2017-04-19 08:54:40
[[file:files/Week 7/screenshot_2017-04-19_08-54-40.png]]
*** Clock vs FIFO + Second Chance
- The two are the same policy: The clock points points to the head of the queue
- Clock is more efficient 
  - Doesn’t need to move pages around in queue 
  - Instead, clock hand moves 
- Works well + efficient to implement: Variations used in many systems (e.g.,
  Linux) 
*** Comparisons of Replacement Policies
| Policy               | Implementation        | Performance                 |
|----------------------+-----------------------+-----------------------------|
| Random               | Easy                  | Poor                        |
| FIFO                 | Easy (queue)          | Poor                        |
| OPT                  | Impossible            | Optimal                     |
| LRU                  | Difficult (timestamp) | Good                        |
| LRU approximation    | Moderate (ref bits)   | Good (depending on approx.) |
| FIFO with 2nd Chance | Easy                  | Good                        |
| Clock                | Easy and efficient    | Good                        |
** Frame Allocation
How many frame to give to each process?
*** Degree of Multiprocessing
- How many processes to keep in memory? 
- Without demand paging: 
  - All of process must be in memory 
  - Severely limits degree of multiprocessing 
- With demand paging: 
  - Only part of process must be in memory 
  - Can achieve high degree of multiprocessing
*** Link between deg of multproc and page fault rate
- Give each process frames for  ~ all of its pages 
  - Low degree of multiprocessing 
  - Few page faults 
  - Slow switching on i/o 
- Give each process 1 frame 
  - High degree of multiprocessing 
  - Many page faults ( thrashing) 
  - Quick switching on i/o 
- Where is the correct tradeoff? 
*** Working Set of a Process
- Set of pages of process needed for execution 
- Intuition:  
  - Working set not in memory  \to  many page faults 
  - Working set in memory  \to  no page faults 
  - More than working set in memory  \to  no gain 
Working set < all pages?
- Principle of locality
- In given time interval:
  - Process only access part of its pages
  - Example: intialization, main, termination, error, ...
*** Tradeoff
| Frame allocation | Page Faults | Degree of Multiprocessing |
|------------------+-------------+---------------------------|
| All frames       | None        | Low                       |
| 1 frame          | Many        | High                      |
| Working Set      | Few         | Moderate                  |
*** Frame Allocation Policy
- Give each process enough frames to maintain its working set in memory 
- If sum of all working sets > memory, swap out one or more processes 
- If sum of all working sets < memory, swap in one or more processes 
*** How to Predict Working Set?
- Working set for next 10’000 refs = working set for last 10’000 refs 
- Prediction is not perfect 
  - Phase change (e.g., from initialization to main) 
  - Will decrease during next phase     
  - Will cause (temporary) high page fault rate 
**** Past Working Set measurement
- We do not really need working set 
- We only need working set size 
- Periodically (every 10’000 references) 
  - Count reference bits set to 1 
  - Set all references bits to 0 
- Working set (for last 10’000 references) = number of reference bits set to 1 
*** Page Replacement vs Frame Allocation
- Frame allocation done periodically 
- Page replacement done at page fault time
*** Global vs Local Replacement
- Local replacement: replace page of faulting process 
- Global replacement: replace any page 
- Done according to page replacement 
- E.g., FIFO: 
  - Local: replace oldest page of faulting process 
  - Global: pick the oldest page overall 
*** Tradeoff: Local - Global
- Assume using working set for frame allocation 
- Local: 
  - You cannot affect anyone else’s working set 
  - Hence, you cannot cause thrashing of others 
  - But inflexible for yourself 
  - If working set grows, cannot react 
  - Hence, you can cause yourself to trash
- Global: 
  - You can affect others’ working set 
  - Hence, you cause thrashing of others 
  - But flexible for yourself 
  - If working set grows, can react 
  - Hence, you can avoid yourself to trash  
- Use frame allocation periodically 
- Use local replacement in-between 
- You may trash for a short time 
- But at next period get bigger allocation, so OK 
- Cannot cause thrashing of others, so OK 
*** Frame Allocation Driving Page Replacement
- Periodically run working set computation 
- If allocation > working set 
- Immediately replace non-WS pages 
- Or favor them during later replacement 
- Algorithm: WSClock does this (not covered) 
** Optimization
*** Prepaging
- So far: page in 1 page at time 
- Prepaging: page in multiple pages at a time 
- Usually, pages “surrounding” faulting page
**** Performance
- Relies on locality of virtual memory access, nearby pages are often accessed
  soon after
- Avoids page faults, process switches, .. 
- Can also get better disk performance (later) 
- May bring in unnecessary pages 
*** Cleaning
- So far: prefer to replace “clean” pages 
- Cleaning: disk idle, write out “dirty” pages
**** Performance
- More “clean” pages at replacement time: quicker replacement  
- But page may be modified again: useless disk traffic 
*** Free Frame Pool
- So far: use all possible frames for pages 
- Free pool: keep some frames unused
**** Performance
- Page fault handling is quick 
- Reduces effective main memory size 
*** Copy -on-Write
- Clever trick for sharing pages between processes 
  - That are initially the same 
  - That are likely to be read-only 
  - But that may (unlikely) be modified by a process 
**** Read-Only Sharing
- Make page table entry point to same frame 
- Set read-only bit so trap if process writes 
- Trap treated as illegal memory access
**** Implementation
- Make page table entry point to same frame 
- Set read-only bit so trap if process writes 
- Fault not treated as illegal memory access 
- Instead: 
  - Create separate frame for faulting process
  - Insert (pageno, frameno) in page table  
  - Set read-only bit to off (i.e., read-write) 
  - Copy page into that frame 
- Further accesses will not page fault 
**** Performance
- Works well if page is rarely written, save frames (as many as sharers - 1)
- Works poorly if page is often written, more page faults, don't gain in frame
  occupation
**** In practice
- The conditions under which copy-on-write works well occur in practice
- All OS’s provide it 
- This is how Linux implements fork() 
* Week 8
** Files and type
*** File
Un-interpreted collection of objects
- File System does not know what data means
- Only application knows
- Objects = byte, records, ...
*** Type or untyped
Typed = FS knows what the object means

Advantages:
- Invoke certain programs by default
- Prevent errors
- More efficient storage
Disadvantages
- Can be inflexible (typecast)
- Can become a lot of code (many types)
We look at untyped
** File System Primitives
*** Acesss
**** Create() and Delete()
- uid = Create([optional arguments])
  - /uid/ unique identifier, not human-readable string
  - Creates an empty file
- Delete(uid)
  - Deletes file with identifier /uid/
  - Usually also deletes all of its content
**** Read()
Read(uid, buffer, from, to)
- Reads from file with identifier /uid/
- From byte /from/ to byte /to/ \rightarrow can cause EOF condition
- Into a memory buffer /buffer/
  - previously allocated
  - must be of sufficient size
**** Write()
Write(uid, buffer, from to)
- Write to file with identifier /uid/
- Into byte /from/ to byte /to/
- From a memory buffer /buffer/
**** Sequential vs Random Access
- Read() and Write() = Random-access primitives, no connection between two
  successive accesses
- Sequential access is very common
  - Read from where you stopped reading
  - Write to where you stopped writing
  - In particular, whole file access is common
- We thus need sequential access methods
**** Sequential Read()
- File system keeps file pointer /fp/ (initially 0)
- Read(uid, buffer, bytes)
  - Read from file with unique identifier /uid/
  - Starting from byte /fp/
  - /Bytes/ bytes
  - Into memory buffer /buffer/
  - /fp/ += /bytes/
**** Sequential from random
- Maintain /fp/-equivalent in user code
- /myfp/ = 0
- Read(uid, buffer, myfp, myfp+bytes-1)
- /myfp/ += /bytes/
- Read(uid, buffer, myfp, myfp+bytes-1)
**** Random from sequential
- Need to have another primitive Seek(uid, to) (fp = to)
- Implementation
  - Read(uid, from, to, buffer)
  - Seek(uid, from)
  - Read(uid, buffer, to-from+1)
**** Random vs Sequential
- Sequential access is very common
- All systems provide sequential access
- Some systems provide only sequential access + Seek()
*** Concurrency
Two processes access the same file, what about /fp/?
**** Open a file with Open() and Close()
- tid = Open(uid, [optional args])
  - Creates an instance of file with /uid/
  - Accessible by this process only
  - With the temporary process-unique id /tid/
  - /fp/ is associated with /tid/, not with /uid/
- Close(tid) \rightarrow destroys the instance
**** Open() with Read()
- tid = Open()
- Read(tid, buffer, bytes)
- Other Read()s or Write()s
- ...
- Close(tid)
**** Semantic of Concurrent Open()s
- Separate instances altogether \rightarrow Write()s by one not visible to
  others
- Separate instances until Close() \rightarrow Write()s visible after Close()
- One single instance of the file \rightarrow Write()s visible immediately to
  others
- /fp/ is private!
*** Naming
- Naming = mapping, from human-readable string \to uid
- Directory = collection of such mappings
**** Directory Structures
- Flat
- Two-level: [user] filename
- Hierarchical: /a/b/c/...
  - Root directory
  - Working directory
**** Naming primitives
- Insert(string, uid)
- uid = Lookup(string)
- Remove(string, uid)
**** Directory Primitives
- CreateDirectory (string)
- DeleteDirectory(string)
- SetWorkingDirectory(string)
- string = ListWorkingDirectory()
- List(directory)
**** Hierarchical Directory Structures
- Tree
- (Acyclic) Graph \to allows sharing of two /uids/ under different names
**** Hard Link
- Assume mapping (string1, uid) already exists
- Hardlink(string2, uid)
- Insert(string2, uid)
- After Hardlink, two mappings are equivalent
**** Soft Link
- Assume mapping (string1, uid) exists
- SoftLink(string2, string1)
- Insert(string2, string1)
- After Softlink, two mappings are different
**** Hard/Soft Link differences
After remove(String1, uid), in the case of a hard link, the mapping (string2,
uid) remains, in the case of a soft link, the mapping (string2, string1) is a
dangling reference.

- HardLink is a mapping to a *file*
- SoftLink is a mapping to a *string*
**** Acyclic Graph
Why?
- (Later) disk storage reclamation by refcounts
- Cycles cause waster disk space
How?
- Soft links cannot make cycles
- Hard links can make cycles
- Do not allow hard links to directories, only to leafs in the graph
*** Linux Primitives
- Collapses in a single interface access, concurrency and naming
- Creat(string)
   - uid = Create()
   - Insert(string, uid)
- fd = Open(string, [optional args])
  - uid = Lookup(string)
  - fd = (tid =) Open(uid, [optional args])
- ...
- /uid/ is never visible at the user level
** Disk terminology
[[file:files/Week 8/screenshot_2017-05-21_11-18-21.png]]
** Disk Interface
- Accessible by sector only
- ReadSector( logical_sector_number, buffer)
- WriteSector( logical_sector_number, buffer)
- Logival_sectore_number = 
  - Platter
  - Cylinder or track
  - Sector
Main task of the file system is to translate from user interface methods
(Read(uid...)) to disk interface methods (ReadSector(logical_sector_number...))
*** Simplifications
1. User Read() allows arbitrary number of bytes
  - Simply to only allowing Read() of a block (Read(uid, block_number))
  - A block is fixed-size
2. Block size = sector size
  - Typically, block size = 2^n * sector size
  - For example, block size = 4096B, sector size = 512B
Simplifications are easily implemented in the library
** Disk Access
Disk access time:
- Head selection, selects a platter
  - Electronic switch
  - Nanoseconds
- Seek, move an arm over cylinder
  - Approximately linear in the number of cylinders
  - 3-12 ms
- Rotational latency, move head over sector
  - Linear in the number of sectors
  - Rotational speed: 4500-15000 RPM
  - One revolution = 1/(RPM/60) seconds
  - Average rotational latency = 1/2 revolution
  - From 2 to 7.1 ms
- Transfer time, read from sector
  - Effective transfer rate ~ 1GB/s
  - Sector = 512B
  - ~ 0.5 \mu s 
| Component           | Time              |
|---------------------+-------------------|
| Head Selection      | nanoseconds       |
| Seek Time           | 3-12 milliseconds |
| Rotational latency  | 2-7 milliseconds  |
| Transfer Time       | microseconds      |
| Controller Overhead | < 1 milliseconds  |
- Disk access time >> memory access time
- Seek time dominates
- Followed by rotational latency 
*** Optimization: File System Cache (Buffer Cache)
- Rule 1: Do not access disk
- use a cache

**** FS cache
- Keeps recently accessed blocks in memory
- reduces latency and disk load
- Need to reverse kernel memory for cache, cache entries = files blocks (of
  block size)

**** Read with a Cache
- If in cache, return data from cache
- If not
  - Find free cache slot
  - Initiate disk read
  - When disk read completes, return data
**** Write with a Cache
 - Always write in cache
 - Write-Through = write directly to disk
 - Write-behind = write later to disk
 Write-Through vs Write-Behind
 - Response time: write-behind is (much) better
 - Disk load
   - Write-behind is (much) better
   - Much data overwritten before it gets tot disk
 - Crash
   - Write through is much better
   - no "window of vulnerability"
 In practice
 - Write-behind
 - Periodic cache flush
 - User primitive to flush data
*** Optimization: Read-Ahead
- Rule 2: Don't wait for disk
- Read-ahead (or prefetching)
- Only for sequential access)
**** Read-Ahead
- User request for block i of a file, also read block i+1 form disk
- Need to put block i+1 in the buffer cache
- This way, no disk I/O on (expected) user access to block i+1
- Works for sequential access (most access is), in Linux = default
**** Caveat
- Does not reduce number of disk I/Os
- In fact, could increase them (if not sequential)
- In practice, very often a win, Linux always reads one block ahead
*** Optimization: minimize seeks
**** Clever disk allocation
 Locate related data (same file) on same cylinder
 - allocate "related" blocks "together"
 - "together" = on the same/nearby cylinder
 - "related" = consecutive blocks in the same file, seq. access
**** Clever scheduling
 Reorder requests to seek as little as possible \to also minimize seeks
- FCFS: next request in queue
- SSTF: pick "nearest" request in queue
  - Very good seek times
  - Subject to starvation, request on inside or outside can get starved
- SCAN
  - Continue moving head in one direcion
    - From 0 to MAX_CYL
    - Then, from MAX_CYL to 0
  - Pick requests as you move head
- C-SCAN
  - Similar to SCAN
  - Always move head
    - From 0 to MAX_CYL; pick up requests as head moves
    - From MAX_CYL to 0; no request served 
  - Number of cylinders slightly higher
  - More uniform wait time
- C-LOOK
  - Similar to C-SCAN
  - Always move head
    - From min_cyl to max_cyl; serve requests as head moves
    - From max_cyl to min_cyl; no requests served
In practice, variation of C-LOOK
*** Optimization: Avoid rotational latency
- Clever disk allocation
- Locate consecutive blocks of file on consecutive sectors in a cylinder
*** When does what works well?
**** High load
Disk scheduling works well
- Many scheduling opportunities, many requests in the queue
- Allocation gets defeated by interleaved requests for different files
**** Low load
Clever allocation works well
- Not much scheduling opportunity, not many requests in the queue
- Sequential user access \to sequential disk access
- Cache tends to reduce load
*** Summary
| Optimization    | Goal                              |
|-----------------+-----------------------------------|
| Caching         | Avoid disk access                 |
| Read-ahead      | Avoid waiting for disk            |
| Disk allocation | Avoid seek and rotational latency |
| Disk Scheduling | Avoid seek                        |

- Disks are random access devices
- But, sequential access >> random access (Bandwidth 100x greater for
  sequential)
- Application should aim for sequential access
- System should aim for sequential access
* Week 9
** Disk vs In. Memory
Simple rule: If not on disk and crash, it's gone.
** Disk Data Structures
*** Boot block
- At fixed location on disk (usually sector 0)
- Contains boot loader
- Read on machine boot
*** Device Directory
- Fixed, reserved area on disk
- Array of records (device directory entry or DDE)
- Indexed by /uid/
- Record contains:
  - In-use bit (more generally, reference count)
  - Size
  - Other info: access rights, etc.
  - Disk address(es) pointing to file data
*** User data Allocation
**** Contiguous allocation
- Disk data blocks contiguous on disk
- Need only 1 pointer in device directory entry
- Creates disk fragmentation (many un-usable holes)
\Rightarrow Impractical
**** Linked List Allocation
- Each data block contains pointer to the next
- Only 1 pointer in device directory entry (head)
- 2 If you want to store the tail
- Inefficient access (esp. random access)
- Pointer space in data block 
**** Indexed Allocation
- N pointers in device directory entry
- Point to data blocks of the file
- Efficient access for small files
- What if file size > N*size of data block?
**** Indexed Allocation with Indirect Blocks
- N pointers in device directory entry
- First M (<N) point to first M data blocks
- Blocks M+1 to N point to /indirect blocks/
/Indirect blocks/ do not contain data, but pointers to subsequent data blocks
(double indirect blocks also possible)
- Efficient for small files
- Possible to extend to very large files
**** Extent-Based Allocation
- Device directory entry
  - Contains disk address and length of extent
  - Instead of just disk address
  - In other words, point to a sequence of disk blocks
- Good sequential and random access
- Can be combined with indirect blocks
- Common practice in Linux
*** Free space
- Linked list
- Bitmap
  - Array[#numsectors]
  - Free/In-use
** In-Memory Data Structures
*** Cache
- Fixed contiguous area of kernel memory
- Size = max number of cache blocks x block size
- A large chunk of memory of the machine
*** Cache directory
- Usually a hash table
- index = hash(disk address)
- With an overflow list in case of collision
- Usually has a 'dirty' bit
*** Active file table
- One array for the entire system (system-wide)
- One entry per /open file/
- Each entry contains
  - Device directory of file
  - Additional info (e.g. refcount of number of file opens)
*** Open file tables
- One array per process
- One entry per /file open/ of that process
- Indexed by file descriptor /fd/
- Each entry contains
  - Pointer to entry in active file table
  - File pointer /fp/
  - Additional info
** Putting it All Together
- File systems main methods
  - Create(), Delete()
  - Open(), Close()
  - Read(), Write(), Lseek()
  - Cache flush and replacement
- With some major simplifications
  - No access permission checks
  - No return value checks
  - etc...
*** uid = Create()
- Find a free uid (refcount = 0)
- Set refcount to 1
- Fill in additional info
- Write back to cache (and to disk)
- Device directory is cached in memory
- Usually easy to find free uid
*** Delete(uid)
- Find inode
- Decrement refcount
- If refcount == 0
  - Free all data blocks and indirect blocks
  - Set entries in free space bitmap to 0
- Write back to cache (and to disk)
*** tid = Open(uid)
- Check in Active File Table if uid already open
- If so
  - Refcount in Active File Table ++
  - Allocate entry in Open File Table
  - Point to entry in Active File Table
  - Set fp = 0
- If not
  - Find free entry in Active File Table
  - Read inode and copy in Active File Table entry
  - Refcount = 0
  - Allocate entry in Open File Table
  - Point to entry in Active File Table
  - Set fp = 0
*** Close(tid)
- Find entry in Active File Table
- Refcount --
- If refcount  == 0, remove entry from Active File Table
- Remove entry from Open File Table
*** Read()
- find fp in open file table and increment
- compute block not to be read
- Find disk address in inode in Active File Table
- Look up in Cache Directory (disk address)
- If present, return
- If not, find free entry in cache
- ReadSector(disk address, free cache block)
*** Write
- find fp in open file table and increment
- compute block not to be written
- Find/allocate disk address in Active File Table
- Look up in cache directory (disk address)
- If present, overwrite and return
- If not, find free cache entry and overwrite
*** Lseek(tid, new_fp)
- In Open File Table, set fp = new_fp
*** Cache Replacement
- Keep LRU list
  - Unlike memory management, here easy to do
  - Accesses are far fewer (file vs memory access)
- If no more free entries in the cache
  - Replace 'clean' block according to LRU
  - Replace 'dirty' block according to LRU
*** Cache Flush
- Find "dirty" entries in cache
- WriteSector(...)
- Periodically (30 seconds)
- When disk is idle
*** Directories
Directories are stored as files
*** Typical Operation
- fd = Open(string)
  - Directory lookup (disk reads)
  - Inode lookups (disk reads)
- Read(fd, ...)
  - Data (disk reads)
*** Disk Behavior
Head moves between
- Directories
- Inodes
- Data
*** Advanced Disk Layout
- Co-locate related
  - Directories
  - Inodes
  - Data
- In same "cylinder group", set of cylinders next to each others
** Loose ends
*** File System Check
- Normally, nothing would be necessary
- Sometimes things are not normal
  - Disk sector goes bad
  - File system software has bugs
  - ...
- Common to "check the file system (fsck)
Checks:
- No sectors are allocated twice
- No sectors are allocated and on free list
- Reconstruct free list
*** Replication
- Some key sectors are replicated
  - Boot blocks
  - Sometimes also device directory
*** Disk Fragmentation
- Free space consists of
  - Small "holes" (of 1 or 2 sectors)
  - Spread all over the disk
- Happens even with good disk allocation
- No longer possible to do good disk allocation
**** Disk Defragmentation Utility
- Takes the file system offline
- Moves files into contiguous locations
- Better performance
- More room for good disk allocation
- Can be done online, but tricky
** Memory Mapping
- mmap() \to map the contents of a file in memory
- munmap() \to remove the mapping
mmap region = between heap and stack in virtual address space, interesting to
have 64bit address space, since the mmap region is now huge.
*** Access to mmap()-ed files
- Access to memory region mmap()-ed
- Causes page fault
- Causes page/block of file to be brought in
*** Implementation
- On mmap()
  - Allocate page table entries
  - Set valid bit to "invalid"
- On access
  - Page fault
  - File = backing store for mapped region of memory
  - Just like in demand paging
  - Except paged from mapped file
- After page fault handling, set valid bit to true
Getting data to disk from mmap, through normal page replacement or through an
explicit call to msync()
*** What is mmap() good for?
**** Random access to large file
- addr = mmap()
- Use memory addresses in [addr, addr+len-1]
- Easy to write
- Only bring in memory what you read
- Easy reuse
**** Random access with Read() interface
- Open
- Read entire file into memory buffer
- Then use memory address in buffer
Advantages with mmap():
- Only accessed portions brought in memory
- Huge advantage for large files or sparsely accessed files.
**** Random access with LSeek()
- Open
- LSeek
- Read into Buffer
- LSeek
- Read into Buffer
Advantages with mmap():
- Much easier programming model
  - Follow pointer in memory
  - As opposed to (Lseek,Read) every time
- Easier if reuse
  - VM system keeps page for you
  - Otherwise, have to do your own replacement
*** Issues with mmap()
- Alignment on page boundary
- not easy to extend a file
- For small files \to Read() more efficient than mmap() + page fault
*** Using mmap() for memory sharing
- Sharing memory between processes
- A form of interprocess communication
- Use shared and anonymous map flags
**** Implementation
- File system has buffer cache, file data on disk, recently used data in memory
- Memory management has page replacement, data in memory, not recently used data
  on disk
- Same thing, but from an opposite angle
**** Integrated Buffer Cache
- One region of memory
- Used both as
  - File system buffer cache
  - Demand paged in-memory data
- Advantage:
  - One piece of code instead of two
  - Avoids "double caching"
* Week 10
** Atomicity
- Atomicity = "all or nothings"
- In a file system:
  - All updates are on disk
  - No updates are on disk
  - Nothing in between
*** Assumption
A single disk write is atomic
*** Implementation
- Make sure we have an old copy on disk
- Make sure we have a new copy on disk
- Switch atomically between the two
*** Atomic Switch with DDE
 - We do a WriteSector()
 - What to write with it?
   - Device directory entry!
   - Because it is smaller than sector
**** How it works with Write-Through
 - open(): read dde into aft
 - write()s
   - allocate *new* blocks on disk for data
   - fill in address of new blocks in /incore/ dde
   - write to cache and disk
 - close() write /incore/ dde to /disk/ dde
**** How it works with Write-Behind
 - open(): read dde into aft
 - write()s
   - allocate *new* blocks for new data
   - Write disk addresses to /incore/ dde
   - write to cache
 - close()
   - Write *all* cached block to new disk blocks
   - write /incore/ dde to /disk/ dde
**** What happens to old blocks?
 - De-allocate them
 - If crash before de-allocate, file system check

*** Atomic Switch with Intentions Log
 - Reserve an area of disk for (intentions) log
 - During normal operations
   - On write
     - Write to cache
     - Write to log
     - Make in-memory inode point to update in log
   - On Close
     - Write old and new inode to log in one disk write
     - Copy updates from log to original disk locations
     - When all updates done, overwrite inode with new value
     - Remove updates and old and new inode from the log
**** Crash Recovery
 - Search forward through log
 - For each new inode found
   - Find and copy updates to their original location
   - If when all updates are done, write new inode
   - Remove updates and old and new inode from log
**** Invariant
 - If new inode in the log and crash: new copy
 - If new inode not in the log and crash: old copy
 - Even if you crash during crash recovery, you may copy and update multiple
   times
*** Comparisons
 Count the number of (Random) disk I/Os:
 - DDE one write per write()/close()
 - Log: two write per write()(log,data), one per close()
 However, log works better
 - Write()'s to log are sequential (no seeks)
 - Data blocks stay in place
 - Good disk allocation stays!
 - Write from cache of log to data - when disk is idle or cache replacement
 DDE works less well:
 - Disk allocation gets messed up
 - Fragmentation
** Log-Structured File System (LFS)
- Alternative way of structuring file system
- Takes idea of log writes to the extreme
*** Rationale
- Large memories \to large buffer caches
- Most reads served from cache
- Most disk traffic is write traffic
- How to optimize disk I/O? By optimizing writes
- How to optimize disk writes? By writing sequentially to disk
*** Key idea
- Log = append-only data structure (on disk)
- /All/ writes are to a log, including data and inode modifications
*** Write() in LFS
- Write first go into cache (write-behind), both inodes and data
- Writes also go into (in-memory) buffer
- When buffer full, append to log
- Called /segment/ of log
- No seeks on writes!
*** Inode Map
Allows to read
- (In-memory) table of inode disk addresses, maps /uid/ to disk address of last
  inode for that /uid/
- Updated every time inode is written to disk
**** Usage
- Open():
  - Get inode address from inode map
  - Read inode from disk into Active File Table
- Read(): as before
  - Get from cache
  - Get from disk address in inode
- Reading seems more complicated because of indirection through inode map
- Performance is determined by disk writes, so little difference
*** Checkpoint
Checkpoint inode map:
- Write copy of inode map to fixed location on disk
- Put marker in the log
**** Crash
 - Start from inode map in checkpoint, contains addresses of all inodes written
   /before/ last checkpoint
 - How to find inodes that were in in-memory inode map before crash, but not
   written in the checkpoint?
**** Roll Forward
- Checkpoint put marker in log
- From marker forward
  - Scan for inodes in the log
  - Add their addresses to inode map
- Result: all inode addresses in inode map before crash are in inode map
  afterwards
**** Time interval
- Too short: lots of disk I/O to write checkpoints
- Too long: long recovery time (forward scan)
- Compromise:
  - Crashes are rare
  - So is recovery
  - Can tolerate longer recovery time
*** Disk if full
- No sector is ever overwritten, always written to end of log
- No sector is every put on free list
- So disk will get fully (quickly)
- Need to "clean" the disk
*** Disk Cleaning
- Reclaim "old" data
- "old" here means
  - Logically overwritten, later write to (uid, blockno)
  - But not physically overwritten, older version of (uid, blockno) somewhere in
    the log
- Done one segment at a time
  - Determine which blocks are new
  - Write them into buffer
  - If buffer is full, write new segment
  - Cleaned segment is marked free
*** Log 
- Log is more complicated than simple linear log
- Log = sequence of segments, some in use, some free
**** Write()
- Rather than append to log, write to free segment in log
- Segments are larges (MBs), still get benefits from sequential access
**** Block age
Change write a bit, instead of just writing data, we write data + uid + blockno

To determine if a block is old:
- For a data block
- Take its disk address
- Take its uid and block no
- Look in inode map and then in inode
- If inode has different disk address \to old
*** Summary
- All writes go to log, including data and inode, but *excluding* checkpoints
- Checkpoint region at fixed disk location on disk, log uses the remainder,
  segments are large contiguous regions on disk
- In the segment
  - data = modified user data sector (includes uid and block no)
  - Inode = modified inode sector
- Cache: regular write-behind buffer cache
- Segment buffer: segment being written
- Inode map:
  - Array
  - Indexed by /uid/
  - Points to last-written inode for /uid/
- In memory, usual (active file table, open file tables)
**** Write()
- Write go into (write-behind) cache, both inodes and data sectors
- Write go into (in-memory) segment buffer, both again
- When segment buffer is full, write to free segment in disk log
- (Almost) no seeks on writes!
- If inode is written to log
- inode_map[uid] = disk address of inode
**** Open()
- Get inode address from inode map
- Read inode from disk into Active File Table
**** Read()
- Get from cache
- If not in cache, get from disk using disk address in inode
As before
**** Conclusion
- Reads mostly from cache
- Writes to disk heavily optimized: few seeks
- Reads from disk: bit more expensive, but few
- Cost of cleaning
- Not become mainstream
* Week 11 
** Disk issues
- Bandwidth:
  - Servers
  - 'Big data' computations
- Response time:
  - Desktops and laptops
  - Transaction systems
** RAID
Redundant Array of Independent Disks

Essential idea:
- Optimize I/O bandwidth through parallel I/O
- Parallel I/O = I/O to multiple disks at once
*** Striping
- Rather than put file on one disk
- Stripe it across a number of disks
  - File = stripe0 stripe1 stripe2
  - stripe0 on disk0
  - stripe1 on disk1
  - ...
- Read and write in parallel
*** Potential gain
- Since disk is the bottleneck
- Bandwidth of RAID with /n/ disks = /n/ * bandwidth of individual disks
- At some points other factors, bandwidth of I/O bus, controller, etc.
- But still, bandwidth of RAID >> bandwidth disk
** RAID format
- Disk now cheap and small
- Many can go into a RAID box
- To OS: RAID box looks like disk
- Also possible: RAID in software
*** Redudancy
Problem with naïve RAID
- One disk fails \to all data unavailable
- MTBF (RAID) = MTBF (disk) / n
- MTBF (disk) ~ 5 years
- MTBF (RAID with 10 disks) ~ 0.5 year
- Not acceptable
Solution: store redundant data on different disks, one disk fails, data still
available

*** RAID-0
- Non-redundant disk array
- Best possible read and write bandwidth
- Failure results in data loss
| D0 | D1 | D2  | D3  |
|----+----+-----+-----|
| s0 | s1 | s2  | s3  |
| s4 | s5 | s6  | s7  |
| s8 | s9 | s10 | s11 |
*** RAID-1
- Mirrored disks
- Write: to data and to mirror disk
- Read: from either data or mirror
- After crash: from surviving disk
Halves the storage capacity with the same number of disk, however survives disk
failure of data or mirror
| D0 | D1 | D2 | D3 |
|----+----+----+----|
| s0 | s1 | s0 | s1 |
| s2 | s3 | s2 | s3 |
| s4 | s5 | s3 | s5 |
*** RAID-4
N data disks + 1 parity disk
| D0 | D1 | D2  | D3  | D4    |
|----+----+-----+-----+-------|
| s0 | s1 | s2  | s3  | p0-3  |
| s4 | s5 | s6  | s7  | p4-7  |
| s8 | s9 | s10 | s11 | p8-11 |

**** Parity
- A simple form of error detection and repair
- Not specific to RAID, also used in communications


- 4 bits: x_0, x_1, x_2, x_3
  - = 0101
- Parity p = x_0 XOR x_1 XOR x_2 XOR x_3 
  - = 0
- if you lose one, say x_2, reconstruct as x_2 = x_0 XOR x_1 XOR x_3 XOR p
  - 0 XOR 1 XOR 1 XOR 0 = 0
**** RAID parity block
- Same idea at the disk block level
- Block on parity disk = XOR of bits of data blocks at same position
**** Usage
- Read: read data disks
- Write: write data disks and parity disk
- Crash: recover from data and parity disk
**** Issue
Every write has to access parity disk, become bottleneck for write-heavy workload
*** RAID-5
- Block interleaved distributed parity
- As RAID-4, but parity distributed over all disks
- Balance parity write load over disks
| D0 | D1 | D2    | D3   | D4   |
|----+----+-------+------+------|
| s0 | s1 | s2    | s3   | p0-3 |
| s4 | s5 | s6    | p4-7 | s7   |
| s8 | s9 | p8-11 | s10  | s11  |
** SSD
- Is not a disk!
- Purely electronic (NAND Flash)
- No moving parts
- Made to look like a disk (to both hardware and software)
*** NAND basics
- Basic unit: page - 4K
- Block = set of pages - e.g., 64 pages
*** NAND Flash Operations
- Read(page)
- Write(page), cannot rewrite a page
- Erase(block)
  - Necessary before page in block can be rewritten
  - Limited number of erase cycles (100,000s)
- Since block must be completely erased before any single page in a block is
  written, pages are typically written sequentially in a block
*** SSD interface
- Very much like a disk
- ReadSector( logical_sector_no, buffer)
- WriteSector(logical_sector_no, buffer)
- Logical sector map maintained on device
*** SSD Characteristics
- Bandwidth higher than disk
- Latency: much lower than disk, 100 microseconds for read, 300 for write
- Several outstanding commands
*** Building a File System for SSD
- Need to write sequentially
- Cannot overwrite (need to erase before)
Looks like LFS
- Clean block before erasing
- Move live data to new block
- Erase block
*** TRIM command
- TRIM(range of logical_sector_no's)
- Indicate to device no longer in use
- No need to do cleaning, just erase
Need to try to even out the number of erase cycles
*** Utility
- Good for
  - Response time
  - Bandwidth
  - Robustness
- Bad for
  - Price
  - Capacity
* Week 12
** Virtual machine
- Virtual Machine Monitor (VMM) or hypervisor
- Machine running on a machine
- Usually, many machines running on a machine
*** Differences from programs running on OS
- Both provide resource management
  - Sharing resources between applications in OS
  - Sharing resources between VMs in VMM
- Differs on abstraction
  - OS provides abstractions (processes, address spaces, file systems,...)
  - VMM does not provide abstraction, provides an identical copy of the machine
*** VMM
- Resources manager for VMs
- Provides
  - Creation, destruction and scheduling of VMs
  - Memory management for VMs
  - Disk management for VMs
  - I/O management for VMs
*** Terminology
- Virtual Machine (VM) = efficient isolated duplicate of real machine
- Virtual Machine Monitor (VMM) or hypervisor = provider of VMs
  - Is in control of the hardware
  - Uses hardware to efficiently provides VMs
- Guest operating system = OS that runs in a VM
- Native mode = application or OS running on real hardware
- Virtualized mode = application or OS running on real hardware
** VMM implementation
- Kernel user boundary between VMM and VMs
- Problem: guest OSes operate in user mode, but written to operate in kernel
  mode
*** System calls
- Application wants to make a syscall in its guest OS
- Syscall vectors for the physical machine in VMM
- Syscall vectors for its OS in its guest OS
- Harware direct systcalls to physical syscalls vectors
- VMM must forward syscalls
  - VMM boot time: Install physical machine syscall vectors
  - OS boot time:
    - OS wants to install its syscall vectors
    - Privileged access from user mode
    - Traps to VMM
    - So VMM knows where they are
  - System call Time
    - Hardware
      - Puts machine into kernel mode
      - Jumps to machine syscall handler
    - VMM
      - Sets PC to appropriate syscall vector in OS
      - Returns to user mode
**** Execution summary
Native mode
| Application | Hardware              | OS                              |
|-------------+-----------------------+---------------------------------|
| System call |                       |                                 |
|             | Switch to kernel mode |                                 |
|             | PC = syscall handler  |                                 |
|             |                       | Run syscall handler             |
|             |                       | Next PC = after syscall         |
|             |                       | Return to user mode instruction |
|             | Return to user mode   |                                 |
| Continue    |                       |                                 |
Virtualized mode
 | Application | Hardware             | VMM                                   | Guest OS                        |
 |-------------+----------------------+---------------------------------------+---------------------------------|
 | System call |                      |                                       |                                 |
 |             | Kernel Mode          |                                       |                                 |
 |             | PC = syscall handler |                                       |                                 |
 |             |                      | Next PC = syscall handler in guest OS |                                 |
 |             |                      | Return to user mode instruction       |                                 |
 |             | User mode            |                                       |                                 |
 |             |                      |                                       | execute syscall                 |
 |             |                      |                                       | return to user mode instruction |
 |             | kernel mode          |                                       |                                 |
 |             | PC = trap handler    |                                       |                                 |
 |             |                      | Trap handler                          |                                 |
 |             |                      | Next PC = after syscall               |                                 |
 |             |                      | Return to user mode instruction       |                                 |
 |             | User mode            |                                       |                                 |
 | Continue    |                      |                                       |                                 |
- Trap into VMM
- VMM handles it
- Returns to OS kernel (except for return to user mode)
*** Protecting Guest OS memory
- Application could access its guest OS memory
How to prevent?
- On switching back from VMM to VM
- If return address is in guest OS, allow all memory accesses within VM
- If return address is outside guest OS, disallow access to guest OS memory
- Done through protections in page tables
*** Implementing virtual memory
Simplification using paging with valid bits
**** Native mode
- Process(or) issues virtual address\
- Memory management produces physical address
  - Either from the TLB
  - Or from the page table
- Physical address goes to memory
- OS allocates physical memory to processes
- OS installs page table and TLB entries to do so
**** Virtualized mode
- VMM allocates memory between VMs
- OS installs page table and TLB entries to do so
- Guest OS thinks
  - It is running on the real machine
  - It has control over all memory of the real machine
- But, guest OS
  - Is running on a virtual machine
  - Has limited portion of memory (allocated by VMM)
**** Terminology
| Native mode           | Virtualized mode             |
|-----------------------+------------------------------|
| Virtual address (VA)  | Virtual address (VA)         |
| Physical address (PA) | Guest physical address (gPA) |
|                       | Host physical address (hPA)  |
**** Two levels of translation
- VA \to gPA: managed by guest OS
  - Using guest TLB and guest page tables
  - One guest page table per process in guest OS
  - Resides in guest OS memory
- gPA \to hPA: managed by VMM
  - Using physical TLB and physical page tables
  - One physical page table per VM on the machine
  - Resides in VMM memory
**** gPA \to hPA translation
- Done by the VMM
- As done by the native OS before
- Allocate a page to a VM
  - Find a free frame
  - Insert page-to-frame mapping in VM's page table
  - Insert mapping in TLB
**** VA \to gPA
- Guest OS does not change
- So, allocate a new page to a process
  - Find a free frame (denoted by a gPA)
  - Insert page-frame mapping in process page table
  - Insert page-frame mapping in TLB
Last two are privileged operations trap to VMM, VMM does insert into VM's
physical page table

Expensive, walk two level of page tables, one in the guest OS, one in the VMM
**** Nested Page Tables
- Available on modern processors
- Allows to walk both sets of page tables
  - In parallel
  - In hardware
- Reduce cost of VM memory management
- But adds complexity to hardware




















** Architecure-centric view of VMs: Popek/Goldberg Theorem
TODO watch https://www.youtube.com/watch?v=zckZkEF7elo
*** Definitions
- An instruction is _privileged_ if it can only be executed in kernel mode
- An instruction is _sensitive_ if it behaves differently in kernel and user
  mode
*** Theorem
VMM exists for and architecture if and only if {sensitive} is a subset of
{privileged}
*** Intuition
- Implement "trap-and-emulate" VMM
- VMM runs guest OS and apps in user mode
- Guest OS: privileged instructions produce traps
- VMM emulates these traps, guest OS thinks it is running in kernel mode
- Everything else "still works"
- If there were sensitive un-privileged instructions
- Guest OS executes one of them
- Behaves differently when guest OS in user mode
- Semantics for guest OS is not maintained
*** Satisfaction of the theorem
- x86 \to nope
- Vmware, workaround: binary rewrite guest OS to remove such sensitive
  unprivileged instructions, tricky...
- Intel VT-x and AMD-v
  - Meets Popek/Goldberg theorem
  - Available on all current 64-bit processors
  - Used by all virtualization solutions today
** Paravirtualization
- VMM provides interface
  - /Almost/ like the real machine
  - /But not quite/
- Requires limited changes to guest OS
- But can offer better performances
** Conclusion
- VMs again popular, mainly because server provisioning and consolidation
- New x86 architectures facilitate efficient VMMS
  - Meet Popek/Goldberg theorem
  - Additional architectural support
- Cost and complexity remain to some extent
  - System calls, virtual memory
  - Also in I/O, especially networking

