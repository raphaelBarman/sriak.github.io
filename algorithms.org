# -*- mode: org; -*-
#+TITLE: Algorithms
#+SETUPFILE: htmlnotes.setup

* Week 1
** Definition of algorithm
- Relation between an input and an ouptut
- Tool to solve a computational problem
** Instance of a problem
Input of the problem. 
Examples :
- Sorting problem, instance : <32,54,65,23,54>
- Sum of all numbers to /n/ : 4, /n/ \neq instance
* Week 2
** Insertion sort algorithm
#+BEGIN_SRC java
for j = 2 to A.length
  key = A[j]
  // Insert A[j] into the sorted sequance A[1..j-1]
  i = j -1
  while i > 0 and A[i] > key
    A[i+1] = A[i]
    i = i -1
  A[i+1] = key
#+END_SRC
** Loop Invariant -> prove corectness of algorithm
- Loop invariant : "a statement that is satisfied during the loop"
- Need to verify :
  - *Initialization* : True at the beginning of the 1st iteration of the loop
  - *Maintenance* :  If it is true before an iteration of of the loop, it remains
    true before the next iteration
  - *Termination* : When the loop terminates, the invariant gives us a useful
    property that helps show that the algorithm is correct
***  Example 1 : Sum up to /n/
The algorithm is the following : \\
*CalculateSum* (/n/):
#+BEGIN_SRC java
ans = 0
for i = 1,2,..,n
 ans = ans + i
return ans
#+END_SRC
- *Loop invariant* : At the start of each iteration $ans = (i-1) * i/2$
- *Initialization* : At the begining of the first iteration, $i =1$ and $ans =
  0 = i(i -1)/2)$ so the invariant is satisfied
- *Maintenance* : Suppose invariant true at the beginning of iteration when
  $i=k$, i.e, $ans = (k-1)k/2$. Then the iteration updates $ans$ by adding $k$,
  i.e., $ans = (k-1)k/2 +k = k(k+1)/2$ so incrementing $i$ then for next
  iteration preserves the loop invariant.
- *Termination* : When the loop terminates $i=n+1$. Hence the algorithm returns
  $ans = n(n+1)/2$
*** Example 2 : Insertion sort
#+BEGIN_HTML
<input checked type=radio name=slider id=slide1 />
		<input type=radio name=slider id=slide2 />
		<input type=radio name=slider id=slide3 />
		<input type=radio name=slider id=slide4 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/insertionsortproof-0.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-1.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-2.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-3.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
			
		</div> <!-- #active -->
#+END_HTML
** Divide and conquer approach
*** Principle : 
- *Divide* the problem into a number of subproblems that are smaller instances
  of the same problem
- *Conquer* the subproblems by solving them recursively. If the subproblem sizes
  are small enough, however, just solve the subproblems in a straightforwad
  manner
- *Combine* the solutions to the subproblems into the solution for the original problem
*** Application on merge and sort : 
To sort $A[p...r]$:
- *Divide* by splitting into two subarrays $A[p...q]$ and $A[q+1,...r]$, whre
  $q$ is the halfway point of $A[p..r]$
- *Conquer* by recursively sorting the two subarrays $A[p..q]$ and $A[q+1,...r]$
- *Combine* by mergie the two sorted subarrays $A[p...q]$ and $A[q+1,...r]$ to
  produce a single sorted subarray $A[p...r]$

*Merge-Sort* $(A,p,r)$
#+BEGIN_SRC java
if p < r  //check for base case
  q = floor((p+r)/2)  // divide
  Merge-Sort(A,p,q)  // conquer
  Merge-Sort(A,q+1,r)  //conquer
  Merge(A,p,q,r) // combine
#+END_SRC
*Merge* ($A$,$p$,$q$,$r$)
#+BEGIN_SRC java
n_1 = q - p + 1
n_2 = r - q
let L[1..n_1+1] and R[1..n_2+1] be new arrays
for i = 1 to n_1
  L[i] = A[p+i-1]
for j = 1 to n_2
  R[j] = A[q+j]
L[n_1+1]= infinity
R[n_2 +1]= infinity
i = 1
j = 1
for k = p to r
  if L[i] <= R[j]
    A[k] = L[i]
    i = i +1
  else
    A[k] = R[j]
    j = j +1
#+END_SRC
**** Corectness
- What does merge(A,p,q,r) do ?
  + It takes array $A$ and indexes $p \le q < r$ s.t. $A[p...q]$ and $A[q+1...r]$ are sorted.
  + Then it outputs $A[p...r]$ contains the same elements in sorted order
- Proof by induction on $n = r - p$ :
  - Base case : $n = 0$, in this case $r = p$ so $A[p...r]$ (single element) is trivially sorted.
  - Inductive case : assume statement true $\forall\ n \in \{0,1,...,n-1\}$ and
    prove true for $n=k$
    - By induction hypothesis ( $q - p < n$ ) Merge-Sort(A,p,q) and
      Merge-Sort(A,q+1,r) sucessfully sort the two subarrays.
    - Therefore a correct merge procedure will sucessfully sort $A[p...q]$ as required.
**** Time analysis
[[file:pdf/mergesorttime.jpg]]
*** Analysis of divide-and-conquer algorithms
Use a *recurrence* equation to describe the running time :
- Let $T(n)$ = "running time on a problem of size $n$"
- If $n$ is small enough say $n\le c$ for some constant $c$ then $T(n) = \Theta(1)$ (by brute force)
- Otherwise, suppose we divide into $a$ sub problems each size $n/b$ (conquer step)
- Let $D(n)$ be the time to divide and let $C(n)$ the time to combine solutions
- We get the reccurence : $T(n) = \Theta(1)$ if $n\le c$, else $T(n) = aT(n/b)+D(n)+C(n)$
* Week 3 
Consider the following recurrence :

$T(n) = c$ if $n = 1$ \\
$T(n) = 2T(n/2) + c \cdot n$ otherwise

Note that this reccurence upper bounds and lower bounds the reccurence for
Merge-Sort by selecting $c$ sufficiently large and small, respectively.

Three solving techiques :
** The substitution method
1. Guess the form of the solution
2. Use mathematical induction to find the constant (substition method in the book)

\begin{align*} T(n) &= 2T(n/2) + c \cdot n \\
&= 2(2T (n/4) + c \cdot n/2) + c \cdot n = 4T(n/4) + 2 \cdot c \cdot n\\
&= 4(2T (n/8) + c \cdot n/4) + 2 \cdot c \cdot n =  8T(n/8) + 3 \cdot c \cdot n\\
&\vdots \\
&= 2^k T(n/2^k) + k \cdot c \cdot n
\end{align*}

A qualified guess is that $T(n) = \Theta (n \log n)$.
- and show that the solution works
  - First upper bound :
    We want to prove there exists $a > 0$ s.t $T(n) \le a \cdot n \cdot \log n$
    $\forall n \ge 2$
    - *Base case* : For any constant $n \in \{2,3,4\}$, $T(n)$ has a constant
      value, selecting a larger than this value will satisfy the base cases when
      $n \in \{2,3,4\}$.
    - *Inductive step* : /Assume statement true $\forall n \{2,3,...,k-1\}$ and
      prove the statement for $n=k$/
      \begin{align*}
      T(n) &= 2T(n/2) + c \cdot n \\
      &\le 2 \cdot \frac{an}{2} \log(n/2) + c \cdot n = a \cdot n \log(n/2) + c \cdot n \\
      &= a \cdot n \log n -a \cdot n + c \cdot n \\
      &\le a \cdot n \log n \text{ (if we select $a \ge c$)}
      \end{align*}
      We can thus select $a$ to be a positive constant so taht both the base
      cases and the inductive step holds. Hence, $T(n) = O(n\log n)$.
  - Second lower bound :
    - We want to prove there exists a constant $b > 0$ s.t $T(n) \ge b \cdot n
      \cdot \log n$ $\forall n \ge 0$
    - *Base case* : For $n=1,T(n)=c$ and $b \cdot n \log n =0$ so the base case
      is satisfied for any b.
    - *Inductive step* : /Assume statement true $\forall n \in \{0,1,...,k-1\}$
      and prove the statement for $n =k$/
      \begin{align*}
      T(n) &= 2T (n/2) + c \cdot n\\
      &\ge 2 \cdot \frac{b\cdot n}{2} \log(n/2) + c \cdot n = b \cdot n \log(n/2) + c \cdot n\\
      &= b \cdot n \log n - b \cdot n + c \cdot n \\
      & \ge b \cdot n \log n \text{ (if we select $b \le c$)}
      \end{align*}
We can thus select $b$ to be positive constant so that both the base cases and
the inductive step holds. Hence, $T(n) = \Omega(n\log n)$.

- Other example
Prove that $T(n) = O(n)$
First $\exists c$ s.t.
\begin{align*}
 T(n) \le F(n)\\
\text{and}\\
F(n) = c \text{ if $n=0, n=1$, else } F(\frac{n}{4} +1 ) + F(\frac{3n}{4} -1) + c
\end{align*} 
We shall prove that $F(n) = O(n)$
claim : there exists constants $b',b > 0$ and $n_0 \ge 0$ s.t. $ \forall n \ge n_0$
$F(n) \le b \cdot n - b'$

Proof : Inductive step : Assume $F(n) = b \cdot n$ $\forall n \in
\{n_0,...k-1\}$ prove true for $n=k$.
\begin{align*}
 F(n) &= F(\frac{n}{4} +1) + F(\frac{3n}{4} -1) + C \\
&= b(\frac{n}{4} + 1) b' + b (\frac{3n}{4} -1) b' + C\\
&= b \cdot n + c - 2b'\\
&\le b \cdot n - b' \text{ if $ b' \ge C$}
\end{align*}

- Again other example :
$T(n) = \Theta(1)$ if $n=1$ \\
$8 T(n/2) + c \cdot n^2$ if $n > 1$ \\
Prove that $T(n)$ is $O(n^3)$.

_Claim_ : $\exists d > 0$ and $n_0 > 0$ s.t. $T(n) \le d n^3 \forall n \ge n_0$

_Base case_: for $n=1,...,k$
$T(1), T(2) ... T(k)$ I can always select $d \ge max \left\lbrace T(1), T(2),...
T(k) \right\rbrace$

__Inductive step__ :
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le 8 \cdot d \cdot (n/2)^3 + c \cdot n^2 \\
= d \cdot n^3 + c \cdot n^2
\end{align*}
How to remove the low order terms ? We add to our claim a $-d' \cdot n^2$ s.t.
$T(n) \le d n^3 - d' \cdot n^2 \forall n \ge n_0$.
Now
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le d \cdot n^3 - 2 \cdot d' \cdot n^2 + c \cdot n^2\\
\le d \cdot n^3 - d' \cdot n^2 \text{if $d' \ge c$}
\end{align*}
** The recursion tree method 
#+BEGIN_HTML
<input checked type=radio name=slider id=slide01 />
		<input type=radio name=slider id=slide02 />
		<input type=radio name=slider id=slide03 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/recursiontree01.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree02.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree03.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
			
		</div> <!-- #active -->
#+END_HTML
- Examples to do with this method :
  - $T(n) = 2 T(n/2) + c \cdot n^2$, cost of base root $cn^2$, but cost of last
    root : $cn$
** The master method
Used to black-box solve reccurences of form $T(n) = a \cdot T(n/b) + f(n)$
(doesn't work with $T(n) = T(n/3) + T(2n/3) + cn$ for example).

_Theorem (Master theorem)_

Let $a \ge 1$ and $b > 1$ be constants, let $T(n)$ be defined on the
  nonnegative integers by the reccurence :
\begin{align*}
T(n) = a \cdot T(n/b) + f(n)
\end{align*}
Then, $T(n)$ has the following asymptotic bounds :
- If $f(n) = O(n^{\log_b a- \epsilon})$ for some constant $\epsilon > 0$, then
  $T(n) = \Theta(n^{\log_b a})$
- If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{\log_b a} \log n)$
- If $f(n) = \Omega(n^{\log_b a+\epsilon})$ for some constant $\epsilon > 0$ and
  if $a \cdot f(n/b) \le c \cdot f(n)$ for some constant $c < 1$ and all
  sufficiently large $n$, then $T(n) = \Theta(f(n))$
* Week 4 
** Maximum-subarray problem

"If we let $A[i]$ = (price after day $i$) - (price after day $i-1$) then if the
maximum subarray is $A[i..j]$ then we should have bought just before day $i$ and
sold just after day $j$.

*Input* : An array $A[1...n]$ of numbers

*Output* Indices $i$ and $j$ such that $A[i...j]$ has the greatest sum of any
 nonempty, contiguous subarray of $A$, along with the sum of the values in $A[i...j]$.
*** Brute force
Simply check all possible subarrays.

$\binom{n}{2} =  \Theta (n^2)$ many.

*Maximum-subarray-slow* $(A[1...n])$
#+BEGIN_SRC java
B.val = - infinity, B.i = 1, B.j = n
for i = 1 to n
  tmp = 0
  for j = i to n
    tmp = tmp + A[j]
    if tmp > B.val
       B.val = tmp
       B.i = i
       B.j = j
return (B.i,B.j.B.val)
#+END_SRC
Running time is $\Theta(n^2)$ and we use $\Theta(n)$ space.
*** Divide and conquer
- *Divide* the subarray into two subarrays of as equal size as possible. Find the midpoint mid of the subarrays, and consider the subarrays $A$[low.. mid]
  and $A$[mid+1...high].
- *Conquer* by finding maximum subarrays of $A$[low . . . mid] and $A$[mid + 1 . . . high].
- *Combine* by finding a maximum subarray that crosses the midpoint,and using the best solution out of the three

 *FIND-MAXIMUM-SUBARRAY* $(A,low,high)$
#+BEGIN_SRC java
if high == low
  return (low, high, A[low]) // base case: only one element
else mid = ceil((low + high)/2)
  (left-low, left-high, left-sum) =
    FIND-MAXIMUM-SUBARRAY(A, low, mid)
  (right-low, right-high, right-sum) =
    FIND-MAXIMUM-SUBARRAY(A, mid + 1, high)
  (cross-low, cross-high, cross-sum) =
    FIND-MAXIMUM-CROSSING-SUBARRAY(A,low,mid,high) 
  if left-sum >= right-sum and left-sum >= cross-sum
    return (left-low, left-high, left-sum)
  else if right-sum >= left-sum and right-sum >= cross-sum
    return (right-low, right-high, right-sum)
  else return (cross-low, cross-high, cross-sum)
#+END_SRC

**** Time analysis
Assume that we can find max-crossing-subarray in time $\Theta(n)$ :

- *Divide* takes constant time, i.e, $D(n) = \Theta(1)$
- *Combine* time dominated by find-max-crossing-subarray, $C(n) = \Theta(n)$
- *Conquer* recusively solve two subproblems, each of size $n/2$, $T(n/2)$.

\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 2T(n/2)+\Theta(n) \text{  otherwise}
\end{align*}
so $T(n)=\Theta)n \log n)$
**** Crossing subarray
- Any subarray crossing the midpoint A[mid] is made of two subarrays $A[i...mid]$ and $A[mid + 1,...,j]$ where $low \le i \le mid$ and $mid < j \le high$
- Find maximum subarrays of the form $A[i...mid]$ and $A[mid + 1...j]$ and then combine them.
*FIND-MAXIMUM-CROSSING-SUBARRAY* $(A,low,mid,high)$
#+BEGIN_SRC java
//Find a maximum subarray of the form A[i...mid]
left-sum = - infinity
sum = 0
for i = mid downto low
  sum = sum + A[i]
  if sum > left-sum
    left-sum = sum
    max-left = i
//Find a maximum subarray of the form A[mid+1...j]
right-sum = - infinity
sum = 0
for j = mid + 1 to high
  sum = sum + A[j]
  if sum > right-sum
    right-sum = sum
    max-right = j
// Return the indices and the sum of the two subarrays.
return (max-left, max-right, left-sum + right-sum)
#+END_SRC

** Matrix multiplication
Multiply two $n\times n$ matrices. $AB=C$

$c_{ij} = \sum\limits_{k=1}^n a_{ik}b_{kj}$

*** Naive algorithm
*SQUARE-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
for i = 1 to n
  for j = 1 to n
    cij = 0
    for k = 1 to n
      cij = cij + aik*bkj
return C
#+END_SRC
Running time : $\Theta(n^3)$

Space : $\Theta(n^2)$

*** Divide-and-Conquer
- *Divide* each of $A,B,C$ into four $n/2 \times n/2$ matrices: so that :
\begin{align*}
  \begin{pmatrix}
  C_{11} & C_{12}\\
  C_{21} & C_{22}
  \end{pmatrix} = \begin{pmatrix}
  A_{11} & A_{12}\\
  A_{21} & A_{22}
  \end{pmatrix} \cdot \begin{pmatrix}
  B_{11} & B_{12}\\
  B_{21} & B_{22}
  \end{pmatrix}
\end{align*}
- *Conquer* : Since
\begin{align*}
C_{11} =  A_{11} \cdot B_{11} + A_{12} \cdot B_{21}\\
C_{12} =  A_{11} \cdot B_{12} + A_{12} \cdot B_{22}\\
C_{21} =  A_{21} \cdot B_{11} + A_{22} \cdot B_{21}\\
C_{22} =  A_{21} \cdot B_{12} + A_{22} \cdot B_{22}
\end{align*}
We recursively solve 8 matrix multiplications that each multiply two $n/2 \times
n/2$ matrices.
- *Combine* Make the additions to get $C$
**** Pseudocode
*REC-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
if n == 1
  c11 = a11 * b11
else partition A,B and C into n/2 x n/2 submatrices
  c11 = REC-MAT-MULT(A11,B11,n/2) + REC-MAT-MULT(A12,B21,n/2)
  c12 = REC-MAT-MULT(A11,B12,n/2) + REC-MAT-MULT(A12,B22,n/2)
  c21 = REC-MAT-MULT(A21,B11,n/2) + REC-MAT-MULT(A22,B21,n/2)
  c22 = REC-MAT-MULT(A21,B12,n/2) + REC-MAT-MULT(A22,B22,n/2)
return C
#+END_SRC
**** Analysis
Let $T(n)$ be the times to multiply two $n \times n$ matrices.
- *Base case*: $n=1$. Perform one scalar multiplication: $\Theta(1)$
- *Recursive case*: $n>1$
  - Dividing takes $\Theta(1)$ time if careful and $\Theta(n^2)$ if simply copying
  - Conquering makes 8 recursive calls, each multiplying $n/2 \times n/2$
    matrices : $8T(n/2)$
  - Combining takes time $\Theta(n^2)$ time to add $n/2 \times n/2$ matrices.
Reccurence is :
\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 8T(n/2)+\Theta(n^2) \text{  if $n>1$}
\end{align*}
So $T(n) = \Theta(n^3)$ (from master method)
*** Strassen's algorithm 
- *Divide* eachf $A,B,C$ into four $n/2 \times n/2$ matrices: so that :

$\begin{pmatrix}
C_{11} & C_{12}\\
C_{21} & C_{22}
\end{pmatrix} = \begin{pmatrix}
A_{11} & A_{12}\\
A_{21} & A_{22}
\end{pmatrix} \cdot \begin{pmatrix}
B_{11} & B_{12}\\
B_{21} & B_{22}
\end{pmatrix}$  

- *Conquer* : calculate recusively 7 matrix multiplications, each of two $n/2
  \times n/2$ matrices :
\begin{align*}
&M_1 := (A_{11}+A_{22})(B_{11}+B_{22}) &\ M_5 := (A_{11}+A_{12})B_{22}\\
&M_2 := (A_{21}+A_{22})B_{11} &\ M_6 := (A_{21}-A_{11})(B_{11}+B_{12})\\
&M_3 := A_{11}(B_{12}-B_{22}) &\ M_7 := (A_{12}-A_{22})(B_{21}+B_{22})\\
&M_4 := A_{11}(B_{21}-B_{11})
\end{align*}

- *Combine* Let
\begin{align*}
&C_{11} = M_1 + M_4 - M_5 + M_7 &\ C_{12} = M_3 + M_5 \\
&C_{21} = M_2 + M_4 &\ C_{22} = M_1 - M_2 + M_3 + M_6
\end{align*}
**** Time analysis
- Base case $n = 1$ $\Rightarrow$ it takes time $\Theta(1)$
- Recursive case: $n > 1$
  - Dividing takes time $\Theta(n^2)$
  - Conquering makes 7 recursive calls, each multiplying $n/2 \times n/2$
    matrices $\Rightarrow 7 T(n/2)$
  - Combining takes time $\Theta(n^2)$ time to add $n/2 \times n/2$ matrices.
Reccurence is
\begin{align*}
T(n) = \Theta(1) \text{ if $n=1$}\\
T(n) = 7T(n/2) + \Theta(n^2) \text{ if $n > 1$}
\end{align*}
Master method $\Rightarrow T(n) = \Theta(n^{\log_2 7})$ 

Best we could hope for is $\Theta(n^2)$ because we need to at least look at each
entry of the matrices.
* Week 5
** Heap 
Heap $A$ ( _not_ garbage-collected storage) is a nearly complete binary trees
- height $\lceil \log_2(n)\rceil$
- Atmost one node has one child

(Max)-Heap property: key of $i$ 's children is _smaller or equal_ to $i$ 's key

(Min)-Heap property: key of $i$ 's children is _greater or equal_ to $i$ 's key

*** Root :
- Max-Heap $\Rightarrow$ maximum element is the root
- Min-heap $\Rightarrow$ minimum elememnt is the root
*** Height :
Height of node = # of edges on a longest simple path from the node down to a leaf

Height of heap = height of root = $\Theta (\log n)$
*** Storage :
Use that tree is almost complete to store it in array
[[file:images/al_ch5_01.png]]
*** Operations
**** MAX-HEAPIFY
MAX-HEAPIFY : given an $i$ such that the subtrees of $i$ are heaps, it ensures
that the subtree rooted at $i$ is a heap satisfying the heap property.

*Algorithm* :
- Compare $A[i],A[Left(i)],A[Right(i)]$
- If necessary, swap $A[i]$ with the largest of the two children to preserve
  heap property
- Continue this process of comparing and swapping down the heap, until subtree
  rooted at $i$ is max-heap
*MAX-HEAPIFY* $(A,i,n)$
#+BEGIN_SRC java
l = Left(i)
r = Right(i)
if l <= n and A[l] > A[i]
  largest = l
else largest = i
if r <= and A[r] > A[largest]
  largest = r
if largest != i
  exchange A[i] with A[largest]
  MAX-HEAPIFY(A,largest,n)
#+END_SRC

*Running time* : $\Theta($ height of $i)=O(\log n)$

*Space* : $\Theta(n)$
**** Building a heaps
Given unordered array $A$ of length $n$, BUILD-MAX-HEAP(A,n)

*BUILD-MAX-HEAP* $(A,n)$
#+BEGIN_SRC java
for i = floor(n/2) downto 1
  MAX-HEAPIFY(A,i,n)
#+END_SRC

*Analysis*

Worst case :
- Simple bound : $O(n)$ calls to MAX-HEAPIFY, each of wich takes $O(\log n)$
  time $\Rightarrow O(n \log n)$ in total
- Tighter anaysis : Time to run MAX-HEAPIFY is linear in the height of the node
  it's run on. Hence the time bounded by :
  
$\sum\limits_{h=0}^{\log n}
  \lbrace \text{# nodes of height } h \rbrace O(h) = O(n \sum\limits_{h=0}^{\log
  n} \frac{h}{2^h})$

which is $O(n)$ since $\sum\limits_{h=0}^{\infty} \frac{h}{2^h} =
\frac{1/2}{(1-1/2)^2} = 2$

*Correctness*

_Loop invariant_ : At start of every iteration of *for* loop, each node
$i+1,i+2,...,n$ is root of max-heap

_Maintenance_ :
- Children of node $i$ are indexed higher than $i$, so by the loop invariant,
  they are both roots of max-heaps
- Therefore, MAX-HEAPIFY makes node $i$ a max-heap root (so $i,i+1,...,n$ are
  all roots of max-heaps)
- Hence, the invariant stays true when decredementing $i$ at the beggining of
  the next iteration

_Termination_:
- When $i = 0$, the loop terminates
- By the loop invariant, each node, notably node 1, is the root of a max-heap

** Heapsort
- Builds a max-heap from the array
- Starting with the root (the maximum element), the algorithm places the maximum
  element into the correct place in the array by swapping it with the element in
  the last position in the array
- "Discard" this last node (knowing that it is in its correct place) by
  decreasing the heap size, and calling MAX-HEAPIFY on the new (possibly
  incorrect-placed) root
- Repeat this "discarding" process until only one node (the smallest element)
  remains, and therefore is in the correct place in the array
*** Pseudocode :
*HEAPSORT* $(A,n)$
#+BEGIN_SRC java
BUILD-MAX-HEAP(A,n)
for i = n downto 2
  exchange A[1] with A[i]
  MAX-HEAPIFY(A,1,i-1)
#+END_SRC
*** Analysis
- BUILD-MAX-HEAP : $O(n)$
- *for* loop: $n-1$ times
- exchange elements: $O(1)$
- MAX-HEAPIFY: $O(\lg n)$

*Total time* : $O(n \lg n)$

** Priorty queue
- Maintains a dynamic set $S$ of elements
- Each set element has a *key* - an associated value that regulates its importance
- Operations :
  - INSERT(S,x): inserts element $x$ into $S$
  - MAXIMUM(S) : returns element of $S$ with largest key
  - EXTRACT-MAX(S): removes and returns element of $S$ with largest key
  - INCREASED-KEY(S,x,K): increases value of element $x$ 's key to $k$; assume $k
    \le x$ 's current key value.
*** Operations 
**** HEAP-MAXIMUM 
Symply return the root in time $\Theta(1)$

*HEAP-MAXIMUM* $(A)$
#+BEGIN_SRC java
Return A[1]
#+END_SRC
**** HEAP-EXTRACT-MAX(A,n)
1. Make sure heap is not empty
2. Make a copy of the maximum element (the root)
3. Make the last node in the tree the new root
4. Re-heapify the heap, with one fewer node

_Analysis_ : Constant-time assignments plus time for MAX-HEAPIFY

So $O(\lg n)$

*HEAP-EXTRACT-MAX* $(A,n)$
#+BEGIN_SRC java
if n < 1
  error "heap underflow"
max = A[1]
A[1] = A[n]
n = n - 1
MAX-HEAPIFY(A,1,n)
return max
#+END_SRC
**** HEAP-INCREASE-KEY(A,i,key)
Given a heap $A$, index $i$, and a new value $key$
1. Make sure $key >= A[i]$
2. Update $A[i]$ 's value to $key$
3. Traverse the tree upward comparing new key to the parent and swapping keys if
   necessary, until the new key is smaller than the parent's keys

_Analysis_ : Upward path frome node $i$ has length $O(\lg n)$ in an $n$ -element
heap.

So $O(\lg n)$

*HEAP-INCREASE-KEY* $(A,i,key)$
#+BEGIN_SRC java
if key < A[i]
  error "new key is smaller than current key"
A[i] = key
while i > 1 and A[Parent(i)] < A[i]
  exchange A[i] with A[Parent(i)]
  i = Parent(i)
#+END_SRC
**** MAX-HEAP-INSERT(A,key,n)
Given a new key to insert into heap
1. Increment the heap size
2. Insert a new node in the last position in the heap, with key − \infty
3. Increase the − \infty value to key using Heap-Increase-Key

*MAX-HEAP-INSERT* $(A,key,n)$
#+BEGIN_SRC java
n = n+1
A[n] = - infinity
HEAP-INCREASE-KEY(A,n,key)
#+END_SRC


** Stacks
Last in, first out.
- Insert operation called PUSH($S,x$)
- Delete operation called POP($S$)
*** Implementation
Implementation using arrays: $S$ consists of elements $S[1,...,S.top]$
- $S[1]$ element at the bottom
- $S[S.top]$ element at the top
*** Operations
*STACK-EMPTY* $(S)$
#+BEGIN_SRC java
if S.top = 0
  return true
else return false
#+END_SRC

*PUSH* $(S,x)$
#+BEGIN_SRC java
S.top = S.top + 1
S[S.top] = x
#+END_SRC

*POP* $(S)$
#+BEGIN_SRC java
if STACK-EMPTY(S)
  error "underflow"
else
  S.top = S.top -1
  return S[S.top+1]
#+END_SRC

All these operations are $O(1)$

** Queue
First-in, first-outputs
- Insert operation called ENQUEUE($Q,x)$
- Delete operation called DEQUEUE$(Q)$
*** Implementation
Implementation using arrays: $Q$ consists of elements $S[Q.head,...,Q.tail-1]$
- $Q.head$ points at the first elements
- $Q.tail$ points at the next location where a newly arrived alement wille be placed
*** Operations
*ENQUEUE* $(Q,x)$
#+BEGIN_SRC java
Q[Q.tail] = x
if Q.tail == Q.length
  Q.tail = 1
else Q.tail = Q.tail + 1
#+END_SRC

*DEQUEUE* $(Q,x)$
#+BEGIN_SRC java
x = Q[Q.head]
if Q.head == Q.length
  Q.head = 1
else Q.head = Q.head + 1
return x
#+END_SRC

All these operations are $O(1)$

** Stacks and queues
*** Pros
- Very efficent
- Natural operations
*** Cons
- Limited support: for example, no search
- Implementations using arrays have a fixed capacity
** Linked list
Objects are arranged in a linear order : not indexes in array, but pointer to
next object in each object.

A list can be :
- Single linked or double linked
- Sorted or unsorted
- etc.

Example : An element in a double linked list :
- x.key (key value)
- x.prev (point to previous element)
- x.next (points to next element)
*** Searching in a linked list
Given $k$ return pointer to first element with key $k$

*LIST-SEARCH* $(L,k)$
#+BEGIN_SRC java
x = L.head
while x != nil and x.key != k
  x = x.next
return x
#+END_SRC

Running time : $O(n)$

If no element with key $k$ ? Returns /nil/

*** Inserting into a linked list
Insert a new element $x$

*LIST-INSERT* $(L,x)$
#+BEGIN_SRC java
x.next = L.head
if L.head != nil
  L.head.prev = x
L.head = x
x.prev = NIL
#+END_SRC

Running time : $O(1)$

*** Deleting from a linked list
Given a pointer to an element $x$ removes it from $L$

*LIST-DELETE* $(L,x)$
#+BEGIN_SRC java
if x.prev != nil
  x.prev.next = x.next
else L.head = x.next
if x.next != nil
  x.next.prev = x.prev
#+END_SRC

Running time : $O(1)$
*** TODO Using Sentinels
*Add image for clarity*

We add a "sentinel" called L.nil which
- Points to itself for prev and next if list is empty
- Next points to first element of list (head), prev points to last element of the list
  and last element of the list points to it

Then we can symplify delete and insert by :

*LIST-DELETE* $(L,x)$
#+BEGIN_SRC java
x.prev.next = x.next
x.next.prev = x.prev
#+END_SRC

*LIST-INSERT* $(L,x)$
#+BEGIN_SRC java
x.next = L.nil.next
L.nil.next.prev = x
L.nil.next = x
x.prev = L.nil
#+END_SRC

*** Summary
- Dynamic data structure without predefined capacity
- Insertion : $O(1)$
- Deletion : $O(1)$ (if double linked)
- Search : $O(n)$
* Week 6
** Binary Search trees
Encodes a strategy whatever number we look for.

Key property:
- if $y$ is in the left subtree of $x$ then $y.key < x.key$ 
- if $y$ is in the right subtree of $x$ then $y.key \ge x.key$

Height ($h$) is the number of edges in longest path from root to leaf.

Basic operations take time proportional to height: $O(h)$

Each element $x$ has :
- $x.left$ : pointer to left child
- $x.right$ : pointer to right child
- $x.p$ : pointer to parent
- $x.key$ : key
*** Searching
*TREE-SEARCH* $(x,k)$
#+BEGIN_SRC java
if x == NIL or k == key[x]
  return x
if k < x.key
  return TREE-SEARCH(x.left,k)
else return TREE-SEARCH(x.right,k)
#+END_SRC

Running time : $O(h)$
*** Min and Max
By key property :
- Minimum is located in leftmost node
- Maximum is located in rightmost node

*TREE-MINIMUM* $(x)$
#+BEGIN_SRC java
while x.left != NIL
  x = x.left
return x
#+END_SRC

*TREE-MAXIMUM* $(x)$
#+BEGIN_SRC java
while x.right != NIL
  x = x.right
return x
#+END_SRC

Running time : $O(h)$
*** Sucessor and Predecessor
Sucessor of a node $x$ is the node $y$ sutch that $y.key$ is the

"smallest key" > $x.key$

Two case :
+ $x$ has a non-empty right subtree \rightarrow $x$ 's sucessor is the minimum
  in the right subtree
+ $x$ has an empty right subtree \rightarrow As long as we go to the left up the
  tree we're visiting smaller keys. $x$ 's sucessor is $y$ is the  node that $x$
  is the predecessor of.

*TREE-SUCESSOR* $(x)$
#+BEGIN_SRC java
if x.right != NIL
  return TREE-MINIMUM(x.right)
y = x.p
while y != NIL and x == y.right
  x = y
  y = y.p
return y
#+END_SRC

Running time : $O(h)$

Predecessor is symmetric !

*** Printing
**** Inorder
- Print left subtree recursively
- Print root
- Print right subtree recursively

*INORDER-TREE-WALK* $(x)$
#+BEGIN_SRC java
if x != NIL
  INORDER-TREE-WALK(x.left)
  print key[x]
  INORDER-TREE-WALK(x.right)
#+END_SRC

Running time : $\theta(n)$
**** Preorder and Postorder
*PREORDER-TREE-WALK* $(x)$
#+BEGIN_SRC java
if x != NIL
  print key[x]
  PREORDER-TREE-WALK(x.left)
  PREORDER-TREE-WALK(x.right)
#+END_SRC

*POSTORDER-TREE-WALK* $(x)$
#+BEGIN_SRC java
if x != NIL
  POSTORDER-TREE-WALK(x.left)
  POSTORDER-TREE-WALK(x.right)
  print key[x]
#+END_SRC
*** Modifying
**** Inserting
- Search for $z.key$
- When arrived at $nil$ insert $z$ at that position

*TREE-INSERT* $(T,z)$
#+BEGIN_SRC java
y = NIL
x = T.root
while x != NIL
  y = x
  if z.key < x.key
    x = x.left
  else x = x.right
z.p = y
if y == NIL
  T.root = z // tree T was empty
elseif z.key < y.key
  y.left = z
else y.right = z
#+END_SRC

Running time : $O(h)$

**** Deleting
3 cases :
- If $z$ has no children, remove it
- if $z$ has one child, then make that child takes $z$ 's position in the tree
- if $z$ has two children, then find its sucessor $y$ and replace $z$ by $y$

We first need to define a helper function which replace subtree rooted at $u$ with
that rooted at $v$

*TRANSPLANT* $(T,u,v)$
#+BEGIN_SRC java
if u.p == NIL
  T.root = v
elseif u == u.p.left
  u.p.left = v
else u.p.right = v
if v != NIL
  v.p = u.p
#+END_SRC

Then the actual deletion algorithm :

*TREE-DELETE* $(T,z)$
#+BEGIN_SRC java
if z.left == NIL
  TRANSPLANT(T,z,z.right) // z has no left child
elseif z.right == NIL
  TRANSPLANT(T,z,z.left) // z has just a left child
else // z has two children
  y = TREE-MINIMUM(z.right) // y is z's sucessor
  if y.p != z
    // y lies within z's right subtree but is not the root of this
    TRANSPLANT(T,y,y.right)
    y.right = z.right
    y.right.p = y
  // Replace z by y
  TRANSPLANT(T,z,y)
  y.left = z.left
  y.left.p = y
#+END_SRC

Running time : $O(h)$
** Dynamic programming
Main idea :
- Remember calculations already made
- Saves enormous amounts of computation
*** Key element in designing a DP-algorithm
_Optimal substructure :_
- Show that a solution to a problem consists of making a choice, which leaves
  one or several subproblems to solve and the optimal solution solves the
  subproblems optimally \Rightarrow allows us to write a recursion

_Overlapping subproblems :_
- A naive recursive algorithm may revisit the same (sub)problem over and over.
- Top-down with memoization : Solve recursively but store each result in a table
  (only solves subproblems really required to find solution)
- Bottom-up Sort the subproblems and solve the smaller ones first; that way,
  when solving a subproblem, have already solved the smaller subproblems we need
  (can be better in term of memory)
** Fibonacci numbers
basic resursive function : same calculations again and again \Rightarrow exponential time !

Solution ? Remember what we have down :

Two ways:
- Top-down with memoization
  - Solve recursively but store each result in a table
  - Memoizing is remembering what we have computed previously
- Bottom-up
  - Sort the subproblems and solve the smaller ones first
  - That way, when solving a subproblem, have already solved the smaller subproblems we need
*** Memoization
*MEMOIZED-FIB* $(n)$
#+BEGIN_SRC java
Let r = [0...n] be a new array
for i = 0 to n
  r[i] = - infinity
return MEMOIZED-FIB-AUX(n,r)
#+END_SRC

*MEMOIZED-FIB-AUX* $(n,r)$
#+BEGIN_SRC java
if r[n] >= 0
  return r[n]
if n = 0 or n = 1
  ans = 1
else
  ans = MEMOIZED-FIB-AUX(n-1,r)+MEMOIZED-FIB-AUX(n-2,r)
r[n] = ans
return r[n]
#+END_SRC

Time analysis :
- Steps 1-3 in MEMOIZED-FIB takes times $\Theta(n)$
- Each call to MEMOIZED-FIB-AUX takes $\Theta(1)$
- Number of calls to MEMOIZED-FIB-AUX is $\Theta(n)$
- Total time is thus $\Theta(n)$
*** Bottom-updates
*BOTTOM-UP-FIB* $(n)$
#+BEGIN_SRC java
Let r = [0..n] be a new aray
r[0] = 1
r[1] = 1
for i = 2 to n
  r[i] = r[i-1]+r[i-2]
return r[n]
#+END_SRC

** Rod cutting
_Instance_ :
- A length $n$ of a metal rod
- A table of prices $p_i$ for rods of lengths $i=1,...n$

_Objective_ : Decide how to cut the rod into pieces and maximize the price

*** Size of the problem
- There are 2^{n-1} possible solutions - either cut or do not cut after every length unit
- Need structure for an efficient algorithm
***  Structural theorem
If
- the leftmost cut in an optimal solution is after $i$ units
- an optimal way to cut a solution of size $n-i$ is into rods of sizes: $s_1,s_2,...,s_k$
Then, an optimal way to cut our rods is into rods of size: $i,s_1,s_2,...,s_k$

_Proof_

Feasability : Since $s_1,s_2,...,s_k$ is a feasible solution for  an instance of
size $n-i$:

$\sum\limits_{j=1}^k s_j = n - i$

Hence, $i + \sum\limits_{j=1}^k s_j = n$

Optimality: Let $i,o_1,o_2,...,o_l$ be an optimal solution - exists by
assumption. Recall that $s_1,s_2,...,s_k$ is an optimal way to cut a rod of size
$n-1$, thus

$\sum\limits_{j=1}^k p_{s_j} \ge \sum\limits_{j=1}^l p_{o_j}$

Hence, $p_i + \sum\limits_{j=1}^k p_{s_j} \ge p_i + \sum\limits_{j=1}^l p_{o_j}$
 
*** First algorithm
if we let $r(n)$ be the optimal revenu from a rod of length $n$, then, by 
the strucural theorem, we can express $r(n)$ recursively as follows

$r(n) = 0$ if $n = 0$

$r(n) = \text{max}_{1\le i \le n}\lbrace p_i + r(n-i)\rbrace$ otherwise $n \ge 1$

*CUT-ROD* $(p,n)$
#+BEGIN_SRC java
if n == 0
  return 0
q = -infinity
for i = 1 to n
  q = max(q,p[i]+ CUT-ROD(p,n-i))
return q
#+END_SRC

Procedure is extremly inefficient, exponential

*** Top-down algorithm
- Keep the recurive structure of the pseudocode
- Memoize (store) the result of every recursive call
- At each recursively call, try to avoid work using memoized results

*MEMOIZED-CUT-ROD* $(p,n)$
#+BEGIN_SRC java
let r[0..n] be a new array
for i = 0 to n
  r[i] = -infinity
return MEMOIZED-CUT-ROD-AUX(p,n,r)
#+END_SRC

*MEMOIZED-CUT-ROD-AUX* $(p,n,r)$
#+BEGIN_SRC java
if r[n] >= 0
  return r[n]
if n == 0
  q == 0
else q = -infinity
  for i = 1 to n
    q = max(q,p[i]+MEMOIZED-CUT-ROD-AUX(p,n-i,r))
r[n] = q
return q
#+END_SRC

- Initialization takes $O(n)$ time
- Processing each sub-problems takes linear time in the number of sub-problems
  it evokes
- The time complexity is proportional to the numbers of nodes and edges in the
  subproblem graph

Time complexity : $O(n^2)$

*** Bottom-up
- Sort the sub-problems by size
- Solve the smaller ones first
- When reaching sub-problem, the smaller ones are already solved

*BOTTOM-UP-CUT-ROD* $(p,n)$
#+BEGIN_SRC java
let r[0..n] be a new array
r[0] = 0
for j = 1 to n
  q = - infinity
  for i = 1 to j
    q = max(q,p[i]+r[j-i]
  r[j] = q
return r[n]
#+END_SRC

Time complexity : $O(n^2)$
*** Analysis
- Choice: where to make the leftmost cut
- Optimal substructure: to obtain an optimal solution, we need to cut the
  remaining piece in an optimal way

Hence, if we let r (n) be the optimal revenue from a rod of length n, we
can express $r(n)$ recursively as follows

$r(n) = 0$ if $n = 0$

$r(n) = max_{1\le i \le n} \lbrace p_i + r(n-i)\rbrace$ otherwise if $n \ge 1$

- Overlapping subproblems: Solve recurrence using top-down with memoization or
  bottom-up which yields an algorithm that runs in time $\Theta(n^2)$
*** Optimal solution
- The above algorithms only return the optimal profit
- Sometimes one needs also to find an optimal solution

_Approach_
- Each cell of the memoization table corresponds to a decision: the location of
  the left most cut
- Store the decision corresponding to every cell in a separate table

*EXTENDED-BOTTOM-UP-CUT-ROD* $(p,n)$
#+BEGIN_SRC java
let r[0..n] and s[0..n] be new arrays
r[0] = 0
for j = 1 to n
  q = - infinity
  for i = 1 to j
    q = p[i] + r[j-i]
    s[j] = i
  r[j] = q
return r and s
#+END_SRC

- The table *s* stores the choices that lead to an optimal solution.
- These decicisions can be extracted from the table

*PRINT-CUT-ROD-SOLUTION* $(p,n)$
#+BEGIN_SRC java
(r,s) = EXTENDED-BOTTOM-UP-CUT-ROD(p,n)
while n > 0
  print s[n]
  n = n - s[n]
#+END_SRC

*** Summary
- We had a recursive formulation for the optimal value for our problem
$r(n) = 0$ if $n = 0$

$r(n) = max_{1\le i \le n} \lbrace p_i + r(n-i)\rbrace$ otherwise if $n \ge 1$
- Speed up the calculations by fillind in a table either "top-down with
  memoization" or with "bottom-up"
- Recovered an optimal solution using an additional table

** Matrix-chain multiplication
*** Cost of matrix multiplication
$A_{p,q} \times B_{q,r}$ = $C_{p,r}$

- Each cell of $C$ requires $q$ scalar multiplication
- In total : $pqr$ scalar multiplication
*** Definition
- Input : A chain $< A_1,A_2,...,A_n >$ of $n$ matrices where for $i=1,2,...,n$
  matrix $A_i$ has dimension $p_{i-1} \times p_i$
- Output : A full parenthesization of the product $A_1A_2...A_n$ in a way that
  minimizes the number of scala multiplications.
- Remarks :
  - We are not asked to calculate the product, only find the best parenthesization
  - The parenthesization can significantly affect the number of multiplications
- Example :
  - A product $A_1A_2A_3$ with dimensions: $50 \times 5$, $5 \times 100$ and
    $100 \times 10$
  - Calculating $(A_1A_2)A_3$ requires: $50 · 5 · 100 + 50 · 100 · 10 = 75000$
    scalar multiplications.
  - Calculating $A_1(A_2A_3)$ requires: $5 · 100 · 10 + 50 · 5 · 10 = 7500$
    scalar multiplications.
*** Optimal substructure
_Theorem_ :

if :
- the outermost parenthesization in an optimal solution is :
  $(A_1A_2...A_i)(A_{i+1}A_{i+2}...A_n)$
- $P_L$ and $P_R$ are optimal parenthesizations for $A_1A_2...A_i$ and
  $A_{i+1}A_{i+2}...A_n$ respectively
Then, $((P_L)·(P_R))$ is an optimal parenthesizations for $A_1A_2...A_n$

_Proof_ :
- Let $((O_L)·(O_R))$ be an optimal parenthesization, where $O_L$ and $O_R$ are
  parenthesization for $A_1A_2...A_i$ and $A_{i+1}A_{i+2}...A_n$ respectively
- Let $M(P)$ be the number of scalar multiplications required by a
  parenthesization

\begin{align*}
M((O_L)·(O_R)) &= p_0·p_i·p_n+M(O_L)+M(O_R)\\
&\ge p_0·p_i·p_n+M(P_L)+M(P_R) = M((P_L)·(P_R))
\end{align*}

- Since $P_L$ and $P_R$ are optimal: $M(P_L) \le M(O_L)$ and $M(P_R) \le M(O_R)$
*** Recursive formula
- Let $m[i,j]$ be the optimal number of scalar multiplications for calulating
  $A_iA_{i+1}....A_J$
- $m[i,j]$ can be expressed recusively as follows :
$m[i,j] = 0$ if i = j

$m[i,j] = min_{i\le k < j}\lbrace m[i,k]+m[k+1,j]+p_{i-1}p_kp_j\rbrace$ if i < j
- Each $m[i,j]$ depend only on subproblems with smaller $j-i$
- A bottom-up algorithm should solve subproblems in increasing $j-i$ order
*** Bottom-up algorithm
*MATRIX-CHAIN-ORDER* $(p)$
#+BEGIN_SRC java
n = p.length - 1
let m[1..n,1..n] and s[1..n,1..n] be new tables
for i = 1 to n
  m[i,i] = 0
for l = 2 to n #l is the chain length
  for i = 1 to n-l+1
    j = i + l -1
    m[i,j] = infinity
    for k = 1 to j - 1
      q = m[i,k] + m[k+1,j] + p[i-1]p[k]p[j]
      if q < m[i,j]
        m[i,j] = q
        s[i,j] = k
return m and s
#+END_SRC
*** Print optimal solution
*PRINT-OPTIMAL-PARENS* $(s,i,j)$
#+BEGIN_SRC java
if i == j
  print "Ai"
else print "("
  PRINT-OPTIMAL-PARENS(s, i, s[i,j])
  PRINT-OPTIMAL-PARENS(s,s[i,j]+1,j)
  print ")"
#+END_SRC
*** Summary
_Choice_ where to make the outermost parenthesis

$(A_1...A_k)(A_{k+1}...A_n)$

_Optimal substructure_ : to obtain an optimal solution, we to parenthesize the
two remaining expressions in a optimal way.

Hence,  if we let $m[i,j]$ be the optimal value for chain multiplication of
matrices $A_i,...,A_j$, we can express $m[i,j]$ recusively as follows

$m[i,j] = 0$ if $i=j$

$m[i,j] = min_{j\le k < j} \lbrace m[i,k] + m[k+1,j] + p_{i-1}p_k p_j\rbrace$
otherwise if $i < j$

_Overlapping subproblems_: Solve reccurence using top-down with memoization or
bottom-up which yields an algorithm that runs in time $\theta(n^3)$
* Week 7
** Longest common subsequence
_Input_ : 2 sequences, $X= < x_1,...x_m >$ and $Y = < y_1,...,y_n >$

_Output_ : A subsequence common to both whose length is longest. (A
subsequence doesn't have to be consecutive, but it has to be in order)
*** Brute force
For every subsequence of $X$, check wether it's a subsequence of $Y$

Time : $\Theta(n2^m)$
- $2^m$ subsequences of $X$ to check
- Each subsequence takes $\Theta(n)$ time to check: scan $Y$ for first letter,
  from there scan for second, and so on
*** Dynamic programming
**** Choice
Start at the end of both words and move to the left step-by-step

*Choice?*

If the same, pick letter to be in the subsequence

If not the same, optimal subsequence can be obtained by moving a step
to the left in one of the words.

Let $X_i= < x_1 , x_2 , ... , x_i >$ and $Y_i= < y_1,y_2,...,y_j >$

*Choice*

if $x_i = y_j$ then either
- OPT "matches" $x_i$ with $y_j$ and remaining OPT is in ($X_{i-1},Y_{j-1})$
No need to check the others possibilities, cf. proof

if $x_i \neq x_j$ then either
- OPT is in $(X_{i-1},Y_j)$ or
- OPT is in $X_i,Y_{j-1}$

We prove that we can assume that OPT “matches” $x_i$ with $y_j$ if they are
equal so we can simplify the first case

**** Optimal substructure
Let $X_i$ and $Y_j$ denote the prefixes $< x_1,x_2,...,x_i >$ and $< y_1,y_2,...,y_j >$

Theorem :

Let $Z_k= < z_1 , z_2 , ... , z_k >$ be any LCS of $X_i$ and $Y_j$

1. If $x_i = y_j$ then $z_k = x_i = y_j$ and $Z_{k-1}$ is an LCS of $X_{i-1}$ and $Y_{j-1}$
2. If $x_i \neq y_j$ $z_k \neq x_i$ \Rightarrow $Z$ is an LCS of $X_{i-1}$ and $Y_j$
3. If $x_i \neq y_j$ $z_k \neq y_j$ \Rightarrow $Z$ is an LCS of $X_i$ and $Y_{j-1}$

**** Recursive formulation
Define $c[i,j] =$ length of LCS of $X_i$ and $Y_j$. We want $c[m,n]$

\begin{align*}
  c[i,j]=\begin{cases}
    0 & \text{if $i=0$ or $j=0$}\\
    c[i-1,j-1] + 1 & \text{if $i,j>0$ and $x_i=y_j$}\\
    \text{max}(c[i-1,j],c[i,j-1]) & \text{if $i,j>0$ and $x_i \neq y_j$}
  \end{cases}
\end{align*}

**** Bottom-up pseudocode
*LCS-LENGTH* $(X,Y,m,n)$
#+BEGIN_SRC java
let b[1..m,1..n] and c[0..m,o..n] be new tables
for i = 1 to m
  c[i,0] = 0
for j = 0 to n
  c[0,j] = 0
for i = 1 to m
  for j = 1 to n
    if x_i == y_j
      c[i,j]=c[i-1,j-1]+1
      b[i,j] = "Top-left arrow"
    else if c[i-1,j] >= c[i,j-1]
      c[i,j] = c[i-1,j]
      b[i,j] = "Top arrow"
      else
        c[i,j] = c[i,j-1]
        b[i,j] = "right arrow"   
return c and b
#+END_SRC

- Time dominated by instructions inside the two nested loops which execute
  $m\cdot n$ times
- Total time is $\Theta(m\cdot n)$


**** Printing solution pseudocode
*PRINT-LCS* $(b,X,i,j)$
#+BEGIN_SRC java
if i == 0 or j == 0
  return
if b[i,j] == "top left arrow"
  PRINT-LCS(b,X,i-1,j-1)
  print x_i
else if b[i,j] == "top arrow"
  PRINT-LCS(b,X,i-1,j)
else PRINT-LCS(b,X,i,j-1)
#+END_SRC

- Each recursive call decreases $i + j$ by at least one.
- Hence, if we let $n = i + j$, the time needed is at most
$T(n) \le T(n − 1) + \Theta(1)$ which is $O(n)$
- We can thus print the found string in time $\Theta(|X | + |Y |)$
(the lower bound following from that $T(n) \ge T(n − 2) + \Theta(1)$)

** Optimal binary search trees
*** Idea
- Give sequence $K= < k_1,k_2,...,k_n >$ of $n$ distinct keys, sorted
  ($k_1 < k_2 < ... < k_n$)
- Want to build a binary search tree from the keys
- For $k_i$, have probability $p_i$ that a search is for $k_i$
- Want BST with minimum expected shared cost
- Actual cost = # of items examined

For key $k_i$, cost = $\text{depth}_T(k_i) + 1$, where $\text{depth}_T(k_i)$
denotes the depth of $k_i$ in BST T

\begin{align*}
\mathbb{E}[\text{search cost in }T] &= \sum\limits_{i=1}^n (\text{depth}_T(k_i) + 1 )p_i\\
&= 1+ \sum\limits_{i=1}^n (\text{depth}_T(k_i))p_i 
\end{align*}

Last equality, because $\sum\limits_{i=1}^n p_i = 1$
*** Observations
- Optimal BST might not have smallest height
- Optimal BST might not have highest-probability key at root

*Build by exhaustive checking ?*
- Construct each $n-$ node BST
- For each put in keys
- Then compute expected search cost
- But there are exponentially many trees
*** Optimal substructure
A binary search tree can be built by first picking the root and then building
the subtrees recursively

After picking root solution to subtrees must be optimal

add image slide

*** Recursive formulation
Let $e[i,j]=$ expected search cost of optimal BST of $k_i,...,k_j$

\begin{align*}
  e[i,j]=\begin{cases}
    0 & \text{if $i=j+1$}\\
    \text{min}_{i\le r\le j} \lbrace e[i,r-1]+e[r+1,j]+\sum\limits_{l=i}^j p_{\mathcal{l}} \rbrace & \text{if $i \le j$}
  \end{cases}
\end{align*}

*** Bottom-up pseudocode
*OPTIMAL-BST* $(p,q,n)$
#+BEGIN_SRC java
let e[1..n+1,0..n], w[1...n+1,0...n], and root[1..n,1..n] be new tables
for i = 1 to n+1
  e[i,i-1] = 0
  w[i,i-1] = 0
for l = 1 to n
  for i = 1 to n-l +1
    j = i + l -1
    e[i,j] = infinity
    w[i,j] = w[i,j-1] + pj
    for r = i to j
      t = e[i,r-1] + e[r+1,j] + w[i,j]
      if t < e[i,j]
        e[i,j] = t
        root[i,j] = t
return e and root
#+END_SRC

$e[i,j]$ records the expected search cost of optimal BST of $k_i,...,k_j$

$r[i,j]$ records the best root in optimal BST of $k_i,...,k_j$

$w[i,j]$ records $\sum\limits_{l=i}^j p_l$

*** Time analysis
Runtime dominated by three nested loops, total time is $\Theta(n^3)$

Alternatively, $\Theta(n^2)$ cells to fill in. Most cells take $\Theta(n)$ time
to fill in. Hence, total time is $\Theta(n^3)$

* Graphs
A graph $G = (V,E)$ consists of
- a vertex set $V$
- an edge set $E$ that contain (ordered) pairs of vertices

Graph can be undirected (goes both ways), directed, vertex-weighted, edge-weighted, etc.

How to store ?
** Adjacency Lists
- Array Adj of $|V|$ lists, one per vertex
- Vertex $u$ 's list has all vertices $v$ such that $(u,v) \in E$
- In pseudocode, we will denote the array as attribute $G.Adj$, so we will see
  notation such as $G.Adj[u]$
- Size ? $\Theta(|v| + |E|)$
** Adjacency matrix
$A$ $|V| \times |V|$ matrix $A=(a_{ij}$ where
\begin{align*}
a_{ij} = \begin{cases}
1 & \text{if $(i,j) \in E$}\\
0 & \text{otherwise}
\end{cases}
\end{align*}
** Comparison
| Adjacency list                                                          | Adjacency matrix                                         |
|-------------------------------------------------------------------------+----------------------------------------------------------|
| Space : $\Theta(V+E)$                                                   | Space : $\Theta(V^2)$                                    |
| Time : to list all vertices adjacent to $u$: $\Theta(\text{degree}(u))$ | Time : to list all vertices adjacent to $u$: $\Theta(V)$ |
| Time to determine wheter $(u,v) \in E$: $O(\text{degree}(u))$           | Time to determine wheter $(u,v) \in E$: $\Theta(1)$      |

** Breadth-First search
Input : Graph $G = (V,E)$ either directed or undirected and source vertex $s \in
V$, not weighted !

Output : $v.d$ = distance (smallest number of edges from $s$ to $v$, forall $v
\in V$

Idea :
- Send a "wave" out from $s$
- First hits all vertices 1 edge from $s$
- From there, hits all vertices 2 edges from $s$

*** Pseudocode
*BFS* $(V,E.s)$
#+BEGIN_SRC java
for each u in V - {s}
  u.d = infinity
s.d = 0
Q = nil // Empty queue
Enqueue(Q.s)
while Q != nil
  u = Dequeue(Q)
  for each v in G.adj[u]
    if v.d == infinity // Not discovered
      v.d = u.d + 1
      ENQUEUE(Q,v)
#+END_SRC
*** Analysis
Informal Idea of corectness :
- Suppose that $v.d$ is greater than the shortest distance from $s$ to $v$
- but since algorithm repeatedly considers the vertices closes to the root (by
  adding them to the queue) this cannot happen

Runtime analysis : $O(V+E)$
- $O(V)$ because each vertex enqueud at most once
- $O(E)$ because vertex dequeued at most once and we examine $(u,v)$ only when
  $u$ is dequeued. Therefore, every edge examined at most once if directed and
  at most twice if undirected.
*** Final notes on BFS
- BFS may not reach all the vertices
- We can save the shortest path tree by keeping track of the edge that
  discovered the vertex

** Depth-First search
Input : Graph $G=(V,E)$, either directed or undirected

Output: 2 timestamps on each vertex: $v.d$ = *discovery time* and $v.f$ =
*finishing time*

Idea :
- Methodically explore every edge
- Start over from different vertices as necessary
- As soon as we discover a vertex explore from it
  - Unlike BFS, which explores vertices that are close to a source first
*** Pseudocode
*DFS* $(G)$
#+BEGIN_SRC java
for each u in G.V
  u.color = WHITE
time = 0
for each u in G.V
  if u.color == WHITE
    DFS-VISIT(G,u)
#+END_SRC

*DFS-VISIT* $(G,u)$
#+BEGIN_SRC java
time = time + 1
u.d = time
u.color = GRAY // discover u
for each v in G.Adj[u] // explore (u,v)
   if v.color == WHITE
     DFS-VISIT(G,v)
u.color = BLACK
time = time + 1
u.f = time //finish u
#+END_SRC

*** Analysis
DFS forms a depth-first forest comprimised of $> 1$ depth-first trees. Each tree
is made of edges $(u,v)$ such that $u$ is gray and $v$ is white, then $(u,v)$ is
explored.

Runtime analysis: $\Theta(V+E)$
- $\Theta(V)$ because each vertex is discovered once
- $\Theta(E)$ because each edge is examined once if directed graph and twice if
  undirected graph
*** Classification of edges
- Tree edge : In the depth-first forest, found by exploring $(u,v)$
- Back edge : $(u,v)$ where $u$ is a descendant of $v$
- Forward edge : $(u,v)$ where $v$ is a descendant of $u$, but not a tree edge
- Cross edge : any other edge
*** Parenthesis theorem
For all $u,v$ exactly one of the following holds :
1) $[u.d , u.f]$ and  $[ v.d, v.f]$ are disjoint neither of $u$ and $v$
   are descendant of each other
2) $u.d < v.d < v.f < u.f$ and $v$ is a descendant of $u$.
3) $v.d < u.d < u.f < v.d$ and $u$ is a descendant of $v$.
*** White-path Theorem
Vertex $v$ is a descendant of $u$ if and only if at time $u.d$ there is a path
from $u$ to $v$ consisting of only white vertices (except for $u$, which was
just colored gray)
** Topological sort
Input: A directed acyclic graph (DAG) $G = (V,E)$

Output: a linear ordering of vertices such that if $(u,v) \in E$, then $u$
appears somewhere before $v$.
*** Acyclic lemma
**** Lemma :
A directed graph $G$ is acyclic if and only if a DFS of $G$ yields no
back edges.

**** Proof
First show that back-edege implies cycle :

Suppose there is a back edge $(u,v)$. Then $v$ is ancestor of $u$ in depth-first
forest. Therefore there is a path from $v$ to $u$, which creates a cycle.

Second show that cycle implies back-edge.

Let $v$ be the first vertex discovered in the cycle $C$ and let $(u,v)$ be the
preceding edge in $C$. At time $v.d$ vertices in $C$ form a white-path from $v$
to $u$ and hence $u$ is a descendant of $v$.
*** Algorithm
*TOPOLOGICAL-SORT* $(G)$

1. Call $DFS(G)$ to compute finishing times $v.f$ for all $v \in G.V$
2. Output vertices in order of decreasing finishing times.
*** Time analysis
Do not need to sort by finishing times :
- Can just output vertices as they are finished and understand that we want the
  reverse of the list
- Or put them onto the front of a linked list as they are finished. When done,
  the list containes vertices in topologically sorted order.

Time : $\Theta(V+E)$ (Same as DFS)
*** Corectness
We need to show that if $(u,v) \in E$ then $v.f < u.f$

When we explore $(u,v) what are the colors of $u$ and $v$ ?
- $u$ is gray
- Is $v$ gray too ?
  - No, because then $v$ would be ancestor of $u$ which implies that there is a
    back edge so the graph is not acyclic (by previous Lemma)
- Is $v$ white ?
  - Then becomes descendant of $u$. By parenthis theorem, $u.d < v.d < v.f < u.f$
- Is $v$ black ?
  - Then $v$ is already finished. Since we are exploring $(u,v)$, we have not
    yet finished $u$. Therefore, $v.f < u.f$.
  
** Strongly connected component
Definition : A strongly connected component (SCC) of a directed graph $G=(V,E)$
is a maximal set of vertices $C\subset V$ such that for all $u,v \in C$, both
$u \to v$ and $v \to u$

Lemma : G^{SCC} is DAG (directed acyclic graph)

*** Algorithm
*SCC* $(G)$
1. Call $DFS(G)$ to compute finishing times $u.f$ for all $u$
2. Compute $G^T$
3. Call $DFS(G^T)$ but in the main loop, consider vertices in order of
   decreasing $u.f$ (as computed in first $DFS$.
4. Output the vertices in each tree of the depth-first forest formed in second
   $DFS$ as a separate $SCC$

Graph $G^T$ is the transpose of $G$:
- $G^T = (V,E)$, $E^T = \lbrace (u,v): (v,u) \in E \rbrace$
- $G^T$ is $G$ with all edges reversed.

Observations:
- Can create $G^T$ in $\Theta(V+E)$ time if using adjacency lists
- $G$ and $G^T$ has the same $SCC$ s
*** Analysis
Runtime : Each step takes $\Theta(V+E)$ so total running time is $\Theta(V+E)$

Why does it work ? Intuition :
- The first DFS order SCC's in topological order (recall G^{SCC} is acyclic)
- Second DFS the output the vertices in each SCC

** Flow networks
*** Definition of network
- Directed graph $G=(V,E)$
- Each edge $(u,v)$ has a capacity $c(u,v) \ge 0$ ($c(u,v) = 0$ if $(u,v) \not\in E$)
- Source $s$ and sink $t$ (flow goes from $s$ to $t$)
- No antiparallel edges (assumed without loss of generality for simplicity)
  - if there are two parallel edges $(u,v)$ and $(v,u)$, choose one of them say $(u,v)$
  - Create a new vertex $v'$
  - Replace $(u,v)$ be two new edges $(u,v')$ and $(v',v)$ with $c(u,v') = c(v',u) = c(u,v)$
  - Repeat this $O(E)$ times to get an equivalent flow with no antiparallel edges
  - Add image
*** Definition of a flow
A flow is a function $f: V \times V \to \mathbb{R}$ satisfying :
- Capacity constraint : $\forall u,v \in V\ : 0 \le f(u,v) \le c(u,v)$
- Flow conservation: $\forall u \in V \backslash \lbrace s,t \rbrace$,
  $\sum\limits_{v\in V} f(u,v) = \sum\limits_{v\in V} f(u,v)$ (flow into $u$ =
  flow out of $u$)
*** Value of a flow
Value of a flow
\begin{align*}
f &= |f|\\
&= \sum\limits_{v\in V} f(s,v) - \sum\limits_{v\in V}f(v,s)\\
&= \text{flow out of source - flow into source}
\end{align*}
** Maximum-flow problem
*** Ford-Fulkerson method'54
*FORD-FULKERSON-METHOD* $(G,s,t)$
1. Initialize flow $f$ to 0
2. while there exists an *augmenting path* $p$ in the *residual network* G_f
3. *augment flow* $f$ along $p$
4. return f

Basic idea :
- As long as there is a path from source to sink, with available capacity on all
  edges in the path
- send flow along one of these paths and then we find another path and so on

*** Residual network
- Given a flow $f$ and a network $G=(V,E)$
- the residual network consists of edges with capacities that represent how we
  can change the flow of edges

*Residual capacity*

\begin{align*}
c_f(u,v) = \begin{cases}
c(u,v) - f(u,v) & \text{if } (u,v) \in E \text{ (amount of capacity left)}\\
f(u,v) & \text{if } (v,u) \in E\text{ (amount of that can be reversed)}\\
0 & \text{otherwise} 
\end{cases}
\end{align*}

*Residual network*

$G_f = (V,E_f)$ where $E_f = \lbrace (u,v \in V \times V : c_f(u,v) > 0 \rbrace$
*** Why is returned flow optimal ? (Min-cuts)
cf. [[https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem#Proof][Wikipedia]] for nice proof
A cut of flow network $G(V,E)$ is
- a partition of $V$ into $S$ and $T=V\backslash S$
- such that $s\in S$ and $t \in T$
**** Net flow across a cut
The net flow across cut $(S,T)$ is

$f(S,T) = \sum\limits_{u\in S, v\in T} f(u,v) - \sum\limits_{u\in S,v\in T} f(v,u)$
**** Capacity a cut
The capacity of a cut $(S,T)$ is

$c(S,T) = \sum\limits_{u\in S,v\in T} c(u,v)$
**** Flow and cut capacity
For any flow $f$ and any cut $(S,T)$:

\begin{align*}
|f| &= f(S,T)\\
&= \sum\limits_{u\in S, v\in T} f(u,v) - \sum\limits_{u\in S,v\in T} f(v,u)\\
&\le \sum\limits_{u\in S, v\in T} f(u,v) \\
&\le \sum\limits_{u\in S, v\in T} c(u,v) \\
&= c(S,T)
\end{align*}

Therefore max-flow $\le$ min-cut

We shall prove :

Theorem (max-flow min-cut theorem)

max-flow = min-cut
*** Running time
It takes $O(E)$ time to find a path in the residual network (use for example breadth-first search)

Each time the flow value is increased by at least 1

Running time is $O(E \cdot |f_{\text{max}}|)$ where $|f_{\text{max}}|$ denotes the value
of a maximum flow

However, if we either take the *shortest path* or the *fattest path* then this
will not happen if the capacities are integer :

BFS shortest path, $\le \frac{1}{2} E \cdot V$ number of iterations

Fattest path, $\le E \cdot \log(E \cdot U)$ number of iterations

- $U$ is the maximum flow value
- Fattest path: choose augmenting path with largest minimum capacity
  (bottleneck)
* Week 10
** DIsjoint-set data strucutre
- Also known as “union find”
- Maintain collection S = {S 1 , . . . , S k } of disjoint dynamic (changing over time) sets
- Each set is identified by a representative, which is some member of the set
  
Doesn’t matter which member is the representative, as long as if we ask for the
representative twice without modifying the set, we get the same answer both
times
** Operations
- Make-Set(x): make a new set $S_i = \lbrace x \rbrace$, and add $S_i$ to $S$
- Union(x, y): if $x \in S_x$, $y \in S_y$ , then $S = S − S_x − S_y \cup
  \lbrace S_x \cup S_y \rbrace$
  + Representative of new set is any member in $S_x \cup S_y$ , often the representative of one of $S_x$ and $S_y$
  + Destroys $S_x$ and $S_y$ (since sets must be disjoint)
- Find(x): return representative of set containing $x$
** Connected components
*CONNECTED-COMPONENTS* $(G)$
#+BEGIN_SRC java
for each vertex v in G.V
  MAKE-SET(v)
for each edge(u,v) in G.E
  if FIND-SET(u) != FIND-SET(v)
    UNION(u,v)
#+END_SRC
** Linked list representation
- Each set is a single linked list represented by a set object that has
  - a pointer to the head of the list (assumed to be the representative
  - a pointer to the tail of the list
- Each object in the list has attributes for the set member, pointer to the set
  object and next

Implementation of operations :
- Make-Set(x): Create a singleton list in time $\Theta(1)$
- Find(x): follow the pointer back to the list object, and then follow the head
  pointer to the representative (time $\Theta(1)$)
- Union, 2 ways to implement:
  1. Append $y$ ’s list onto the end of $x$ ’s list. Use $x$ ’s tail pointer to find the end.
     - Need to update the pointer back to the set object for every node on $y$ ’s
       list.
     - If appending a large list onto a small list, it can take a while
       ($\Theta$(length of 2nd list))
  2. Weighted-union heuristic always append the smaller list to the larger list
     (break ties arbitrarily)
*** Theorem weighted-union heuristic
With weighted-uinon heuristic, a sequence of $m$ operations on $n$ elements take
$O(m+n \lg n)$ time
** Disjoint-set forest
- One tree per set. Root is representative
- Each node only points to its parent

Make-Set(x): Make a single-node tree $\Theta(1)$

Find(x): follow pointers to the root $O(h)$

Union(x, y): make one root a child of another $\Theta(1)$
*** Great heuristics
*Union by rank*: make the root of the smaller tree a child of the root of the larger tree
- Don’t actually use size
- Use rank, which is an upper bound on height of node
- Make the root with the smaller rank a child of the root with the larger rank

*Path compression: Find path* = nodes visited during Find on the trip to the
root, make all nodes on the find path direct children to root.
*** Pseudocode
*MAKE-SET* $(x)$
#+BEGIN_SRC java
x.p = x
x.rank = 0
#+END_SRC

*FIND-SET* $(x)$
#+BEGIN_SRC java
if x != x.p
  x.p = FIND-SET(x.p)
return x.p
#+END_SRC

*UNION* $(x,y)$
#+BEGIN_SRC java
LINK(FIND-SET(x),FIND-SET(y))
#+END_SRC

*LINK* $(x,y)$
#+BEGIN_SRC java
if x.rank > y.rank
  y.p = x
else x.p = y
  if x.rank == y.rank
    y.rank = y.rank+1
#+END_SRC
*** Running time
If use both union by rank and path compression,

$O(m \cdot \alpha(n))$

where $\alpha(n)$ is an extremly slowly growing function.

- $\alpha(n) \le 5$ for any practical purpose
- The bound O(m\cdot \alpha(n))$ is tight
**** Runnin time of connected components
*CONNECTED-COMPONENTS* $(G)$
#+BEGIN_SRC java
for each vertex v in G.V
  MAKE-SET(v)
for each edge(u,v) in G.E
  if FIND-SET(u) != FIND-SET(v)
    UNION(u,v)
#+END_SRC
- $V$ elements
- $\le V + 3E$ operations on Union-Find data structure
- Total running time if implemented as linked list with weighted-union
  heuristic :

$O(V \log V+E)$

- Total running time if implemented as forest with union-by-rank and
  path-compression :

$O((V+E)\alpha(V)) \approx O(V+E)$
** Minimum spanning trees
Spanning tree of a graph = A set $T$ of edges that is
- acyclic
- Spanning (connects all vertices)

INPUT: an undirected graph $G = (V , E )$ with weight $w (u, v )$ for each edge $(u, v ) \in E$

OUTPUT: a spanning tree of minimum total weight
** Prim's algorithm
Start with any vertex $v$ , set tree $T$ to singleton $v$

Greedily grow tree $T$ :

at each step add to $T$ a minimum weight crossing edge with respect to the cut induced by $T$
*** Why does it work ?
+ A cut $(S, V \backslash S)$ is a partition of the vertices into two nonempty
  disjoint sets $S$ and $V \backslash S$
+ A crossing edge is an edge connecting vertex $S$ to vertex in $V \backslash S$
**** Cut property
Consider a cut $(S, V \backslash S)$ and let
- $T$ be a tree on $S$ which is part of a MST
- $e$ be a crossing edge of minimum weight
Then there is MST of $G$ containing $e$ and $T$

Proof :
- If $e$ is already in MST we are done.
- Otherwise add $e$ to the MST
  - This creates a cycle
  - At least one other crossing edge $f$ in cycle $w(f) \ge w(e)$ (actually must
    be equal)
  - Replace $f$ by $e$ in MST
  - This gives new MST which contains $T$ and $e$
**** $T$ is always a subtree of a MST
Proof by induction on number of nodes in T . Final T is MST by this result.
[[file:images/al_proofMST.png]]
*** Implementation challenge
How do we find minimum crossing edge at every iteration?

Check all outgoing edges:
- $O(E)$ comparisons at every iteration
- $O(E V)$ running time in total

More clever solution:
- For every node $w$ , keep value dist($w$ ) that measures the “distance” of $w$ from
  current tree
- When a new node $u$ is added to tree, check whether neighbors of $u$ decreases
  their distance to tree; if so, decrease distance
- Maintain a min-priority queue for the nodes and their distances
*** Pseudocode and analysis
*PRIM* $(G,w,r)$
#+BEGIN_SRC java
Q = {}
for each u in G.V
  u.key = infinity
  u.pi = NIL
  INSERT(Q.u)
DECREASE-KEY(Q,r,0) //r.key = 0
while Q != {}
  u = EXTRACT-MIN(Q)
  for each v in G.Adj[u]
    if v in Q and w(u,v) < v.key
      v.pi = u
      DECREASE-KEY(Q,v,w(u,v))
#+END_SRC

- Initialize $Q$ and first for loop: $O(V\lg V)$
- Decrease key of $r$ : $O(\lg V )$
- while loop:
  - $V$ Extract-Min calls \Leftrightarrow $O(V\lg V)$
  - $\le E$ Decrease-Key calls \Leftrightarrow $O(E\lg V)$
- Total: $O(E\lg V)$ (can be made $O(V\lg V)$ with careful queue implementation)
** Kruskal's algorithm
Start from empty forest $T$

Greedily maintain forest $T$ which will become MST at the end: at each step add
cheapest edge that does not create a cycle
*** Why does it work ?
Claim: $T$ is always a sub-forest of a MST

Proof by induction on the number of components/edges in $T$
[[file:images/al_proofMSTKruskal.png]]
*** Implementation challenge
In each iteration, we need to check whether cheapest edge creates a cycle

This is the same thing as checking whether its endpoints belong to the same
component \Leftrightarrow *use disjoint sets (union-find) data structure*

Let the connected components denote sets
- Initially each singleton is a set
- When edge $(u, v)$ is added to $T$ , make union of the two connected
  components/sets
*** Implemenation and analysis
*KRUSKAL* $(G,w)$
#+BEGIN_SRC java
A = {}
for each vertex v in G.V
  MAKE-SET(v)
sort the edges of G.E into nondecreasing order by weight w
for each (u,v) taken from the sorted list
  if FIND-SET(u) != FIND-SET(v)
    A = A union {(u,v)}
    UNION(u,v)
return A
#+END_SRC

- Initialize A: $O(1)$
- First for loop: $V$ Make-Sets
- Sort $E$ : $O(E \lg E)$
- Second for loop: $O(E)$ Find-Sets and Unions
- Total time: $O((V + E )\alpha(V)) + O(E\lg E) = O(E\lg E ) = O(E\lg V)$
If edges already sorted time is $O(E \alpha(V))$ which is almost linear
** Problem solving
A feedback edge set of a graph $G$ is a subset $F$ of the edges such that every
cycle in $G$ contains at least one edge in $F$ . In other words, removing every edge
in $F$ makes the graph $G$ acyclic. Describe and analyze a fast algorithm to compute
the minimum weight feedback edge set of of a given edge-weighted graph.

Let $G = (V , E)$ be an arbitrary connected graph with weighted edges. Assume
further that no two edges have the same weight.
1) Prove that for any partition of the vertices $V$ into two subsets, the
   minimum-weight edge with one endpoint in each subset is in the minimum
   spanning tree of $G$.
2) Prove that the maximum-weight edge in any cycle of $G$ is not in the minimum
   spanning tree of $G$.


* Week 11
** The shortest path problem 
Input: directed graph $G = (V,E)$, edge-weights $w(u,v)$ for $(u,v) \in E$

Shortest paths from $a$: (may have many solutions)
*** Variants
- Single-source: Find shortest paths from source vertex to every vertex
- Single-destination: Find shortest paths to given destination vertex
  - Can be solved by single-source by reversing edge directions
- Single-pair: Find shortest path from u to v
  - No algorithm known that is better in worst case than solving single-source
- All-pairs: Find shortest path from u to v for all pairs u, v of vertices
  - Can be solved by solving single-source for each vertex. Better algorithms known
*** Negative weights and applications
We will allow negative weights

OK, as long as no negative-weight cycle is reachable from source:
- Then we can just keep going around it to have paths of length $-\infty$

Some algorithms only work with positive weights (Dijkstra’s algorithm)
*** Bellman-Ford algorithm
Input: directed graph with edge weights, a source s, no negative cycles

For each vertex v keep track of
- $l(v)$ = current upper estimate of length of shortest path to v
- $π(v)$ = is the predecessor of $v$ in this shortest path

Start by trivial initialization:
*INIT-SINGLE-SOURCE* $(G,s)$
#+BEGIN_SRC java
for each v in G.V
  v.d = infinity
  v.π = NIL
s.d = 0
#+END_SRC

Can we improve the shortest path estimate for $v$ by going through $u$ and taking $(u,v)$?

*RELAX* $(u,v,w)$
#+BEGIN_SRC java
if v.d > u.d w(u,v)
   v.d = u.d + w(u,v)
   v.π = u
#+END_SRC

Bellman-Ford updates shortest-path estimates iteratively by using Relax.

*BELLMAN-FORD* $(G,w,s)$
#+BEGIN_SRC java
INIT-SINGLE-SOURCE(G,s)
for i = 1 to |G.V| - 1
  for each edge (u,v) in G.E
    RELAX(u,v,w)
#+END_SRC
**** Correctness
Only guaranteed to work if no negative cycles!

As we shall see later, it can also be used to detect negative cycles
**** Optimal substructure
If $< s, v_1,...,v_k,v_{k+1} >$ is a shortest path from $s$ to $v_{k+1}$
Then $< s,v_1,...,v_k >$ is a shortest path from $s$ to $v_k$

Proof:
- Suppose toward contradiction: there exists shorter path $p'$ from $s$ to $v_k$
- Then weight of $p' + (v_k , v_{k+1} )$ is smaller than $p + (v_k , v_{k+1} )$
  which contradicts that $p + (v_k , v_{k+1})$ was a shortest path from $s$ to $v_{k+1}$
**** Proof of corectness
*Invariant*: $\mathcal{l}(v)$ is at most the length of the shortest path from $s$ to $v$ using
at most $i$ edges after the $i$ ’th iteration


*If there are no negative cycles reachable from* $s$, then for any $v$ there is a
shortest path from $s$ to $v$ using at most $n − 1$ edges

*Proof*: If there is a path with $n$ or more edges, then there is a cycle and
since its weight is non-negative it can be removed

Therefore, Bellman-Ford will return correct answer if no negative cycles after
$n − 1$ iterations
**** Detecting negative cycles
There is a negative cycle reachable from the source if and only if the
$\mathcal{l}$ -value of at least one node changes if we run one more ($n$:th)
iteration of Bellman-Ford

*BELLMAN-FORD* $(G,w,s)$
#+BEGIN_SRC java
INIT-SINGLE-SOURCE(G,s)
for i = 1 to |G.V| - 1
  for each edge (u,v) in G.E
    RELAX(u,v,w)
for each edge (u,v) in G.E
  if v.d > u.d + w(u,v)
    return False
return True
#+END_SRC

There is no negative cycle reachable from the source if and only if the
$\mathcal{l}$ -value
of no node changes if we run one more ($n$:th) iteration of Bellman-Ford

From the correctness proof, we have that if there are no negative cycles
reachable from the source, then the $\mathcal{L}$ -values don’t change in $n$:th iteration.

We need to prove: If the $\mathcal{l}$ -value of the vertices don’t change in
the $n$:th iteration, then there is no negative cycle that is reachable from the source

*Proof*. In this case $\forall (u,v) \in E: \mathcal{l}(u)+w(u,v) \ge
\mathcal{l}(v)$

So for a cycle $v_0 - v_1 - ... - v_{t-1} - v_t = v_0$,

$\sum\limits_{i=1}^t \mathcal{l}(v_i) \le
\sum\limits_{i=1}^t(\mathcal{l}(v_{i-1}) + w(v_{i-1},v_i)) = \sum\limits_{i=1}^t
\mathcal{l}(v_{i-1}) + \sum\limits_{i=1}^t w(v_{i-1},v_i)$

$\sum\limits_{i=1}^t \mathcal{l}(v_i)=\sum\limits_{i=1}^t \mathcal{l}(v_{i-1})$,
hence the cycle is non-negative $0 \le \sum\limits_{i=1}^t w(v_{i-1},v_i)$

**** Runtime analysis
- Init-Single-Source updates $\mathcal$, π for each vertex in time $\Theta(V)$
- Nested for loops runs Relax $V − 1$ times for each edge. Hence total time for
  these loops is $\Theta(E \cdot V)$
- Final for loop runs once for each edge. Time is $\Theta(E)$

Total time: $\Theta(E\cdot V)$
**** Final comments on Bellman-Ford
- Can be used to find negative cycles
  - Run for $n$ -iterations and detect cycles in “shortest path tree” these will
    correspond to negative cycles
- Easy to implement in distributed settings: each vertex repeatedly ask their
  neighbors for the best path
  - Good for routing and dynamic networks



*** Dijkstra's Algorithm
- Only works when all weights are nonnegative
- Greedy and faster than Bellman-Ford
- Similar idea to Prim’s algorithm (essentially weighted version of BFS)

Start with source $S =\lbrace s \rbrace$

Greedily grow S:

at each step add to $S$ the vertex is closest to $S$ (minimum $v \not\in S : u.d + w(u,v)$)

**** Implemenation
Implementation with priority-queue as Prim’s algorithm with shortest path keys:
*DIJKSTRA* $(G,w,s)$
#+BEGIN_SRC java
INIT-SINGLE-SOURCE(G,s)
S = {}
Q = G.V //i.e insert all vertices into Q
while Q != {}
  u = EXTRACT-MIN(Q)
  S = S union {u}
  for each vertex v in G.Adj[u]
    RELAX(u,v,w)
#+END_SRC

**** Running time
Running time Like Prim’s dominated by operations on priority queue:
- If binary heap, each operation takes $O(\lg V)$ time \Rightarrow $O(E\lg V)$
- More careful implementation time is $O(V \lg V + E)$
* Proba analysis and randomized algorithms
** The Hiring Problem
To complete
