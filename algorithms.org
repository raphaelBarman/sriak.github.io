# -*- mode: org; -*-
#+TITLE: Algorithms
#+SETUPFILE: htmlnotes.setup

* Chapter 1
** Definition of algorithm
- Relation between an input and an ouptut
- Tool to solve a computational problem
** Instance of a problem
Input of the problem. 
Examples :
- Sorting problem, instance : <32,54,65,23,54>
- Sum of all numbers to /n/ : 4, /n/ \neq instance
* Chapter 2
** Insertion sort algorithm
#+BEGIN_SRC java
for j = 2 to A.length
  key = A[j]
  // Insert A[j] into the sorted sequance A[1..j-1]
  i = j -1
  while i > 0 and A[i] > key
    A[i+1] = A[i]
    i = i -1
  A[i+1] = key
#+END_SRC
** Loop Invariant -> prove corectness of algorithm
- Loop invariant : "a statement that is satisfied during the loop"
- Need to verify :
  - *Initialization* : True at the beginning of the 1st iteration of the loop
  - *Maintenance* :  If it is true before an iteration of of the loop, it remains
    true before the next iteration
  - *Termination* : When the loop terminates, the invariant gives us a useful
    property that helps show that the algorithm is correct
***  Example 1 : Sum up to /n/
The algorithm is the following : \\
*CalculateSum* (/n/):
#+BEGIN_SRC java
ans = 0
for i = 1,2,..,n
 ans = ans + i
return ans
#+END_SRC
- *Loop invariant* : At the start of each iteration $ans = (i-1) * i/2$
- *Initialization* : At the begining of the first iteration, $i =1$ and $ans =
  0 = i(i -1)/2)$ so the invariant is satisfied
- *Maintenance* : Suppose invariant true at the beginning of iteration when
  $i=k$, i.e, $ans = (k-1)k/2$. Then the iteration updates $ans$ by adding $k$,
  i.e., $ans = (k-1)k/2 +k = k(k+1)/2$ so incrementing $i$ then for next
  iteration preserves the loop invariant.
- *Termination* : When the loop terminates $i=n+1$. Hence the algorithm returns
  $ans = n(n+1)/2$
*** Example 2 : Insertion sort
#+BEGIN_HTML
<input checked type=radio name=slider id=slide1 />
		<input type=radio name=slider id=slide2 />
		<input type=radio name=slider id=slide3 />
		<input type=radio name=slider id=slide4 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/insertionsortproof-0.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-1.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-2.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-3.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
			
		</div> <!-- #active -->
#+END_HTML
** Divide and conquer approach
*** Principle : 
- *Divide* the problem into a number of subproblems that are smaller instances
  of the same problem
- *Conquer* the subproblems by solving them recursively. If the subproblem sizes
  are small enough, however, just solve the subproblems in a straightforwad
  manner
- *Combine* the solutions to the subproblems into the solution for the original problem
*** Application on merge and sort : 
To sort $A[p...r]$:
- *Divide* by splitting into two subarrays $A[p...q]$ and $A[q+1,...r]$, whre
  $q$ is the halfway point of $A[p..r]$
- *Conquer* by recursively sorting the two subarrays $A[p..q]$ and $A[q+1,...r]$
- *Combine* by mergie the two sorted subarrays $A[p...q]$ and $A[q+1,...r]$ to
  produce a single sorted subarray $A[p...r]$

*Merge-Sort* $(A,p,r)$
#+BEGIN_SRC java
if p < r  //check for base case
  q = floor((p+r)/2)  // divide
  Merge-Sort(A,p,q)  // conquer
  Merge-Sort(A,q+1,r)  //conquer
  Merge(A,p,q,r) // combine
#+END_SRC
*Merge* ($A$,$p$,$q$,$r$)
#+BEGIN_SRC java
n_1 = q - p + 1
n_2 = r - q
let L[1..n_1+1] and R[1..n_2+1] be new arrays
for i = 1 to n_1
  L[i] = A[p+i-1]
for j = 1 to n_2
  R[j] = A[q+j]
L[n_1+1]= infinity
R[n_2 +1]= infinity
i = 1
j = 1
for k = p to r
  if L[i] <= R[j]
    A[k] = L[i]
    i = i +1
  else
    A[k] = R[j]
    j = j +1
#+END_SRC
**** Corectness
- What does merge(A,p,q,r) do ?
  + It takes array $A$ and indexes $p \le q < r$ s.t. $A[p...q]$ and $A[q+1...r]$ are sorted.
  + Then it outputs $A[p...r]$ contains the same elements in sorted order
- Proof by induction on $n = r - p$ :
  - Base case : $n = 0$, in this case $r = p$ so $A[p...r]$ (single element) is trivially sorted.
  - Inductive case : assume statement true $\forall\ n \in \{0,1,...,n-1\}$ and
    prove true for $n=k$
    - By induction hypothesis ( $q - p < n$ ) Merge-Sort(A,p,q) and
      Merge-Sort(A,q+1,r) sucessfully sort the two subarrays.
    - Therefore a correct merge procedure will sucessfully sort $A[p...q]$ as required.
**** Time analysis
[[file:pdf/mergesorttime.jpg]]
*** Analysis of divide-and-conquer algorithms
Use a *recurrence* equation to describe the running time :
- Let $T(n)$ = "running time on a problem of size $n$"
- If $n$ is small enough say $n\le c$ for some constant $c$ then $T(n) = \Theta(1)$ (by brute force)
- Otherwise, suppose we divide into a sub problems each size $n/b$ (conquer step)
- Let $D(n)$ be the time to divide and let $C(n)$ the time to combine solutions
- We get the reccurence : $T(n) = \Theta(1)$ if $n\le c$, else $T(n) = aT(n/b)+D(n)+C(n)$
* Chapter 3 
Consider the following recurrence :

$T(n) = c$ if $n = 1$ \\
$T(n) = 2T(n/2) + c \cdot n$ otherwise

Note that this reccurence upper bounds and lower bounds the reccurence for
Merge-Sort by selecting $c$ sufficiently large and small, respectively.

Three solving techiques :
** The substitution method
1. Guess the form of the solution
2. Use mathematical induction to find the constant (substition method in the book)

\begin{align*} T(n) &= 2T(n/2) + c \cdot n \\
&= 2(2T (n/4) + c \cdot n/2) + c \cdot n = 4T(n/4) + 2 \cdot c \cdot n\\
&= 4(2T (n/8) + c \cdot n/4) + 2 \cdot c \cdot n =  8T(n/8) + 3 \cdot c \cdot n\\
&\vdots \\
&= 2^k T(n/2^k) + k \cdot c \cdot n
\end{align*}

A qualified guess is that $T(n) = \Theta (n \log n)$.
- and show that the solution works
  - First upper bound :
    We want to prove there exists $a > 0$ s.t $T(n) \le a \cdot n \cdot \log n$
    $\forall n \ge 2$
    - *Base case* : For any constant $n \in \{2,3,4\}$, $T(n)$ has a constant
      value, selecting a larger than this value will satisfy the base cases when
      $n \in \{2,3,4\}$.
    - *Inductive step* : /Assume statement true $\forall n \{2,3,...,k-1\}$ and
      prove the statement for $n=k$/
      \begin{align*}
      T(n) &= 2T(n/2) + c \cdot n \\
      &\le 2 \cdot \frac{an}{2} \log(n/2) + c \cdot n = a \cdot n \log(n/2) + c \cdot n \\
      &= a \cdot n \log n -a \cdot n + c \cdot n \\
      &\le a \cdot n \log n \text{ (if we select $a \ge c$)}
      \end{align*}
      We can thus select $a$ to be a positive constant so taht both the base
      cases and the inductive step holds. Hence, $T(n) = O(n\log n)$.
  - Second lower bound :
    - We want to prove there exists a constant $b > 0$ s.t $T(n) \ge b \cdot n
      \cdot \log n$ $\forall n \ge 0$
    - *Base case* : For $n=1,T(n)=c$ and $b \cdot n \log n =0$ so the base case
      is satisfied for any b.
    - *Inductive step* : /Assume statement true $\forall n \in \{0,1,...,k-1\}$
      and prove the statement for $n =k$/
      \begin{align*}
      T(n) &= 2T (n/2) + c \cdot n\\
      &\ge 2 \cdot \frac{b\cdot n}{2} \log(n/2) + c \cdot n = b \cdot n \log(n/2) + c \cdot n\\
      &= b \cdot n \log n - b \cdot n + c \cdot n \\
      & \ge b \cdot n \log n \text{ (if we select $b \le c$)}
      \end{align*}
We can thus select $b$ to be positive constant so that both the base cases and
the inductive step holds. Hence, $T(n) = \Omega(n\log n)$.

- Other example
Prove that $T(n) = O(n)$
First $\exists c$ s.t.
\begin{align*}
 T(n) \le F(n)\\
\text{and}\\
F(n) = c \text{ if $n=0, n=1$, else } F(\frac{n}{4} +1 ) + F(\frac{3n}{4} -1) + c
\end{align*} 
We shall prove that $F(n) = O(n)$
claim : there exists constants $b',b > 0$ and $n_0 \ge 0$ s.t. $ \forall n \ge n_0$
$F(n) \le b \cdot n - b'$

Proof : Inductive step : Assume $F(n) = b \cdot n$ $\forall n \in
\{n_0,...k-1\}$ prove true for $n=k$.
\begin{align*}
 F(n) &= F(\frac{n}{4} +1) + F(\frac{3n}{4} -1) + C \\
&= b(\frac{n}{4} + 1) b' + b (\frac{3n}{4} -1) b' + C\\
&= b \cdot n + c - 2b'\\
&\le b \cdot n - b' \text{ if $ b' \ge C$}
\end{align*}

- Again other example :
$T(n) = \Theta(1)$ if $n=1$ \\
$8 T(n/2) + c \cdot n^2$ if $n > 1$ \\
Prove that $T(n)$ is $O(n^3)$.

_Claim_ : $\exists d > 0$ and $n_0 > 0$ s.t. $T(n) \le d n^3 \forall n \ge n_0$

_Base case_: for $n=1,...,k$
$T(1), T(2) ... T(k)$ I can always select $d \ge max \left\lbrace T(1), T(2),...
T(k) \right\rbrace$

__Inductive step__ :
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le 8 \cdot d \cdot (n/2)^3 + c \cdot n^2 \\
= d \cdot n^3 + c \cdot n^2
\end{align*}
How to remove the low order terms ? We add to our claim a $-d' \cdot n^2$ s.t.
$T(n) \le d n^3 - d' \cdot n^2 \forall n \ge n_0$.
Now
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le d \cdot n^3 - 2 \cdot d' \cdot n^2 + c \cdot n^2\\
\le d \cdot n^3 - d' \cdot n^2 \text{if $d' \ge c$}
\end{align*}
** The recursion tree method 
#+BEGIN_HTML
<input checked type=radio name=slider id=slide01 />
		<input type=radio name=slider id=slide02 />
		<input type=radio name=slider id=slide03 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/recursiontree01.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree02.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree03.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
			
		</div> <!-- #active -->
#+END_HTML
- Examples to do with this method :
  - $T(n) = 2 T(n/2) + c \cdot n^2$, cost of base root $cn^2$, but cost of last
    root : $cn$
** The master method
Used to black-box solve reccurences of form $T(n) = a \cdot T(n/b) + f(n)$
(doesn't work with $T(n) = T(n/3) + T(2n/3) + cn$ for example).

_Theorem (Master theorem)_

Let $a \ge 1$ and $b > 1$ be constants, let $T(n)$ be defined on the
  nonnegative integers by the reccurence :
\begin{align*}
T(n) = a \cdot T(n/b) + f(n)
\end{align*}
Then, $T(n)$ has the following asymptotic bounds :
- If $f(n) = O(n^{\log_b a- \epsilon})$ for some constant $\epsilon > 0$, then
  $T(n) = \Theta(n^{\log_b a})$
- If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{\log_b a} \log n)$
- If $f(n) = \Omega(n^{\log_b a+\epsilon})$ for some constant $\epsilon > 0$ and
  if $a \cdot f(n/b) \le c \cdot f(n)$ for some constant $c < 1$ and all
  sufficiently large $n$, then $T(n) = \Theta(f(n))$
* Chapter 4 
** Maximum-subarray problem

"If we let $A[i]$ = (price after day $i$) - (price after day $i-1$) then if the
maximum subarray is $A[i..j]$ then we should have bought just before day $i$ and
sold just after day $j$.

*Input* : An array $A[1...n]$ of numbers

*Output* Indices $i$ and $j$ such that $A[i...j]$ has the greatest sum of any
 nonempty, contiguous subarray of $A$, along with the sum of the values in $A[i...j]$.
*** Brute force
Simply check all possible subarrays.

$\binom{n}{2} =  \Theta (n^2)$ many.

*Maximum-subarray-slow* $(A[1...n])$
#+BEGIN_SRC java
B.val = - infinity, B.i = 1, B.j = n
for i = 1 to n
  tmp = 0
  for j = i to n
    tmp = tmp + A[j]
    if tmp > B.val
       B.val = tmp
       B.i = i
       B.j = j
return (B.i,B.j.B.val)
#+END_SRC
Running time is $\Theta(n^2)$ and we use $\Theta(n)$ space.
*** Divide and conquer
- *Divide* the subarray into two subarrays of as equal size as possible. Find the midpoint mid of the subarrays, and consider the subarrays $A$[low.. mid]
  and $A$[mid+1...high].
- *Conquer* by finding maximum subarrays of $A$[low . . . mid] and $A$[mid + 1 . . . high].
- *Combine* by finding a maximum subarray that crosses the midpoint,and using the best solution out of the three

 *FIND-MAXIMUM-SUBARRAY* $(A,low,high)$
#+BEGIN_SRC java
if high == low
  return (low, high, A[low]) // base case: only one element
else mid = ceil((low + high)/2)
  (left-low, left-high, left-sum) =
    FIND-MAXIMUM-SUBARRAY(A, low, mid)
  (right-low, right-high, right-sum) =
    FIND-MAXIMUM-SUBARRAY(A, mid + 1, high)
  (cross-low, cross-high, cross-sum) =
    FIND-MAXIMUM-CROSSING-SUBARRAY(A,low,mid,high) 
  if left-sum >= right-sum and left-sum >= cross-sum
    return (left-low, left-high, left-sum)
  else if right-sum >= left-sum and right-sum >= cross-sum
    return (right-low, right-high, right-sum)
  else return (cross-low, cross-high, cross-sum)
#+END_SRC

**** Time analysis
Assume that we can find max-crossing-subarray in time $\Theta(n)$ :

- *Divide* takes constant time, i.e, $D(n) = \Theta(1)$
- *Combine* time dominated by find-max-crossing-subarray, $C(n) = \Theta(n)$
- *Conquer* recusively solve two subproblems, each of size $n/2$, $T(n/2)$.

\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 2T(n/2)+\Theta(n) \text{  otherwise}
\end{align*}
so $T(n)=\Theta)n \log n)$
**** Crossing subarray
- Any subarray crossing the midpoint A[mid] is made of two subarrays $A[i...mid]$ and $A[mid + 1,...,j]$ where $low \le i \le mid$ and $mid < j \le high$
- Find maximum subarrays of the form $A[i...mid]$ and $A[mid + 1...j]$ and then combine them.
*FIND-MAXIMUM-CROSSING-SUBARRAY* $(A,low,mid,high)$
#+BEGIN_SRC java
//Find a maximum subarray of the form A[i...mid]
left-sum = - infinity
sum = 0
for i = mid downto low
  sum = sum + A[i]
  if sum > left-sum
    left-sum = sum
    max-left = i
//Find a maximum subarray of the form A[mid+1...j]
right-sum = - infinity
sum = 0
for j = mid + 1 to high
  sum = sum + A[j]
  if sum > right-sum
    right-sum = sum
    max-right = j
// Return the indices and the sum of the two subarrays.
return (max-left, max-right, left-sum + right-sum)
#+END_SRC

** Matrix multiplication
Multiply two $n\times n$ matrices. $AB=C$

$c_{ij} = \sum\limits_{k=1}^n a_{ik}b_{kj}$

*** Naive algorithm
*SQUARE-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
for i = 1 to n
  for j = 1 to n
    cij = 0
    for k = 1 to n
      cij = cij + aik*bkj
return C
#+END_SRC
Running time : $\Theta(n^3)$

Space : $\Theta(n^2)$

*** Divide-and-Conquer
- *Divide* each of $A,B,C$ into four $n/2 \times n/2$ matrices: so that :
\begin{align*}
  \begin{pmatrix}
  C_{11} & C_{12}\\
  C_{21} & C_{22}
  \end{pmatrix} = \begin{pmatrix}
  A_{11} & A_{12}\\
  A_{21} & A_{22}
  \end{pmatrix} \cdot \begin{pmatrix}
  B_{11} & B_{12}\\
  B_{21} & B_{22}
  \end{pmatrix}
\end{align*}
- *Conquer* : Since
\begin{align*}
C_{11} =  A_{11} \cdot B_{11} + A_{12} \cdot B_{21}\\
C_{12} =  A_{11} \cdot B_{12} + A_{12} \cdot B_{22}\\
C_{21} =  A_{21} \cdot B_{11} + A_{22} \cdot B_{21}\\
C_{22} =  A_{21} \cdot B_{12} + A_{22} \cdot B_{22}
\end{align*}
We recursively solve 8 matrix multiplications that each multiply two $n/2 \times
n/2$ matrices.
- *Combine* Make the additions to get $C$
**** Pseudocode
*REC-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
if n == 1
  c11 = a11 * b11
else partition A,B and C into n/2 x n/2 submatrices
  c11 = REC-MAT-MULT(A11,B11,n/2) + REC-MAT-MULT(A12,B21,n/2)
  c12 = REC-MAT-MULT(A11,B12,n/2) + REC-MAT-MULT(A12,B22,n/2)
  c21 = REC-MAT-MULT(A21,B11,n/2) + REC-MAT-MULT(A22,B21,n/2)
  c22 = REC-MAT-MULT(A21,B12,n/2) + REC-MAT-MULT(A22,B22,n/2)
return C
#+END_SRC
**** Analysis
Let $T(n)$ be the times to multiply two $n \times n$ matrices.
- *Base case*: $n=1$. Perform one scalar multiplication: $\Theta(1)$
- *Recursive case*: $n>1$
  - Dividing takes $\Theta(1)$ time if careful and $\Theta(n^2)$ if simply copying
  - Conquering makes 8 recursive calls, each multiplying $n/2 \times n/2$
    matrices : $8T(n/2)$
  - Combining takes time $\Theta(n^2)$ time to add $n/2 \times n/2$ matrices.
Reccurence is :
\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 8T(n/2)+\Theta(n^2) \text{  if $n>1$}
\end{align*}
So $T(n) = \Theta(n^3)$ (from master method)
*** Strassen's algorithm 
- *Divide* eachf $A,B,C$ into four $n/2 \times n/2$ matrices: so that :

$\begin{pmatrix}
C_{11} & C_{12}\\
C_{21} & C_{22}
\end{pmatrix} = \begin{pmatrix}
A_{11} & A_{12}\\
A_{21} & A_{22}
\end{pmatrix} \cdot \begin{pmatrix}
B_{11} & B_{12}\\
B_{21} & B_{22}
\end{pmatrix}$  

- *Conquer* : calculate recusively 7 matrix multiplications, each of two $n/2
  \times n/2$ matrices :
\begin{align*}
&M_1 := (A_{11}+A_{22})(B_{11}+B_{22}) &\ M_5 := (A_{11}+A_{12})B_{22}\\
&M_2 := (A_{21}+A_{22})B_{11} &\ M_6 := (A_{21}-A_{11})(B_{11}+B_{12})\\
&M_3 := A_{11}(B_{12}-B_{22}) &\ M_7 := (A_{12}-A_{22})(B_{21}+B_{22})\\
&M_4 := A_{11}(B_{21}-B_{11})
\end{align*}

- *Combine* Let
\begin{align*}
&C_{11} = M_1 + M_4 - M_5 + M_7 &\ C_{12} = M_3 + M_5 \\
&C_{21} = M_2 + M_4 &\ C_{22} = M_1 - M_2 + M_3 + M_6
\end{align*}
**** Time analysis
- Base case $n = 1$ $\Rightarrow$ it takes time $\Theta(1)$
- Recursive case: $n > 1$
  - Dividing takes time $\Theta(n^2)$
  - Conquering makes 7 recursive calls, each multiplying $n/2 \times n/2$
    matrices $\Rightarrow 7 T(n/2)$
  - Combining takes time $\Theta(n^2)$ time to add $n/2 \times n/2$ matrices.
Reccurence is
\begin{align*}
T(n) = \Theta(1) \text{ if $n=1$}\\
T(n) = 7T(n/2) + \Theta(n^2) \text{ if $n > 1$}
\end{align*}
Master method $\Rightarrow T(n) = \Theta(n^{\log_2 7})$ 

Best we could hope for is $\Theta(n^2)$ because we need to at least look at each
entry of the matrices.
* Chapter 5
** Heap 
Heap $A$ ( _not_ garbage-collected storage) is a nearly complete binary trees
- height $\lceil \log_2(n)\rceil$
- Atmost one node has one child

(Max)-Heap property: key of $i$ 's children is _smaller or equal_ to $i$ 's key

(Min)-Heap property: key of $i$ 's children is _greater or equal_ to $i$ 's key

*** Root :
- Max-Heap $\Rightarrow$ maximum element is the root
- Min-heap $\Rightarrow$ minimum elememnt is the root
*** Height :
Height of node = # of edges on a longest simple path from the node down to a leaf

Height of heap = height of root = $\Theta (\log n)$
*** Storage :
Use that tree is almost complete to store it in array
[[file:images/al_ch5_01.png]]
*** Operations
**** MAX-HEAPIFY
MAX-HEAPIFY : given an $i$ such that the subtrees of $i$ are heaps, it ensures
that the subtree rooted at $i$ is a heap satisfying the heap property.

*Algorithm* :
- Compare $A[i],A[Left(i)],A[Right(i)]$
- If necessary, swap $A[i]$ with the largest of the two children to preserve
  heap property
- Continue this process of comparing and swapping down the heap, until subtree
  rooted at $i$ is max-heap
*MAX-HEAPIFY* $(A,i,n)$
#+BEGIN_SRC java
l = Left(i)
r = Right(i)
if l <= n and A[l] > A[i]
  largest = l
else largest = i
if r <= and A[r] > A[largest]
  largest = r
if largest != i
  exchange A[i] with A[largest]
  MAX-HEAPIFY(A,largest,n)
#+END_SRC

*Running time* : $\Theta($ height of $i)=O(\log n)$

*Space* : $\Theta(n)$
**** Building a heaps
Given unordered array $A$ of length $n$, BUILD-MAX-HEAP(A,n)

*BUILD-MAX-HEAP* $(A,n)$
#+BEGIN_SRC java
for i = floor(n/2) downto 1
  MAX-HEAPIFY(A,i,n)
#+END_SRC

*Analysis*

Worst case :
- Simple bound : $O(n)$ calls to MAX-HEAPIFY, each of wich takes $O(\log n)$
  time $\Rightarrow O(n \log n)$ in total
- Tighter anaysis : Time to run MAX-HEAPIFY is linear in the height of the node
  it's run on. Hence the time bounded by :
  
$\sum\limits_{h=0}^{\log n}
  \lbrace \text{# nodes of height } h \rbrace O(h) = O(n \sum\limits_{h=0}^{\log
  n} \frac{h}{2^h})$

which is $O(n)$ since $\sum\limits_{h=0}^{\infty} \frac{h}{2^h} =
\frac{1/2}{(1-1/2)^2} = 2$

*Correctness*

_Loop invariant_ : At start of every iteration of *for* loop, each node
$i+1,i+2,...,n$ is root of max-heap

_Maintenance_ :
- Children of node $i$ are indexed higher than $i$, so by the loop invariant,
  they are both roots of max-heaps
- Therefore, MAX-HEAPIFY makes node $i$ a max-heap root (so $i,i+1,...,n$ are
  all roots of max-heaps)
- Hence, the invariant stays true when decredementing $i$ at the beggining of
  the next iteration

_Termination_:
- When $i = 0$, the loop terminates
- By the loop invariant, each node, notably node 1, is the root of a max-heap

** Heapsort
- Builds a max-heap from the array
- Starting with the root (the maximum element), the algorithm places the maximum
  element into the correct place in the array by swapping it with the element in
  the last position in the array
- "Discard" this last node (knowing that it is in its correct place) by
  decreasing the heap size, and calling MAX-HEAPIFY on the new (possibly
  incorrect-placed) root
- Repeat this "discarding" process until only one node (the smallest element)
  remains, and therefore is in the correct place in the array
*** Pseudocode :
*HEAPSORT* $(A,n)$
#+BEGIN_SRC java
BUILD-MAX-HEAP(A,n)
for i = n downto 2
  exchange A[1] with A[i]
  MAX-HEAPIFY(A,1,i-1)
#+END_SRC
*** Analysis
- BUILD-MAX-HEAP : $O(n)$
- *for* loop: $n-1$ times
- exchange elements: $O(1)$
- MAX-HEAPIFY: $O(\lg n)$

*Total time* : $O(n \lg n)$

** Priorty queue
- Maintains a dynamic set $S$ of elements
- Each set element has a *key* - an associated value that regulates its importance
- Operations :
  - INSERT(S,x): inserts element $x$ into $S$
  - MAXIMUM(S) : returns element of $S$ with largest key
  - EXTRACT-MAX(S): removes and returns element of $S$ with largest key
  - INCREASED-KEY(S,x,K): increases value of element $x$ 's key to $k$; assume $k
    \le x$ 's current key value.
*** Operations 
**** HEAP-MAXIMUM 
Symply return the root in time $\Theta(1)$

*HEAP-MAXIMUM* $(A)$
#+BEGIN_SRC java
Return A[1]
#+END_SRC
**** HEAP-EXTRACT-MAX(A,n)
1. Make sure heap is not empty
2. Make a copy of the maximum element (the root)
3. Make the last node in the tree the new root
4. Re-heapify the heap, with one fewer node

_Analysis_ : Constant-time assignments plus time for MAX-HEAPIFY

So $O(\lg n)$

*HEAP-EXTRACT-MAX* $(A,n)$
#+BEGIN_SRC java
if n < 1
  error "heap underflow"
max = A[1]
A[1] = A[n]
n = n - 1
MAX-HEAPIFY(A,1,n)
return max
#+END_SRC
**** HEAP-INCREASE-KEY(A,i,key)
Given a heap $A$, index $i$, and a new value $key$
1. Make sure $key >= A[i]$
2. Update $A[i]$ 's value to $key$
3. Traverse the tree upward comparing new key to the parent and swapping keys if
   necessary, until the new key is smaller than the parent's keys

_Analysis_ : Upward path frome node $i$ has length $O(\lg n)$ in an $n$ -element
heap.

So $O(\lg n)$

*HEAP-INCREASE-KEY* $(A,i,key)$
#+BEGIN_SRC java
if key < A[i]
  error "new key is smaller than current key"
A[i] = key
while i > 1 and A[Parent(i)] < A[i]
  exchange A[i] with A[Parent(i)]
  i = Parent(i)
#+END_SRC

** Stacks
Last in, first out.
- Insert operation called PUSH($S,x$)
- Delete operation called POP($S$)
*** Implementation
Implementation using arrays: $S$ consists of elements $S[1,...,S.top]$
- $S[1]$ element at the bottom
- $S[S.top]$ element at the top
*** Operations
*STACK-EMPTY* $(S)$
#+BEGIN_SRC java
if S.top = 0
  return true
else return false
#+END_SRC

*PUSH* $(S,x)$
#+BEGIN_SRC java
S.top = S.top + 1
S[S.top] = x
#+END_SRC

*POP* $(S)$
#+BEGIN_SRC java
if STACK-EMPTY(S)
  error "underflow"
else
  S.top = S.top -1
  return S[S.top+1]
#+END_SRC

All these operations are $O(1)$

** Queue
First-in, first-outputs
- Insert operation called ENQUEUE($Q,x)$
- Delete operation called DEQUEUE$(Q)$
*** Implementation
Implementation using arrays: $Q$ consists of elements $S[Q.head,...,Q.tail-1]$
- $Q.head$ points at the first elements
- $Q.tail$ points at the next location where a newly arrived alement wille be placed
*** Operations
*ENQUEUE* $(Q,x)$
#+BEGIN_SRC java
Q[Q.tail] = x
if Q.tail == Q.length
  Q.tail = 1
else Q.tail = Q.tail + 1
#+END_SRC

*DEQUEUE* $(Q,x)$
#+BEGIN_SRC java
x = Q[Q.head]
if Q.head == Q.length
  Q.head = 1
else Q.head = Q.head + 1
return x
#+END_SRC

All these operations are $O(1)$

** Stacks and queues
*** Pros
- Very efficent
- Natural operations
*** Cons
- Limited support: for example, no search
- Implementations using arrays have a fixed capacity
** Linked list
Objects are arranged in a linear order : not indexes in array, but pointer to
next object in each object.

A list can be :
- Single linked or double linked
- Sorted or unsorted
- etc.

Example : An element in a double linked list :
- x.key (key value)
- x.prev (point to previous element)
- x.next (points to next element)
*** Searching in a linked list
Given $k$ return pointer to first element with key $k$

*LIST-SEARCH* $(L,k)$
#+BEGIN_SRC java
x = L.head
while x != nil and x.key != k
  x = x.next
return x
#+END_SRC

Running time : $O(n)$

If no element with key $k$ ? Returns /nil/

*** Inserting into a linked list
Insert a new element $x$

*LIST-INSERT* $(L,x)$
#+BEGIN_SRC java
x.next = L.head
if L.head != nil
  L.head.prev = x
L.head = x
x.prev = NIL
#+END_SRC

Running time : $O(1)$

*** Deleting from a linked list
Given a pointer to an element $x$ removes it from $L$

*LIST-DELETE* $(L,x)$
#+BEGIN_SRC java
if x.prev != nil
  x.prev.next = x.next
else L.head = x.next
if x.next != nil
  x.next.prev = x.prev
#+END_SRC

Running time : $O(1)$
*** TODO Using Sentinels
*Add image for clarity*

We add a "sentinel" called L.nil which
- Points to itself for prev and next if list is empty
- Next points to first element of list (head), prev points to last element of the list
  and last element of the list points to it

Then we can symplify delete and insert by :

*LIST-DELETE* $(L,x)$
#+BEGIN_SRC java
x.prev.next = x.next
x.next.prev = x.prev
#+END_SRC

*LIST-INSERT* $(L,x)$
#+BEGIN_SRC java
x.next = L.nil.next
L.nil.next.prev = x
L.nil.next = x
x.prev = L.nil
#+END_SRC

*** Summary
- Dynamic data structure without predefined capacity
- Insertion : $O(1)$
- Deletion : $O(1)$ (if double linked)
- Search : $O(n)$
* Chapter 6
** Binary Search trees
Encodes a strategy whatever number we look for.

Key property:
- if $y$ is in the left subtree of $x$ then $y.key < x.key$ 
- if $y$ is in the right subtree of $x$ then $y.key \ge x.key$

Height ($h$) is the number of edges in longest path from root to leaf.

Basic operations take time proportional to height: $O(h)$

Each element $x$ has :
- $x.left$ : pointer to left child
- $x.right : pointer to right child
- $x.p$ : pointer to parent
- $x.key$ : key
*** Searching
Running time : $O(h)$
*** Min and Max
*** Sucessor and Predecessor
*** Printing
**** Inorder
**** Preorder and Postorder
*** Modifying
**** Inserting
**** Deleting
