# -*- mode: org; -*-
#+TITLE: Algorithms
#+SETUPFILE: htmlnotes.setup

* Chapter 1
** Definition of algorithm
- Relation between an input and an ouptut
- Tool to solve a computational problem
** Instance of a problem
Input of the problem. 
Examples :
- Sorting problem, instance : <32,54,65,23,54>
- Sum of all numbers to /n/ : 4, /n/ \neq instance
* Chapter 2
** Insertion sort algorithm
#+BEGIN_SRC java
for j = 2 to A.length
  key = A[j]
  // Insert A[j] into the sorted sequance A[1..j-1]
  i = j -1
  while i > 0 and A[i] > key
    A[i+1] = A[i]
    i = i -1
  A[i+1] = key
#+END_SRC
** Loop Invariant -> prove corectness of algorithm
- Loop invariant : "a statement that is satisfied during the loop"
- Need to verify :
  - *Initialization* : True at the beginning of the 1st iteration of the loop
  - *Maintenance* :  If it is true before an iteration of of the loop, it remains
    true before the next iteration
  - *Termination* : When the loop terminates, the invariant gives us a useful
    property that helps show that the algorithm is correct
***  Example 1 : Sum up to /n/
The algorithm is the following : \\
*CalculateSum* (/n/):
#+BEGIN_SRC java
ans = 0
for i = 1,2,..,n
 ans = ans + i
return ans
#+END_SRC
- *Loop invariant* : At the start of each iteration $ans = (i-1) * i/2$
- *Initialization* : At the begining of the first iteration, $i =1$ and $ans =
  0 = i(i -1)/2)$ so the invariant is satisfied
- *Maintenance* : Suppose invariant true at the beginning of iteration when
  $i=k$, i.e, $ans = (k-1)k/2$. Then the iteration updates $ans$ by adding $k$,
  i.e., $ans = (k-1)k/2 +k = k(k+1)/2$ so incrementing $i$ then for next
  iteration preserves the loop invariant.
- *Termination* : When the loop terminates $i=n+1$. Hence the algorithm returns
  $ans = n(n+1)/2$
*** Example 2 : Insertion sort
#+BEGIN_HTML
<input checked type=radio name=slider id=slide1 />
		<input type=radio name=slider id=slide2 />
		<input type=radio name=slider id=slide3 />
		<input type=radio name=slider id=slide4 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/insertionsortproof-0.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-1.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-2.jpg />
					</article>
					
					<article>
						<img src=pdf/insertionsortproof-3.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide1></label>
			<label for=slide2></label>
			<label for=slide3></label>
			<label for=slide4></label>
			
		</div> <!-- #active -->
#+END_HTML
** Divide and conquer approach
*** Principle : 
- *Divide* the problem into a number of subproblems that are smaller instances
  of the same problem
- *Conquer* the subproblems by solving them recursively. If the subproblem sizes
  are small enough, however, just solve the subproblems in a straightforwad
  manner
- *Combine* the solutions to the subproblems into the solution for the original problem
*** Application on merge and sort : 
To sort $A[p...r]$:
- *Divide* by splitting into two subarrays $A[p...q]$ and $A[q+1,...r]$, whre
  $q$ is the halfway point of $A[p..r]$
- *Conquer* by recursively sorting the two subarrays $A[p..q]$ and $A[q+1,...r]$
- *Combine* by mergie the two sorted subarrays $A[p...q]$ and $A[q+1,...r]$ to
  produce a single sorted subarray $A[p...r]$

*Merge-Sort* $(A,p,r)$
#+BEGIN_SRC java
if p < r  //check for base case
  q = floor((p+r)/2)  // divide
  Merge-Sort(A,p,q)  // conquer
  Merge-Sort(A,q+1,r)  //conquer
  Merge(A,p,q,r) // combine
#+END_SRC
*Merge* ($A$,$p$,$q$,$r$)
#+BEGIN_SRC java
n_1 = q - p + 1
n_2 = r - q
let L[1..n_1+1] and R[1..n_2+1] be new arrays
for i = 1 to n_1
  L[i] = A[p+i-1]
for j = 1 to n_2
  R[j] = A[q+j]
L[n_1+1]= infinity
R[n_2 +1]= infinity
i = 1
j = 1
for k = p to r
  if L[i] <= R[j]
    A[k] = L[i]
    i = i +1
  else
    A[k] = R[j]
    j = j +1
#+END_SRC
**** Corectness
- What does merge(A,p,q,r) do ?
  + It takes array $A$ and indexes $p \le q < r$ s.t. $A[p...q]$ and $A[q+1...r]$ are sorted.
  + Then it outputs $A[p...r]$ contains the same elements in sorted order
- Proof by induction on $n = r - p$ :
  - Base case : $n = 0$, in this case $r = p$ so $A[p...r]$ (single element) is trivially sorted.
  - Inductive case : assume statement true $\forall\ n \in \{0,1,...,n-1\}$ and
    prove true for $n=k$
    - By induction hypothesis ( $q - p < n$ ) Merge-Sort(A,p,q) and
      Merge-Sort(A,q+1,r) sucessfully sort the two subarrays.
    - Therefore a correct merge procedure will sucessfully sort $A[p...q]$ as required.
**** Time analysis
[[file:pdf/mergesorttime.jpg]]
*** Analysis of divide-and-conquer algorithms
Use a *recurrence* equation to describe the running time :
- Let $T(n)$ = "running time on a problem of size $n$"
- If $n$ is small enough say $n\le c$ for some constant $c$ then $T(n) = \Theta(1)$ (by brute force)
- Otherwise, suppose we divide into a sub problems each size $n/b$ (conquer step)
- Let $D(n)$ be the time to divide and let $C(n)$ the time to combine solutions
- We get the reccurence : $T(n) = \Theta(1)$ if $n\le c$, else $T(n) = aT(n/b)+D(n)+C(n)$
* Chapter 3 
Consider the following recurrence :

$T(n) = c$ if $n = 1$ \\
$T(n) = 2T(n/2) + c \cdot n$ otherwise

Note that this reccurence upper bounds and lower bounds the reccurence for
Merge-Sort by selecting $c$ sufficiently large and small, respectively.

Three solving techiques :
** The substitution method
1. Guess the form of the solution
2. Use mathematical induction to find the constant (substition method in the book)

\begin{align*} T(n) &= 2T(n/2) + c \cdot n \\
&= 2(2T (n/4) + c \cdot n/2) + c \cdot n = 4T(n/4) + 2 \cdot c \cdot n\\
&= 4(2T (n/8) + c \cdot n/4) + 2 \cdot c \cdot n =  8T(n/8) + 3 \cdot c \cdot n\\
&\vdots \\
&= 2^k T(n/2^k) + k \cdot c \cdot n
\end{align*}

A qualified guess is that $T(n) = \Theta (n \log n)$.
- and show that the solution works
  - First upper bound :
    We want to prove there exists $a > 0$ s.t $T(n) \le a \cdot n \cdot \log n$
    $\forall n \ge 2$
    - *Base case* : For any constant $n \in \{2,3,4\}$, $T(n)$ has a constant
      value, selecting a larger than this value will satisfy the base cases when
      $n \in \{2,3,4\}$.
    - *Inductive step* : /Assume statement true $\forall n \{2,3,...,k-1\}$ and
      prove the statement for $n=k$/
      \begin{align*}
      T(n) &= 2T(n/2) + c \cdot n \\
      &\le 2 \cdot \frac{an}{2} \log(n/2) + c \cdot n = a \cdot n \log(n/2) + c \cdot n \\
      &= a \cdot n \log n -a \cdot n + c \cdot n \\
      &\le a \cdot n \log n \text{ (if we select $a \ge c$)}
      \end{align*}
      We can thus select $a$ to be a positive constant so taht both the base
      cases and the inductive step holds. Hence, $T(n) = O(n\log n)$.
  - Second lower bound :
    - We want to prove there exists a constant $b > 0$ s.t $T(n) \ge b \cdot n
      \cdot \log n$ $\forall n \ge 0$
    - *Base case* : For $n=1,T(n)=c$ and $b \cdot n \log n =0$ so the base case
      is satisfied for any b.
    - *Inductive step* : /Assume statement true $\forall n \in \{0,1,...,k-1\}$
      and prove the statement for $n =k$/
      \begin{align*}
      T(n) &= 2T (n/2) + c \cdot n\\
      &\ge 2 \cdot \frac{b\cdot n}{2} \log(n/2) + c \cdot n = b \cdot n \log(n/2) + c \cdot n\\
      &= b \cdot n \log n - b \cdot n + c \cdot n \\
      & \ge b \cdot n \log n \text{ (if we select $b \le c$)}
      \end{align*}
We can thus select $b$ to be positive constant so that both the base cases and
the inductive step holds. Hence, $T(n) = \Omega(n\log n)$.

- Other example
Prove that $T(n) = O(n)$
First $\exists c$ s.t.
\begin{align*}
 T(n) \le F(n)\\
\text{and}\\
F(n) = c \text{ if $n=0, n=1$, else } F(\frac{n}{4} +1 ) + F(\frac{3n}{4} -1) + c
\end{align*} 
We shall prove that $F(n) = O(n)$
claim : there exists constants $b',b > 0$ and $n_0 \ge 0$ s.t. $ \forall n \ge n_0$
$F(n) \le b \cdot n - b'$

Proof : Inductive step : Assume $F(n) = b \cdot n$ $\forall n \in
\{n_0,...k-1\}$ prove true for $n=k$.
\begin{align*}
 F(n) &= F(\frac{n}{4} +1) + F(\frac{3n}{4} -1) + C \\
&= b(\frac{n}{4} + 1) b' + b (\frac{3n}{4} -1) b' + C\\
&= b \cdot n + c - 2b'\\
&\le b \cdot n - b' \text{ if $ b' \ge C$}
\end{align*}

- Again other example :
$T(n) = \Theta(1)$ if $n=1$ \\
$8 T(n/2) + c \cdot n^2$ if $n > 1$ \\
Prove that $T(n)$ is $O(n^3)$.

_Claim_ : $\exists d > 0$ and $n_0 > 0$ s.t. $T(n) \le d n^3 \forall n \ge n_0$

_Base case_: for $n=1,...,k$
$T(1), T(2) ... T(k)$ I can always select $d \ge max \left\lbrace T(1), T(2),...
T(k) \right\rbrace$

__Inductive step__ :
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le 8 \cdot d \cdot (n/2)^3 + c \cdot n^2 \\
= d \cdot n^3 + c \cdot n^2
\end{align*}
How to remove the low order terms ? We add to our claim a $-d' \cdot n^2$ s.t.
$T(n) \le d n^3 - d' \cdot n^2 \forall n \ge n_0$.
Now
\begin{align*}
T(n) = 8 \cdot T(n/2) + c \cdot n^2 \\
\le d \cdot n^3 - 2 \cdot d' \cdot n^2 + c \cdot n^2\\
\le d \cdot n^3 - d' \cdot n^2 \text{if $d' \ge c$}
\end{align*}
** The recursion tree method 
#+BEGIN_HTML
<input checked type=radio name=slider id=slide01 />
		<input type=radio name=slider id=slide02 />
		<input type=radio name=slider id=slide03 />
	
	
		<!-- The Slider -->
		
		<div id=slides>
		
			<div id=overflow>
			
				<div class=inner>
				
					<article>
						<img src=pdf/recursiontree01.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree02.jpg />
					</article>
					
					<article>
						<img src=pdf/recursiontree03.jpg />
					</article>
					
				</div> <!-- .inner -->
				
			</div> <!-- #overflow -->
		
		</div> <!-- #slides -->
	
	
		<!-- Controls and Active Slide Display -->
	
		<div id=controls>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
		
		</div> <!-- #controls -->
		
		<div id=active>

			<label for=slide01></label>
			<label for=slide02></label>
			<label for=slide03></label>
			
		</div> <!-- #active -->
#+END_HTML
- Examples to do with this method :
  - $T(n) = 2 T(n/2) + c \cdot n^2$, cost of base root $cn^2$, but cost of last
    root : $cn$
** The master method
Used to black-box solve reccurences of form $T(n) = a \cdot T(n/b) + f(n)$
(doesn't work with $T(n) = T(n/3) + T(2n/3) + cn$ for example).

_Theorem (Master theorem)_

Let $a \ge 1$ and $b > 1$ be constants, let $T(n) be defined on the
  nonnegative integers by the reccurence :
\begin{align*}
T(n) = a \cdot T(n/b) + f(n)
\end{align*}
Then, $T(n)$ has the following asymptotic bounds :
- If $f(n) = O(n^{\log_b a- \epsilon})$ for some constant $\epsilon > 0$, then
  $T(n) = \Theta(n^{\log_b a})$
- If $f(n) = \Theta(n^{\log_b a})$, then $T(n) = \Theta(n^{\log_b a} \log n)$
- If $f(n) = \Omega(n^{\log_b a+\epsilon})$ for some constant $\epsilon > 0$ and
  if $a \cdot f(n/b) \le c \cdot f(n)$ for some constant $c < 1$ and all
  sufficiently large $n$, then $T(n) = \Theta(f(n))$
* Chapter 4 
** Maximum-subarray problem

"If we let $A[i]$ = (price after day $i$) - (price after day $i-1$) then if the
maximum subarray is $A[i..j]$ then we should have bought just before day $i$ and
sold just after day $j$.

*Input* : An array $A[1...n]$ of numbers

*Output* Indices $i$ and $j$ such that $A[i...j]$ has the greatest sum of any
 nonempty, contiguous subarray of $A$, along with the sum of the values in $A[i...j]$.
*** Brute force
Simply check all possible subarrays.

$\binom{n}{2} =  \Theta (n^2)$ many.

*Maximum-subarray-slow* $(A[1...n])$
#+BEGIN_SRC java
B.val = - infinity, B.i = 1, B.j = n
for i = 1 to n
  tmp = 0
  for j = i to n
    tmp = tmp + A[j]
    if tmp > B.val
       B.val = tmp
       B.i = i
       B.j = j
return (B.i,B.j.B.val)
#+END_SRC
Running time is $\Theta(n^2)$ and we use $\Theta(n)$ space.
*** Divide and conquer
- *Divide* the subarray into two subarrays of as equal size as possible. Find the midpoint mid of the subarrays, and consider the subarrays $A$[low.. mid]
  and $A$[mid+1...high].
- *Conquer* by finding maximum subarrays of $A$[low . . . mid] and $A$[mid + 1 . . . high].
- *Combine* by finding a maximum subarray that crosses the midpoint,and using the best solution out of the three

 *FIND-MAXIMUM-SUBARRAY* $(A,low,high)$
#+BEGIN_SRC java
if high == low
  return (low, high, A[low]) // base case: only one element
else mid = ceil((low + high)/2)
  (left-low, left-high, left-sum) =
    FIND-MAXIMUM-SUBARRAY(A, low, mid)
  (right-low, right-high, right-sum) =
    FIND-MAXIMUM-SUBARRAY(A, mid + 1, high)
  (cross-low, cross-high, cross-sum) =
    FIND-MAXIMUM-CROSSING-SUBARRAY(A,low,mid,high) 
  if left-sum >= right-sum and left-sum >= cross-sum
    return (left-low, left-high, left-sum)
  else if right-sum >= left-sum and right-sum >= cross-sum
    return (right-low, right-high, right-sum)
  else return (cross-low, cross-high, cross-sum)
#+END_SRC

**** Time analysis
Assume that we can find max-crossing-subarray in time $\Theta(n)$ :

- *Divide* takes constant time, i.e, $D(n) = \Theta(1)$
- *Combine* time dominated by find-max-crossing-subarray, $C(n) = \Theta(n)$
- *Conquer* recusively solve two subproblems, each of size $n/2$, $T(n/2)$.

\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 2T(n/2)+\Theta(n) \text{  otherwise}
\end{align*}
so $T(n)=\Theta)n \log n)$
**** Crossing subarray
- Any subarray crossing the midpoint A[mid] is made of two subarrays $A[i...mid]$ and $A[mid + 1,...,j]$ where $low \le i \le mid$ and $mid < j \le high$
- Find maximum subarrays of the form $A[i...mid]$ and $A[mid + 1...j]$ and then combine them.
*FIND-MAXIMUM-CROSSING-SUBARRAY* $(A,low,mid,high)$
#+BEGIN_SRC java
//Find a maximum subarray of the form A[i...mid]
left-sum = - infinity
sum = 0
for i = mid downto low
  sum = sum + A[i]
  if sum > left-sum
    left-sum = sum
    max-left = i
//Find a maximum subarray of the form A[mid+1...j]
right-sum = - infinity
sum = 0
for j = mid + 1 to high
  sum = sum + A[j]
  if sum > right-sum
    right-sum = sum
    max-right = j
// Return the indices and the sum of the two subarrays.
return (max-left, max-right, left-sum + right-sum)
#+END_SRC

** Matrix multiplication
Multiply two $n\times n$ matrices. $AB=C$

$c_{ij} = \sum\limits_{k=1}^n a_{ik}b_{kj}$

*** Naive algorithm
*SQUARE-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
for i = 1 to n
  for j = 1 to n
    cij = 0
    for k = 1 to n
      cij = cij + aik*bkj
return C
#+END_SRC
Running time : $\Theta(n^3)$

Space : $\Theta(n^2)$

*** Divide-and-Conquer
- *Divide* each of $A,B,C$ into four $n/2 \times n/2$ matrices: so that :
\begin{align*}
  \begin{pmatrix}
  C_{11} & C_{12}\\
  C_{21} & C_{22}
  \end{pmatrix} = \begin{pmatrix}
  A_{11} & A_{12}\\
  A_{21} & A_{22}
  \end{pmatrix} \cdot \begin{pmatrix}
  B_{11} & B_{12}\\
  B_{21} & B_{22}
  \end{pmatrix}
\end{align*}
- *Conquer* : Since
\begin{align*}
C_{11} =  A_{11} \cdot B_{11} + A_{12} \cdot B_{21}\\
C_{12} =  A_{11} \cdot B_{12} + A_{12} \cdot B_{22}\\
C_{21} =  A_{21} \cdot B_{11} + A_{22} \cdot B_{21}\\
C_{22} =  A_{21} \cdot B_{12} + A_{22} \cdot B_{22}
\end{align*}
We recursively solve 8 matrix multiplications that each multiply two $n/2 \times
n/2$ matrices.
- *Combine* Make the additions to get $C$
**** Pseudocode
*REC-MAT-MULT* $(A,B,n)$
#+BEGIN_SRC java
let C be a new n x n matrix
if n == 1
  c11 = a11 * b11
else partition A,B and C into n/2 x n/2 submatrices
  c11 = REC-MAT-MULT(A11,B11,n/2) + REC-MAT-MULT(A12,B21,n/2)
  c12 = REC-MAT-MULT(A11,B12,n/2) + REC-MAT-MULT(A12,B22,n/2)
  c21 = REC-MAT-MULT(A21,B11,n/2) + REC-MAT-MULT(A22,B21,n/2)
  c22 = REC-MAT-MULT(A21,B12,n/2) + REC-MAT-MULT(A22,B22,n/2)
return C
#+END_SRC
**** Analysis
Let $T(n)$ be the times to multiply two $n \times n$ matrices.
- *Base case*: $n=1$. Perform one scalar multiplication: $\Theta(1)$
- *Recursive case*: $n>1$
  - Dividing takes $\Theta(1)$ time if careful and $\Theta(n^2)$ if simply copying
  - Conquering makes 8 recursive calls, each multiplying $n/2 \times n/2$
    matrices : $8T(n/2)$
  - Combining takes time $\Theta(n^2)$ time to add $n/2 \times n/2$ matrices.
Reccurence is :
\begin{align*}
T(n) &= \Theta(1) \text{  if $n=1$}\\
T(n) &= 8T(n/2)+\Theta(n^2) \text{  if $n>1$}
\end{align*}
So $T(n) = \Theta(n^3)$ (from master method)
*** Strassen's algorithm 
