<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-03-20 Mon 16:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>COM-308: Internet Analytics</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/home/raph/school/theme.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">COM-308: Internet Analytics</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga40dd5d">1. Grading</a></li>
<li><a href="#orgd3f1888">2. Lecture 1</a></li>
<li><a href="#orgbbe0a6c">3. Lecture 2</a>
<ul>
<li><a href="#org351c1f4">3.1. What are models for</a></li>
<li><a href="#org17fefa9">3.2. Social networks</a></li>
<li><a href="#orgf4cb89b">3.3. Information networks</a></li>
<li><a href="#orgd3e697f">3.4. Common features</a></li>
<li><a href="#org1ac3bda">3.5. Giant component</a></li>
<li><a href="#orgd4c4917">3.6. Clustering</a></li>
<li><a href="#org883fcdd">3.7. Strong and weak ties</a></li>
<li><a href="#org361e6d2">3.8. Short paths</a></li>
</ul>
</li>
<li><a href="#org19c334e">4. Lecture 3</a>
<ul>
<li><a href="#orgf3b6055">4.1. Herding and "watching thy neighbor"</a></li>
<li><a href="#orgb06cc47">4.2. Watching thy Neighbor</a></li>
<li><a href="#orgbb69a61">4.3. Herding how it can go wrong? Urn model</a></li>
<li><a href="#orga078e9a">4.4. Herding in networks</a></li>
<li><a href="#org7ed2929">4.5. Preferential attachment in growing nets</a></li>
<li><a href="#org8530906">4.6. Observing</a></li>
</ul>
</li>
<li><a href="#org3c5c58c">5. Lecture 4</a>
<ul>
<li><a href="#org407c339">5.1. Node statistics in large networks</a></li>
<li><a href="#orgf127015">5.2. Random walks on graphs</a></li>
<li><a href="#orga148ca1">5.3. Epidemics</a></li>
</ul>
</li>
<li><a href="#org172dcec">6. Lecture 5</a>
<ul>
<li><a href="#orgb26ceea">6.1. Structure of the web</a></li>
<li><a href="#org6b8235a">6.2. Search &rarr; Ranking</a></li>
</ul>
</li>
<li><a href="#org5faf26e">7. Tips and tricks</a>
<ul>
<li><a href="#orgaf054f1">7.1. Stationary distribution of RW</a></li>
</ul>
</li>
<li><a href="#orgdc28593">8. Lab1</a>
<ul>
<li><a href="#orgc639100">8.1. </a></li>
<li><a href="#org07b6129">8.2. Finish lab 1</a></li>
</ul>
</li>
<li><a href="#orgc107e10">9. Research</a>
<ul>
<li><a href="#org986f827">9.1. Principle of deffered decision for giant component discovery</a></li>
<li><a href="#org4c5d72d">9.2. Unbiased estimator for Random Walks</a></li>
<li><a href="#org9dc2a6d">9.3. Mixing time for RW</a></li>
<li><a href="#org83cc3f8">9.4. Conductance</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orga40dd5d" class="outline-2">
<h2 id="orga40dd5d"><span class="section-number-2">1</span> Grading</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Project 20%</li>
<li>Midterm 30%</li>
<li>Final   50%</li>
</ul>
</div>
</div>

<div id="outline-container-orgd3f1888" class="outline-2">
<h2 id="orgd3f1888"><span class="section-number-2">2</span> Lecture 1</h2>
</div>
<div id="outline-container-orgbbe0a6c" class="outline-2">
<h2 id="orgbbe0a6c"><span class="section-number-2">3</span> Lecture 2</h2>
<div class="outline-text-2" id="text-3">
<p>
For <b>networks</b> we use models, can't be exactly accurate (would be too complicated), but can
still be useful to understand.
</p>
</div>

<div id="outline-container-org351c1f4" class="outline-3">
<h3 id="org351c1f4"><span class="section-number-3">3.1</span> What are models for</h3>
<div class="outline-text-3" id="text-3-1">
</div><div id="outline-container-orgc9a7d1b" class="outline-4">
<h4 id="orgc9a7d1b"><span class="section-number-4">3.1.1</span> Explanation of phenomenon/behavior/pattern</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>Occam's razor = simplest possible model preferred</li>
<li>Uses most important features of the system</li>
<li>E.g. economics - income distribution</li>
</ul>
</div>
</div>

<div id="outline-container-org103fea4" class="outline-4">
<h4 id="org103fea4"><span class="section-number-4">3.1.2</span> Prediction and inference</h4>
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>Favor performance, not parsimony</li>
<li>Can be black-box</li>
<li>E.g. stock-price prediction for time series</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org17fefa9" class="outline-3">
<h3 id="org17fefa9"><span class="section-number-3">3.2</span> Social networks</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>Captures ties between individuals</li>
<li>Ties can be several things (friendship,business relationship,&#x2026;)</li>
<li>Tie formation:
<ul class="org-ul">
<li>Complex social process</li>
<li>Type and level of tie</li>
</ul></li>
<li>Most parsimonious model: "social network"
<ul class="org-ul">
<li>Graph</li>
<li>Undirected edges, without attributes or weights</li>
<li>Only structure</li>
</ul></li>
<li>Becoming really huge (more data)</li>
</ul>
</div>
</div>

<div id="outline-container-orgf4cb89b" class="outline-3">
<h3 id="orgf4cb89b"><span class="section-number-3">3.3</span> Information networks</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>Web</li>
<li>Assymetric links &rArr; directed graph</li>
</ul>
</div>

<div id="outline-container-org40679be" class="outline-4">
<h4 id="org40679be"><span class="section-number-4">3.3.1</span> Examples</h4>
<div class="outline-text-4" id="text-3-3-1">
<ul class="org-ul">
<li>Internet, traffic exchange arrangements (peering) w/o central coordination</li>
<li>Wikipedia, references to related concepts</li>
<li>Scientific literature, bibliography cites prior works</li>
<li>Dictionary: explaining one term in terms of other terms</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgd3e697f" class="outline-3">
<h3 id="orgd3e697f"><span class="section-number-3">3.4</span> Common features</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>No rules or centralized design</li>
<li>Nobody has global information (knowledge of "neighborhood" only)</li>
<li>Element of chance (random models)</li>
</ul>
</div>
</div>

<div id="outline-container-org1ac3bda" class="outline-3">
<h3 id="org1ac3bda"><span class="section-number-3">3.5</span> Giant component</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>Definition: Connected component that is:
<ul class="org-ul">
<li>Much larger than other connected components</li>
<li>Significant fraction of whole network (Same order of magnitude)</li>
</ul></li>
<li>The could be several very large components</li>
<li>Finding GC:
<ul class="org-ul">
<li>Finding components: start at vertex \(u\) and run BFS/DFS to exhaustion</li>
<li>Find dominant component</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org68b8301" class="outline-4">
<h4 id="org68b8301"><span class="section-number-4">3.5.1</span> Random graphs (\(G(n,p)\))</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
\(G(n,p)\) model:
</p>
<ul class="org-ul">
<li>\(n\) vertices</li>
<li>Every edge \((u,v)\) exists independently with prob \(p\)</li>
</ul>
<p>
Expected degree (# of neighbors) =  \(c\) = \((n-1)p \sim np\)
</p>
</div>
</div>

<div id="outline-container-orgafb669d" class="outline-4">
<h4 id="orgafb669d"><span class="section-number-4">3.5.2</span> Giant component in \(G(n,p)\)</h4>
<div class="outline-text-4" id="text-3-5-2">
<ul class="org-ul">
<li>Conditions for GC in \(G(n,p)\)</li>
<li>As \(n \to \infty\), what functions \(p_n\) ensure that \(P(G(n,p_n)\) has GC \(\to
  1\)</li>
<li>Discovery process:
<ul class="org-ul">
<li>Start at a node \(u\)</li>
<li>Find \(u\) 's neighbors recursively until done (BFS/DFS)</li>
</ul></li>
<li>Property of set of discovered nodes: only a function of edges and independent
of other edges</li>
</ul>
</div>

<div id="outline-container-orgb1f4fb5" class="outline-5">
<h5 id="orgb1f4fb5"><span class="section-number-5">3.5.2.1</span> Using Principle of Deferred Decision (PDD) for node discovery</h5>
<div class="outline-text-5" id="text-3-5-2-1">
<p>
Flip coin to choose next node (amongst connected and not already checked nodes)
</p>
<ul class="org-ul">
<li>Three types of node:
<ul class="org-ul">
<li>Potential, could be in the component</li>
<li>Active, is in the component, edges not checked</li>
<li>saturated, in the component and edges already checked</li>
</ul></li>
<li>\(k\) th step:
<ul class="org-ul">
<li>\(A_k\): # active</li>
<li>\(k\): # saturated (used)</li>
<li>\(n-k-A_k\): # potential</li>
</ul></li>
<li>Number of new active nodes from old active node:
<ul class="org-ul">
<li>\(X_k \sim \text{Binom}(n-k-A_k,p)\)</li>
<li>Indep.</li>
</ul></li>
<li>Approximation: while \(k\) and \(A_k << n\): \(\text{Binom}(n-k-A_k,p) \simeq
  \text{Binom}(n,p)\)</li>
<li>Termination: if E[# offspring] &gt; 1; then P[termination] &lt; 1; if E[# offspring]
&lt; 1, then P[termination] = 1</li>
</ul>
</div>
</div>

<div id="outline-container-org8b5448d" class="outline-5">
<h5 id="org8b5448d"><span class="section-number-5">3.5.2.2</span> Condition for GC in \(G(n,p)\)</h5>
<div class="outline-text-5" id="text-3-5-2-2">
<ul class="org-ul">
<li>Set \(p=\frac{c}{n}\)
<ul class="org-ul">
<li>\(c\): average degree</li>
<li>Number of offspring \(\sim\) Poisson(\(c\))</li>
</ul></li>
<li>Theorem:
<ul class="org-ul">
<li>if \(c > 1\), then \(G(n,p)\) has a single component of size \(\Theta(n)\)
asymptotically almost surely; all other components are small (of size
\(o(n)\))</li>
<li>if \(c < 1\), then \(G(n,p)\) has only small components</li>
</ul></li>
<li>Interpretation:
<ul class="org-ul">
<li>Giant component emerges naturally, even with completely random edge
generation</li>
<li>No network-wide “coordination” needed</li>
<li>Sharp threshold – phase transition! At avg degree c = 1</li>
</ul></li>
<li>Note:
<ul class="org-ul">
<li>More to prove: single component; impact of ignoring \(k + A_k\)</li>
<li>Below threshold (c &lt; 1): all small components are trees</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org82f3e37" class="outline-5">
<h5 id="org82f3e37"><span class="section-number-5">3.5.2.3</span> \(G(n,p)\) model: connectivity</h5>
<div class="outline-text-5" id="text-3-5-2-3">
<ul class="org-ul">
<li>Another phase transition: Consider threshold function \(t(n) = \frac{\log n}{n}\)</li>
<li>Theorem:
<ul class="org-ul">
<li>If \(p(n)/t(n) \to \infty\), then \(G(n, p)\) is connected (a.a.s.)</li>
<li>If \(p(n)/t(n) \to 0\), then \(G(n, p)\) is not connected (a.a.s.)</li>
<li>Gap between these two &rarr; harder to analyze</li>
</ul></li>
<li>Intuition: <a href="https://en.wikipedia.org/wiki/Coupon_collector's_problem">Coupon collector problem</a>: \(n \log n\) balls needed to have no empty
bins</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd4c4917" class="outline-3">
<h3 id="orgd4c4917"><span class="section-number-3">3.6</span> Clustering</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>Clustering = transitivity, two nodes with common neighbor likely to be
connected</li>
<li>Clustering coefficient = # links among friends / # possible links among
friends = empirical probability that \((v,w)\) exists given \((u,v)\) and \((u,w)\)
exist</li>
<li>If links were entirely random:
<ul class="org-ul">
<li>\(c_u=p=\frac{m}{\binom{n}{2}} \simeq \frac{2m}{n^2}\), where \(m\) = # edges in
network</li>
<li>So \(c_u >> p\) means the network has high clustering</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org0353ed2" class="outline-4">
<h4 id="org0353ed2"><span class="section-number-4">3.6.1</span> Network metrics</h4>
<div class="outline-text-4" id="text-3-6-1">
<ul class="org-ul">
<li>Average clustering coefficient: \(c_G = \frac{1}{n} \sum_u c_u\)</li>
<li>Weighted average clustering coefficient (also called "transitivity"):</li>
</ul>
<p>
\[c_G = \frac{\sum_u \binom{d_u}{2}c_u}{\sum_u \binom{d_u}{2}} = \frac{\text{#
closed triples}}{\text{# connected triples}} = 3 \frac{\text{#
triangles}}{\text{# connected triples}}\]
</p>
</div>
</div>
</div>

<div id="outline-container-org883fcdd" class="outline-3">
<h3 id="org883fcdd"><span class="section-number-3">3.7</span> Strong and weak ties</h3>
<div class="outline-text-3" id="text-3-7">
<p>
Now edges have propery: "Strong" = close person, "weak" = acquaintances
</p>
</div>

<div id="outline-container-orge9a8abe" class="outline-4">
<h4 id="orge9a8abe"><span class="section-number-4">3.7.1</span> Bridges</h4>
<div class="outline-text-4" id="text-3-7-1">
<ul class="org-ul">
<li>Bridge, removing it disconnects two components</li>
<li>Local bridge, makes distance big, no more common neighbors</li>
</ul>
</div>
</div>

<div id="outline-container-orgcfb8ba0" class="outline-4">
<h4 id="orgcfb8ba0"><span class="section-number-4">3.7.2</span> Strong Triadic Closure (STC) node property</h4>
<div class="outline-text-4" id="text-3-7-2">
<p>
A node a violates <a href="https://en.wikipedia.org/wiki/Triadic_closure">STC</a> if there are two strong edges \((a, b)\) and \((a, c)\), but there
is no edge \((b, c)\)
</p>
</div>
</div>

<div id="outline-container-orgc1771bb" class="outline-4">
<h4 id="orgc1771bb"><span class="section-number-4">3.7.3</span> STC &rArr; local bridges are weak ties</h4>
<div class="outline-text-4" id="text-3-7-3">
<ul class="org-ul">
<li>Lemma: if a node \(a\) satisfies STC (and has at least two strong ties), then
any local bridge \((a,b)\) is weak.</li>
<li>Proof:
<ul class="org-ul">
<li>Assume node a satisfies STC, but \((a, b)\) is strong and local bridge</li>
<li>By assumption, there is at least one other strong tie \((a, c)\)</li>
<li>By STC, \((b, c)\) must exist</li>
<li>But then \(a\) and \(b\) have common neighbor \(c\), so \((a, b)\) is not a local bridge</li>
<li>Contradiction</li>
</ul></li>
<li>Insight: Social ties to communities usually go through weak links</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org361e6d2" class="outline-3">
<h3 id="org361e6d2"><span class="section-number-3">3.8</span> Short paths</h3>
<div class="outline-text-3" id="text-3-8">
<p>
<a href="https://en.wikipedia.org/wiki/Small-world_experiment">Milgram 1969</a>
</p>
<ul class="org-ul">
<li>Theorem: \(G(n,p)\) has diameter \(\log(n) / \log(np)\)</li>
<li>Intuition:
<ul class="org-ul">
<li>Graph looke close to a tree from every node</li>
<li>Randomness creates very "efficient" graphs (edges used very well to reach
large number of nodes + few short cycle, incl. triangles)</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org89eb35a" class="outline-4">
<h4 id="org89eb35a"><span class="section-number-4">3.8.1</span> Evolution of average distance with shortcuts</h4>
<div class="outline-text-4" id="text-3-8-1">
<ul class="org-ul">
<li>Average distance on a circle: \(cn\), where \(c\) is a constant</li>
<li>Average distance with one "ideal" shortcut \(cn/2\)</li>
<li>With \(k\) shortcuts \(O(\frac{n}{2^k})\)</li>
<li>Distance drops quickly with \(k\)</li>
</ul>
</div>
</div>

<div id="outline-container-org83c3568" class="outline-4">
<h4 id="org83c3568"><span class="section-number-4">3.8.2</span> Evolution of clustering coeff with shortcuts</h4>
<div class="outline-text-4" id="text-3-8-2">
<ul class="org-ul">
<li>As long as \(k << n\), small impact on clustering coefficient, few triangles get
destroyed</li>
</ul>
</div>
</div>

<div id="outline-container-org55ec833" class="outline-4">
<h4 id="org55ec833"><span class="section-number-4">3.8.3</span> Small worlds</h4>
<div class="outline-text-4" id="text-3-8-3">
<ul class="org-ul">
<li>high clustering</li>
<li>small distances</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org19c334e" class="outline-2">
<h2 id="org19c334e"><span class="section-number-2">4</span> Lecture 3</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgf3b6055" class="outline-3">
<h3 id="orgf3b6055"><span class="section-number-3">4.1</span> Herding and "watching thy neighbor"</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Information cascades: why imitating your friends makes sense – and how it can lead to surprising group behavior</li>
<li>Heavy-tailed degree distributions: “the rich get richer” applied to networks</li>
</ul>
</div>
</div>

<div id="outline-container-orgb06cc47" class="outline-3">
<h3 id="orgb06cc47"><span class="section-number-3">4.2</span> Watching thy Neighbor</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Human decision-making:
<ul class="org-ul">
<li>Primary private information&#x2026;</li>
<li>Heavily influenced by what decisions taken by others</li>
</ul></li>
<li>Reason:
<ul class="org-ul">
<li>Primary information: often too voluminous, noisy, not trustworthy,&#x2026;</li>
<li>By imitating others, piggyback on their effort to interpret primary information</li>
</ul></li>
<li>Question: Macro behavior of such systems?</li>
</ul>
</div>
</div>

<div id="outline-container-orgbb69a61" class="outline-3">
<h3 id="orgbb69a61"><span class="section-number-3">4.3</span> Herding how it can go wrong? Urn model</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>Urn with 3 balls
<ul class="org-ul">
<li>A priori distribution (blue/red majority) = (0.5,0.5)</li>
<li>majority blue: 2 blue + 1 red</li>
<li>majority red: 2 red + 1 blue</li>
</ul></li>
<li>A group of people take turns:
<ul class="org-ul">
<li>Draw a ball from the urn at random</li>
<li>Check the color of the ball privately, put it back in urn</li>
<li>Announce their guess (blue/red majority) to everybody</li>
</ul></li>
<li>Assumption:
<ol class="org-ol">
<li>Each individual is altruistic: do what allows others to make best guess</li>
<li>Each individual is selfish = tries to make best guess for himself</li>
</ol></li>
</ul>
</div>

<div id="outline-container-orgc1e3033" class="outline-4">
<h4 id="orgc1e3033"><span class="section-number-4">4.3.1</span> Urn model: altruistic</h4>
<div class="outline-text-4" id="text-4-3-1">
<ul class="org-ul">
<li>Every person:
<ul class="org-ul">
<li>Selects a ball at random (with replacement)</li>
<li>Announces the color of the ball to everybody</li>
</ul></li>
<li>As \(n \to \infty\), majority color of urn is equal to color most frequently observed
<ul class="org-ul">
<li>Consequence of law of large numbers</li>
</ul></li>
<li>After a few “sacrifices”, everybody could produce best guess
<ul class="org-ul">
<li>Sacrifice in the sense that first few individuals might be forced to say red
(color of their ball) even if previous information suggests blue majority</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd0488d7" class="outline-4">
<h4 id="orgd0488d7"><span class="section-number-4">4.3.2</span> Urn model: selfish</h4>
<div class="outline-text-4" id="text-4-3-2">
<ul class="org-ul">
<li>Sequential decision-making
<ul class="org-ul">
<li>Public guess – goal: correct guess for many/most people</li>
<li>Observed color remains private</li>
</ul></li>
<li>First individual:
<ul class="org-ul">
<li>Blue ball: announce guess(1) = blue</li>
<li>Red ball: announce guess(1) = red</li>
<li>Public guess of first fully reveals private information</li>
</ul></li>
<li>Second individual:
<ul class="org-ul">
<li>If color(2) = guess(1): announce this color</li>
<li>If color(2) ≠ guess(1): does not matter (assume color(2))</li>
<li>Public guess of second fully reveals private information</li>
</ul></li>
<li>Third individual:
<ul class="org-ul">
<li>if guess(1) ≠ guess(2): announce guess(3) = color(3)</li>
<li>If guess(1) = guess(2): Announce guess(3)=guess(2)=guess(1), regardless of color(3)</li>
<li>Why is this?
<ul class="org-ul">
<li>Person 3 knows that guesses 1+2 reveal perfect information</li>
<li>Therefore, regardless of color(3), guess(1)=guess(2) dominates guess</li>
</ul></li>
</ul></li>
<li>Fourth,&#x2026;,∞th individual:
<ul class="org-ul">
<li>If guess(1) = guess(2): Announce guess(i) = guess(2)=guess(1), regardless of
color(i)</li>
</ul></li>
</ul>
<p>
This model leads to cascade
</p>
<ul class="org-ul">
<li>If guess(1) = guess(2) were both wrong, then all future guesses are wrong!</li>
<li>This happens with prob. 1/9</li>
<li>Even though each individual is using available information in the best way to make a guess</li>
<li>Can show that in this model, this is sure to happen eventually (even if not
for 3 rd individual)</li>
</ul>
</div>
</div>

<div id="outline-container-orgbccfb4a" class="outline-4">
<h4 id="orgbccfb4a"><span class="section-number-4">4.3.3</span> Information cascade: suboptimal decision</h4>
<div class="outline-text-4" id="text-4-3-3">
<ul class="org-ul">
<li>Cascade: sequential decisions</li>
<li>Individual:
<ul class="org-ul">
<li>Efficiency gain by observing others’ decisions</li>
</ul></li>
<li>Global behavior:
<ul class="org-ul">
<li>Primary information can “wash out”</li>
<li>Suboptimal or random decisions</li>
</ul></li>
<li>Might these be cascades:
<ul class="org-ul">
<li>Stock market gyrations, “flash crash”</li>
<li>Inexplicable shifts in popularity of {restaurants, clubs, celebrities,&#x2026;}</li>
<li>Fashion, style, celebrity,&#x2026;</li>
<li>&#x2026;</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga078e9a" class="outline-3">
<h3 id="orga078e9a"><span class="section-number-3">4.4</span> Herding in networks</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Observation: degree distributions in networks often resemble power laws</li>
<li>Power law: \(P(D > d) \propto d^{-\gamma}\)</li>
<li>Must distributions have "light tails": eg. \(P(D > d) \propto e^{0\alpha d}\)</li>
</ul>
<p>
Example of such distribution: <a href="https://en.wikipedia.org/wiki/Pareto_distribution">Pareto distribution</a> 
</p>

<p>
Useful to compare the tail of the distribution, what is d* s.t P(D &gt; d*) = 10<sup>-9</sup>
</p>
</div>
</div>

<div id="outline-container-org7ed2929" class="outline-3">
<h3 id="org7ed2929"><span class="section-number-3">4.5</span> Preferential attachment in growing nets</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li>Growth model: nodes arrive one by one and join the existing network
<ul class="org-ul">
<li>Directed graph</li>
<li>In-degree \(d_{in{(v)\) measures "popularity" and attractiveness" of node</li>
</ul></li>
<li>Preferential attachment: new node creates one edge</li>
<li>Prob. of connecting to \(v\) is \(\propto d_{in}(v)\)</li>
<li>Intuition: high-degree easier to meet; more popular; more useful;&#x2026;</li>
<li>Node with in-degree 0 never gets “started”</li>
<li>Need another assumption:
<ul class="org-ul">
<li>With prob. \(\alpha\), new node connects uniformly at random</li>
<li>With prob. \(1 - \alpha\) , preferential attachment</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org899407c" class="outline-4">
<h4 id="org899407c"><span class="section-number-4">4.5.1</span> Analysis</h4>
<div class="outline-text-4" id="text-4-5-1">
<p>
cf. slides for details.
</p>
</div>
</div>
</div>

<div id="outline-container-org8530906" class="outline-3">
<h3 id="org8530906"><span class="section-number-3">4.6</span> Observing</h3>
<div class="outline-text-3" id="text-4-6">
</div><div id="outline-container-org74dee9b" class="outline-4">
<h4 id="org74dee9b"><span class="section-number-4">4.6.1</span> <a href="https://en.wikipedia.org/wiki/Friendship_paradox">Friendship Paradox</a></h4>
<div class="outline-text-4" id="text-4-6-1">
<ul class="org-ul">
<li>“Your friends have more friends than you”</li>
<li>Social network = \(G(V,E)\)</li>
<li>\(d_v\) : degree of node \(v\)</li>
<li>\(n = |V|\) : number of nodes, \(m = |E|\) : number of edges</li>
<li>Average number of friends: \(\mu = \frac{\sum d_v}{n}\)</li>
<li>How to talk about average number of friends’ friends?
<ul class="org-ul">
<li>Average degree over nodes: \(\mu = \frac{\sum d_v}{n} = 2 \frac{m}{n}\), look at each person</li>
<li>Average degree over edges: \(\frac{\sum_{(u,v)\in E} d_v}{2m}\), look at each person's list of friends</li>
</ul></li>
<li>Lemma:
<ul class="org-ul">
<li>\(\frac{\sum_{(u,v)\in E} d_v}{2m} = \mu\left(1+\frac{\sigma^2}{\mu^2}\right)\)</li>
<li>Degree variance: \(\sigma^2 = \frac{1}{n} \sum_{v\in V} d^2_v - \left(\frac{1}{n} \sum_{v\in V} d_v\right)^2\)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org769c15d" class="outline-4">
<h4 id="org769c15d"><span class="section-number-4">4.6.2</span> Observer matters</h4>
<div class="outline-text-4" id="text-4-6-2">
<p>
Sampling bias
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3c5c58c" class="outline-2">
<h2 id="org3c5c58c"><span class="section-number-2">5</span> Lecture 4</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-org407c339" class="outline-3">
<h3 id="org407c339"><span class="section-number-3">5.1</span> Node statistics in large networks</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>Breadth-First Search
<ul class="org-ul">
<li>Problem: “locality bias”</li>
<li>E.g.: starting node is a page in English &rarr; most nearby pages probably are as well</li>
</ul></li>
<li>Depth-First Search
<ul class="org-ul">
<li>Advantage: avoid locality bias</li>
<li>Problem: bias in ordering of links</li>
<li>E.g.: suppose FB friends listed alphabetically -&gt; only visit people named
“A*"</li>
</ul></li>
<li>Node sampling
<ul class="org-ul">
<li>Urn model: select every node with prob. 1/n indep. with replacement; compute
average over many samples</li>
<li>Problem: usually not available, because we only have “neighbors of current
node”!</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgf127015" class="outline-3">
<h3 id="orgf127015"><span class="section-number-3">5.2</span> Random walks on graphs</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>Random Walk
<ul class="org-ul">
<li>Advantage: no ordering bias (by def); no locality bias (under some conditions)</li>
<li>A bit like DFS with shuffled neighbors (but RW can return)</li>
</ul></li>
<li>Undirected graph \(G(V,E)\)</li>
<li>Assume connected (otherwise assume G is the GC for the actual network)
<ul class="org-ul">
<li>Random Walk:
<ul class="org-ul">
<li>Discrete time \(t\)</li>
<li>Node at time \(t\): \(X_t\)</li>
<li>At each time step, go to a neighbor of \(X_t\) uniformly at random &rarr;
\(X_{t+1}\)</li>
</ul></li>
</ul></li>
</ul>
<p>
Problem, bias: high-degree nodes sampled more frequently
</p>
</div>

<div id="outline-container-org1240bec" class="outline-4">
<h4 id="org1240bec"><span class="section-number-4">5.2.1</span> Random walks as Markov chain</h4>
<div class="outline-text-4" id="text-5-2-1">
<ul class="org-ul">
<li>Transition matrix \(P\):</li>
</ul>
<p>
\[P= \begin{cases}
p_{ij} = 1/d_i & (i,j) \in E\\
0 & \text{otherwise}
\end{cases}\]
</p>
<ul class="org-ul">
<li>If \(G(V, E)\) is undirected, connected and non-bipartite, then \(\lbrace X_t
  \rbrace\) is an ergodic (irreducible, aperiodic) Markov chain</li>
</ul>
<p>
(Ergodicity: Stationary distribution \(\pi\), \(p_{ij}(t) \to \pi_j\)
</p>
</div>
</div>

<div id="outline-container-org94fb40b" class="outline-4">
<h4 id="org94fb40b"><span class="section-number-4">5.2.2</span> Stationary distribution &pi;</h4>
<div class="outline-text-4" id="text-5-2-2">
<ul class="org-ul">
<li>Lemma: \(\pi \propto [d_1 , d_2 , \dots , d_n ]\)</li>
<li>Proof:
<ul class="org-ul">
<li>Def of stationary distribution: \(\pi = \pi P\)</li>
<li>[d<sub>1</sub> , d<sub>2</sub> , d<sub>3</sub> , &hellip; , d<sub>n</sub>] P = x</li>
<li>\(x_j = \sum_i d_i p_{ij} = \sum_i 1_{\lbrace(i,j) \in E \rbrace} = d_j\)</li>
<li>\([d_1 , \dots , d_n]\) is eigenvector with eigenvalue=1 &rarr; stationary distribution</li>
</ul></li>
</ul>
<p>
Intuition:
</p>
<ul class="org-ul">
<li>Random walk “sees” uniformly random edges; nodes biased by # of edges = degree</li>
<li>Similar to Friendship Paradox!</li>
</ul>
</div>
</div>

<div id="outline-container-org3c90ad6" class="outline-4">
<h4 id="org3c90ad6"><span class="section-number-4">5.2.3</span> Obtaining unbiased estimator from RW</h4>
<div class="outline-text-4" id="text-5-2-3">
<ul class="org-ul">
<li>Node statistic \(f(i)\)</li>
<li>Would like to know \(F = \frac{1}{n} \sum_{v\in V} f(v)\)</li>
<li>Sampling:
<ul class="org-ul">
<li>Ideal: \(P(X_t = v) = \frac{1}{n}\)</li>
<li>RW: \(P(X_t = v) = \frac{d_v}{||d||_1} = \frac{d_v}{2m}\)</li>
</ul></li>
<li>Compensate for degree bias:
<ul class="org-ul">
<li>Let RW run for \(T\) time steps</li>
<li>Compute \(\hat{F} = \frac{2m \sum_t f(X_t)/d_{X_t}}{nT}\)</li>
</ul></li>
<li>Stationary regime: unbiased \(E[\hat{F}] = E[f(X_t)] = F\), but we cannot start
in stationary regime, instead at a specific state &rarr; how large does \(T\) have
to be?</li>
</ul>
</div>
</div>

<div id="outline-container-org52eac06" class="outline-4">
<h4 id="org52eac06"><span class="section-number-4">5.2.4</span> Estimator without knowledge of \(n,m\)</h4>
<div class="outline-text-4" id="text-5-2-4">
<ul class="org-ul">
<li>In practice, we may not know \(n, m\)</li>
<li>Eliminate from estimate:
<ul class="org-ul">
<li>Can estimate normalization constant from sample path</li>
<li>\(\hat{F} = \frac{\sum_t f(X_t)/d_{X_t}}{\sum_t 1/d_{X_t}}\)</li>
<li>Denominator: sum of all (random) weights</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgf921919" class="outline-4">
<h4 id="orgf921919"><span class="section-number-4">5.2.5</span> Spectral theorem applied to RW</h4>
<div class="outline-text-4" id="text-5-2-5">
<ul class="org-ul">
<li>Want to compute powers of \(P\): \(p_{ij}(t) = [P^t]_{ij}\): \(P\)(at \(j\) after \(t\)
steps|starting at \(i\))</li>
<li>Spectral decomposition: But P is not symmetric</li>
<li>Work with a “symmetrized” version of P
<ul class="org-ul">
<li>Def: D =diag(\frac{1}{d_1},&hellip;, \frac{1}{d_n})</li>
<li>Def: A = adjacency matrix</li>
<li>N = D1/2 AD1/2 = D −1/2 PD1/2</li>
<li>Symmetric &rarr; spectral form: \(N = \sum_{k=1}^n \lambda_k v_k v_k^T\)</li>
<li>\(\lambda k\) : eigenvalues</li>
<li>\(v_k\) : normalized eigenvectors</li>
</ul></li>
<li>\(p_{ij}(t) = [P^t]_{ij} = \pi_j + \sum_{k=2}^n \lambda_k^t \nu_{ki}\nu_{kj}
  \sqrt{\frac{d_j}{d_i}}\)</li>
</ul>
</div>
</div>

<div id="outline-container-org9d95164" class="outline-4">
<h4 id="org9d95164"><span class="section-number-4">5.2.6</span> Conductance</h4>
</div>
</div>
<div id="outline-container-orga148ca1" class="outline-3">
<h3 id="orga148ca1"><span class="section-number-3">5.3</span> Epidemics</h3>
</div>
</div>
<div id="outline-container-org172dcec" class="outline-2">
<h2 id="org172dcec"><span class="section-number-2">6</span> Lecture 5</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgb26ceea" class="outline-3">
<h3 id="orgb26ceea"><span class="section-number-3">6.1</span> Structure of the web</h3>
<div class="outline-text-3" id="text-6-1">
<p>
One dominant giant strongly connected component.
</p>
</div>
</div>
<div id="outline-container-org6b8235a" class="outline-3">
<h3 id="org6b8235a"><span class="section-number-3">6.2</span> Search &rarr; Ranking</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>Search query &rarr; ranked list of results</li>
<li>Two ingredients:
<ul class="org-ul">
<li>Relevance score: how relevant is the result to the query (cf retrieval lectures)</li>
<li>Importance score: quality, importance of the result independent of query</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgb332542" class="outline-4">
<h4 id="orgb332542"><span class="section-number-4">6.2.1</span> Hyperlink: intuition</h4>
<div class="outline-text-4" id="text-6-2-1">
<p>
Links are asymmetric
</p>
<ul class="org-ul">
<li>Existence under control of link tail
<ul class="org-ul">
<li>Means “X considers Y relevant”</li>
<li>Does not necessarily mean “quality” or “agreement”</li>
</ul></li>
<li>Represented as directed graph</li>
</ul>
</div>
</div>
<div id="outline-container-org09e781d" class="outline-4">
<h4 id="org09e781d"><span class="section-number-4">6.2.2</span> Turning hyperlink net into ranking</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>Importance score of page \(u:\) \(\pi_u\)</li>
<li>Approach 1: \(\pi_u = i_u\) (in-degree)
<ul class="org-ul">
<li>More endorsements = more important</li>
<li>Problem: easy to spam (e.g., link-farm)</li>
</ul></li>
<li>Approach 2: take into account importance of endorser &rarr; circular
<ul class="org-ul">
<li>\(\pi = \sum_{(v,u)}\pi_v\)</li>
<li>More important endorsers = more important</li>
<li>Problem: a page pointing to a single other page should be stronger endorsement
than e.g. a long list of links</li>
</ul></li>
</ul>
<p>
Approach 3:
\[\pi_u = \sum_{(v,u)} \frac{\pi_v}{o_v}\]
</p>
</div>
</div>
<div id="outline-container-org60dd826" class="outline-4">
<h4 id="org60dd826"><span class="section-number-4">6.2.3</span> Score-flow matrix \(H\)</h4>
<div class="outline-text-4" id="text-6-2-3">
<ul class="org-ul">
<li>Def: $H<sub>uv</sub> = \begin{cases} \frac{1}{o_u} &amp; (u,v) &isin; E <br /></li>
</ul>
<p>
0 &amp; \text{otherwise}
\end{cases}$
</p>
<ul class="org-ul">
<li>Note: \(H\) is the transition matrix of a RW on the web
<ul class="org-ul">
<li>"Random surfer": \(P(\text{at }v\text{ at time }t+1) = \sum_u P(\text{at
    }u\text{ at time }t)/o_u\)</li>
<li>\(\pi_{t+1} = \pi_t H\)</li>
</ul></li>
<li>If RW is ergodic, then \(\pi_t \to \pi\)
<ul class="org-ul">
<li>\(\pi = \pi H\), i.e., solves the score-flow equation</li>
<li>Condition for ergodicity: graph has to be non-periodic and strongly connected
&rarr; aperiodic and irreducible Markov chain</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2abf29c" class="outline-4">
<h4 id="org2abf29c"><span class="section-number-4">6.2.4</span> Dangling nodes</h4>
<div class="outline-text-4" id="text-6-2-4">
<p>
Dangling node = absorbing state of RW (not strongly connected)
</p>

<p>
&rArr; There is no (non-zero) \(\pi\) that solves \(\pi = \pi H\)
</p>
</div>
</div>
<div id="outline-container-org53692de" class="outline-4">
<h4 id="org53692de"><span class="section-number-4">6.2.5</span> Dealing with dangling nodes</h4>
<div class="outline-text-4" id="text-6-2-5">
<ul class="org-ul">
<li>Idea: if random surfer arrives at dangling node &rarr; go to any webpage uniformly at random
<ul class="org-ul">
<li>Or following some well-chosen distribution a over all nodes</li>
</ul></li>
<li>Def: $w=$indicator of dangling nodes, Example: \(w = (0,0,0,1)\)</li>
<li>\(\hat{H} = H +\frac{1}{n} (w^T e)\) (stochastic matrix)</li>
</ul>
</div>
</div>
<div id="outline-container-org1107111" class="outline-4">
<h4 id="org1107111"><span class="section-number-4">6.2.6</span> Google Matrix \(G\)</h4>
<div class="outline-text-4" id="text-6-2-6">
<p>
Can have sets that forms absorbing class or can have several solutions.
</p>
</div>
<div id="outline-container-org6537023" class="outline-5">
<h5 id="org6537023"><span class="section-number-5">6.2.6.1</span> Solution</h5>
<div class="outline-text-5" id="text-6-2-6-1">
<p>
Add randomization: at every iteration, coin flip: with prob \(\theta\) walk on the
graph \(\hat{H}\), with prob. \(1-\theta\) jump to a random page.
</p>
<ul class="org-ul">
<li>\(G = \theta \hat{H} = (1-\theta) \frac{e^T e}{n}\), where \(e^T e/n\) is called
the teleportation matrix</li>
<li>Theorem: if \(\theta < 1\), \(\pi = \pi G\) has exactly one solution for any
network graph</li>
<li>PageRank algoithm computes this solution</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org5faf26e" class="outline-2">
<h2 id="org5faf26e"><span class="section-number-2">7</span> Tips and tricks</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgaf054f1" class="outline-3">
<h3 id="orgaf054f1"><span class="section-number-3">7.1</span> Stationary distribution of RW</h3>
<div class="outline-text-3" id="text-7-1">
<p>
HOW to: compute eigenvectors of P (probability transition matrix), then &pi;
where &pi; P = P, is the stationary distribution.
</p>
</div>
</div>
</div>

<div id="outline-container-orgdc28593" class="outline-2">
<h2 id="orgdc28593"><span class="section-number-2">8</span> Lab1</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-orgc639100" class="outline-3">
<h3 id="orgc639100"><span class="section-number-3">8.1</span> <a href="https://github.com/mdeff/python_tour_of_data_science">https://github.com/mdeff/python_tour_of_data_science</a></h3>
</div>
<div id="outline-container-org07b6129" class="outline-3">
<h3 id="org07b6129"><span class="section-number-3">8.2</span> Finish lab 1</h3>
</div>
</div>
<div id="outline-container-orgc107e10" class="outline-2">
<h2 id="orgc107e10"><span class="section-number-2">9</span> Research</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-org986f827" class="outline-3">
<h3 id="org986f827"><span class="section-number-3">9.1</span> Principle of deffered decision for giant component discovery</h3>
</div>
<div id="outline-container-org4c5d72d" class="outline-3">
<h3 id="org4c5d72d"><span class="section-number-3">9.2</span> Unbiased estimator for Random Walks</h3>
</div>
<div id="outline-container-org9dc2a6d" class="outline-3">
<h3 id="org9dc2a6d"><span class="section-number-3">9.3</span> Mixing time for RW</h3>
</div>
<div id="outline-container-org83cc3f8" class="outline-3">
<h3 id="org83cc3f8"><span class="section-number-3">9.4</span> Conductance</h3>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2017-03-20 Mon 16:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
